# Distribution Infrastructure

Several computing resources and infrastructure are used in the ARTIS data pipeline. We utilize cloud high-performance computing, cloud-based storage, and local machines to integrate and analyze seafood supply chain data. These resources help us create the ARTIS database that is available at two access points.

The schematic below depicts how components of the ARTIS pipeline fit together. Blue boxes represent processes on local machines, green represent data access points, yellow represents cloud HPC and purple represents cloud database storage.

![](../images/artis_lucidchart_inf.png)

## Pipeline component descriptions and details:

Original Raw Data
: Trade data from FAO, UN Comtrade, and Sea Around Us

Clean Data
: Data cleaned and standardized in R and Python scripts and used in `model_inputs/` folder in ARTIS model

ARTIS Model
: A model that estimates the seafood trade flows and consumption




## Github Code Repositories:

artis-model
: Release version of model code [link here](https://github.com/Seafood-Globalization-Lab/artis-model)

artis-development
: Private development version of model code [link here](https://github.com/jagephart/ARTIS)

artis-hpc
: High-performance computing scripts for running model on Amazon Web Services [link here](https://github.com/Seafood-Globalization-Lab/artis-hpc)

artis-database
: Scripts for creating and maintaining the Postgres database in the cloud and locally [link here](https://github.com/Seafood-Globalization-Lab/artis-hpc)

artis-API
: Scripts for creating and maintaining the ARTIS API to website [link here](https://github.com/Seafood-Globalization-Lab/artis-hpc)

knb-submit
: Scripts for submitting data to the KNB data repository [link here](https://github.com/Seafood-Globalization-Lab/artis-hpc)
