[
  {
    "objectID": "external-content/knb-submit-readme.html",
    "href": "external-content/knb-submit-readme.html",
    "title": "Submit ARTIS to data repository",
    "section": "",
    "text": "Long-term stable archive of model inpusts, model, database, and metadata\nOpen-acess distribution point\n\nARTIS uses the The Knowledge Network for Biocomplexity KNB data repository to archive and distribute stable releases of the model codebase and resulting database. Archiving, documenting and openly distributing ARTIS is a critical component in contributing to the larger open-science and reproducible science community. ARTIS uses KNB as an access point for anyone to download the ARTIS model codebase and clean inputs and output ARTIS database\nKNB is guided by FAIR (findable, accessible, interoperable, resuble) principles of data sharing and preservation and issues unique DOIs (digital object identifier) to each data package and every version of the package for long term access, transparency, and informative citations.\n\n\n\nKNB is a member of DataONE (Data Observation Network for Earth); a network of data repositories. KNB uses EML (Ecological Metadata Language) to document objects within a data packages which can be created via the website GUI (graphical user interface) or through a series of R packages rdataone and arcticdatautils.\nNote: a user will need an ORCiD to log into KNB.\nFor additional resources on how to submit data to KNB see the links below from NCEAS (National Center for Ecological Analysis and Synthesis) who develops and maintains DataONE and KNB.\n\nKNB and ADC Data Team Training\nData Team Reference Guide"
  },
  {
    "objectID": "external-content/knb-submit-readme.html#purpose",
    "href": "external-content/knb-submit-readme.html#purpose",
    "title": "Submit ARTIS to data repository",
    "section": "",
    "text": "Long-term stable archive of model inpusts, model, database, and metadata\nOpen-acess distribution point\n\nARTIS uses the The Knowledge Network for Biocomplexity KNB data repository to archive and distribute stable releases of the model codebase and resulting database. Archiving, documenting and openly distributing ARTIS is a critical component in contributing to the larger open-science and reproducible science community. ARTIS uses KNB as an access point for anyone to download the ARTIS model codebase and clean inputs and output ARTIS database\nKNB is guided by FAIR (findable, accessible, interoperable, resuble) principles of data sharing and preservation and issues unique DOIs (digital object identifier) to each data package and every version of the package for long term access, transparency, and informative citations."
  },
  {
    "objectID": "external-content/knb-submit-readme.html#further-details-resources",
    "href": "external-content/knb-submit-readme.html#further-details-resources",
    "title": "Submit ARTIS to data repository",
    "section": "",
    "text": "KNB is a member of DataONE (Data Observation Network for Earth); a network of data repositories. KNB uses EML (Ecological Metadata Language) to document objects within a data packages which can be created via the website GUI (graphical user interface) or through a series of R packages rdataone and arcticdatautils.\nNote: a user will need an ORCiD to log into KNB.\nFor additional resources on how to submit data to KNB see the links below from NCEAS (National Center for Ecological Analysis and Synthesis) who develops and maintains DataONE and KNB.\n\nKNB and ADC Data Team Training\nData Team Reference Guide"
  },
  {
    "objectID": "external-content/artis-model-readme.html",
    "href": "external-content/artis-model-readme.html",
    "title": "ARTIS Model",
    "section": "",
    "text": "The ARTIS model codebase. This repository contains the demo version and the full model.\n\n\nThis project uses Python 3.10.9 which can be downloaded here and RStudio which can be downloaded here.\nIt should take approximately 10 minutes to run this full installation.\n\n\n\nNOTE: This protocol may not be successful for every individual local machine. The interaction in package versions and computer architecture (i.e. arm64 M1, M2 chips) may complicate this virtual environment set up. We are working on setting up a portable docker image to increase the reproducibility of this code.\n\nOpen the artis-model repository in RStudio.\nClick on the terminal tab.\nType pwd in the terminal.\nCopy the result of the “pwd” terminal command.\nType python3 -m venv [RESULT FROM pwd]/venv (ie. python3 -m venv /home/artis-model/venv)\nType source venv/bin/activate in terminal.\nType pip3 install qpsolvers in terminal.\nType pip3 install quadprog in terminal.\nType pip3 install cvxopt in terminal.\nConfirm you have successfully installed the packages qpsolvers, quadprog, cvxopt by running pip list.\nType deactivate in terminal.\nClick on the Console tab.\n\nNote: You only need to install the solvers the first time you run this code. Warnings about the latest version of pip may also appear during the installation - these are okay, but errors are not.\n\n\n\n\nClick “Build” on the build tab on the top right hand side corner of RStudio.\nClick on the dropdown arrow in the “Install” subtab within the “Build” window.\nClick the option “Configure Build Tools…”\nMake sure options mirror the image below and click OK.\n\n\n\nClick on the dropdown arrow in the “Install” subtab and select the option “Clean and Install”\n\n\n\n\nRunning the demo for the ARTIS model should take approximately 10 minutes. To run the demo for ARTIS run the 02-artis-pipeline.R script and then run the 04-build-artis-timeseries.R script.\n\n\n\nThe outputs of the demo will appear in the demo/outputs directory. Within this folder demo/outputs/custom_ts will contain all the final files that if run on the full model inputs would be used to create the results of the ARTIS research paper.\nPlease find below descriptions of main files: - demo/outputs/custom_ts/mid_custom_ts.csv: This is the demo version of the main ARTIS trade records table. - demo/outputs/custom_ts/summary_consumption_midpoint.csv: This is the demo version of the main ARTIS seafood consumption records table.\n\n\n\n\n\nThe following diagrams describes how ARTIS trade records are obtained.\n  \n\n\n\nThe following diagrams describe the how the codebase follows the workflow illustrated above.\n  \n\n\n\n\n\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Ventura 13.3.1\nR version 4.2.2\nR packages:\n\nreadxl 1.4.1\njanitor 2.1.0\ncountrycode 1.4.0\ndoParallel 1.0.17\niterators 1.0.14\nforeach 1.5.2\nslam 0.1-50\nMatrix 1.5-1\nmagrittr 2.0.3\ndata.table 1.14.6\nforcats 0.5.2\nstringr 1.5.0\ndplyr 1.0.10\npurrr 1.0.1\nreadr 2.1.3\ntidyr 1.2.1\ntibble 3.1.8\nggplot2 3.4.0\ntidyverse 1.3.2\nreticulate 1.26\n\nPython version 3.10.9\nPython packages:\n\ncvxopt 1.3.0\ndaqp 0.5.1\necos 2.0.12\nnumpy 1.24.3\nosqp 0.6.2.post9\npip 22.3.1\nqdldl 0.1.7\nqpsolvers 3.4.0\nquadprog 0.1.11\nscipy 1.10.1\nscs 3.2.3\nsetuptools 65.6.3"
  },
  {
    "objectID": "external-content/artis-model-readme.html#installation-guide",
    "href": "external-content/artis-model-readme.html#installation-guide",
    "title": "ARTIS Model",
    "section": "",
    "text": "This project uses Python 3.10.9 which can be downloaded here and RStudio which can be downloaded here.\nIt should take approximately 10 minutes to run this full installation."
  },
  {
    "objectID": "external-content/artis-model-readme.html#creating-python-virtual-environment",
    "href": "external-content/artis-model-readme.html#creating-python-virtual-environment",
    "title": "ARTIS Model",
    "section": "",
    "text": "NOTE: This protocol may not be successful for every individual local machine. The interaction in package versions and computer architecture (i.e. arm64 M1, M2 chips) may complicate this virtual environment set up. We are working on setting up a portable docker image to increase the reproducibility of this code.\n\nOpen the artis-model repository in RStudio.\nClick on the terminal tab.\nType pwd in the terminal.\nCopy the result of the “pwd” terminal command.\nType python3 -m venv [RESULT FROM pwd]/venv (ie. python3 -m venv /home/artis-model/venv)\nType source venv/bin/activate in terminal.\nType pip3 install qpsolvers in terminal.\nType pip3 install quadprog in terminal.\nType pip3 install cvxopt in terminal.\nConfirm you have successfully installed the packages qpsolvers, quadprog, cvxopt by running pip list.\nType deactivate in terminal.\nClick on the Console tab.\n\nNote: You only need to install the solvers the first time you run this code. Warnings about the latest version of pip may also appear during the installation - these are okay, but errors are not."
  },
  {
    "objectID": "external-content/artis-model-readme.html#r-installation-instructions",
    "href": "external-content/artis-model-readme.html#r-installation-instructions",
    "title": "ARTIS Model",
    "section": "",
    "text": "Click “Build” on the build tab on the top right hand side corner of RStudio.\nClick on the dropdown arrow in the “Install” subtab within the “Build” window.\nClick the option “Configure Build Tools…”\nMake sure options mirror the image below and click OK.\n\n\n\nClick on the dropdown arrow in the “Install” subtab and select the option “Clean and Install”"
  },
  {
    "objectID": "external-content/artis-model-readme.html#running-the-model-demo",
    "href": "external-content/artis-model-readme.html#running-the-model-demo",
    "title": "ARTIS Model",
    "section": "",
    "text": "Running the demo for the ARTIS model should take approximately 10 minutes. To run the demo for ARTIS run the 02-artis-pipeline.R script and then run the 04-build-artis-timeseries.R script."
  },
  {
    "objectID": "external-content/artis-model-readme.html#outputs",
    "href": "external-content/artis-model-readme.html#outputs",
    "title": "ARTIS Model",
    "section": "",
    "text": "The outputs of the demo will appear in the demo/outputs directory. Within this folder demo/outputs/custom_ts will contain all the final files that if run on the full model inputs would be used to create the results of the ARTIS research paper.\nPlease find below descriptions of main files: - demo/outputs/custom_ts/mid_custom_ts.csv: This is the demo version of the main ARTIS trade records table. - demo/outputs/custom_ts/summary_consumption_midpoint.csv: This is the demo version of the main ARTIS seafood consumption records table."
  },
  {
    "objectID": "external-content/artis-model-readme.html#methods-and-workflow",
    "href": "external-content/artis-model-readme.html#methods-and-workflow",
    "title": "ARTIS Model",
    "section": "",
    "text": "The following diagrams describes how ARTIS trade records are obtained.\n  \n\n\n\nThe following diagrams describe the how the codebase follows the workflow illustrated above."
  },
  {
    "objectID": "external-content/artis-model-readme.html#system-requirements",
    "href": "external-content/artis-model-readme.html#system-requirements",
    "title": "ARTIS Model",
    "section": "",
    "text": "Platform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Ventura 13.3.1\nR version 4.2.2\nR packages:\n\nreadxl 1.4.1\njanitor 2.1.0\ncountrycode 1.4.0\ndoParallel 1.0.17\niterators 1.0.14\nforeach 1.5.2\nslam 0.1-50\nMatrix 1.5-1\nmagrittr 2.0.3\ndata.table 1.14.6\nforcats 0.5.2\nstringr 1.5.0\ndplyr 1.0.10\npurrr 1.0.1\nreadr 2.1.3\ntidyr 1.2.1\ntibble 3.1.8\nggplot2 3.4.0\ntidyverse 1.3.2\nreticulate 1.26\n\nPython version 3.10.9\nPython packages:\n\ncvxopt 1.3.0\ndaqp 0.5.1\necos 2.0.12\nnumpy 1.24.3\nosqp 0.6.2.post9\npip 22.3.1\nqdldl 0.1.7\nqpsolvers 3.4.0\nquadprog 0.1.11\nscipy 1.10.1\nscs 3.2.3\nsetuptools 65.6.3"
  },
  {
    "objectID": "external-content/artis-api-making_requests.html",
    "href": "external-content/artis-api-making_requests.html",
    "title": "How to make Requests to the ARTIS API",
    "section": "",
    "text": "All API users require an API key to submit requests to the ARTIS API. Any requests not containing an API key will automatically be rejected. Please place your API key in the HTTP request header under the name “X-API-KEY”.\n\n\n\n\n\n\n\nHTTP Request Type\n\n\nEndpoint\n\n\nParameters\n\n\nResponse\n\n\n\n\nGET\n\n\n/snet/query\n\n\nRequired Parameters:\n\n\ncols_wanted (at least one value required): “exporter_iso3c”, “importer_iso3c”, “source_country_iso3c”, “hs6”, “sciname”, “habitat”, “method”, “dom_source”, “year”.\n\n\nweight_type (only one value can be selected): “product_weight_t” or “live_weight_t”\n\n\nstart_year: year from 1996-2020 (inclusive)\n\n\nend_year: year from 1996-2020 (inclusive)\n\n\nsearch_criteria: Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required.\n\nNote: start_year cannot be greater than end_year. For results for just 1 year, make start_year equal to end_year.\n\nOptional Parameters:\n\n\nSend in additional column names as parameters, with strings as values for the values you want to filter for.\n\n\n\n\n\nAn ID number corresponding to the job that has been submitted containing your query request.\n\n\nA summary of the data request sent to the endpoint.\n\n\n\n\n\nGET\n\n\n/consumption/query\n\n\nRequired Parameters:\n\n\ncols_wanted (at least one value required): “consumer_iso3c”, “exporter_iso3c”, “source_country_iso3”, “year”, “hs_version”, “dom_source”, “sciname”, “habitat”, “method”, “consumption_source”, “sciname_hs_modified”\n\n\nstart_year: year from 1996-2020 (inclusive)\n\n\nend_year: year from 1996-2020 (inclusive)\n\n\nsearch_criteria: Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required.\n\n\nOptional Parameters:\n\n\nSend in additional column names as parameters, with strings as values for the values you want to filter for.\n\n\n\n\n\n\n\n\nGET\n\n\n/jobs/status\n\n\nRequired Parameters:\n\n\nid: Job id number returned from an snet or consumption query\n\n\n\n\n\n\nstatus: job status (wait/active/completed etc)\n\n\nresult: a list of objects where each object would correspond to a row of results for the query submitted\n\n\n\n\n\n\nGET\n\n\n/supplemental\n\n\nRequired Parameters:\n\n\ntable: supplemental table name.\n\n\nvariable: column name within supplemental table.\n\n\n\n\nA list of strings representing unique values in the column requested.\n\n\n\n\nGET\n\n\n/supplemental/query\n\n\nRequired Parameters:\n\n\ntable: supplemental table name you want information from.\n\n\ncols_wanted: columns include in the table requested.\n\n\nsearch_criteria: Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required.\n\n\nOptional Parameters:\n\n\nSend in additional column names as parameters, with strings as values for the values you want to filter for.\n\n\n\n\nA list of object where each object represents a row of data in the supplemental table requested.\n\n\n\n\nAn indepth review of the API requests and responses is available below.\n\n\n\nTo get an update on jobs that have been submitted send a GET request to the /jobs/status endpoint.\nNote: only requests to the /snet/query, /consumption/query, /baci/query, /production/query endpoints will generate a job ID. Requests to /supplemental endpoints will generate an immediate JSON response with data back to the requester.\nA correct request to the /jobs/status endpoint consists of 1 query parameter: - id (REQUIRED): a job ID number. This job ID was provided by the /snet/query endpoint.\nA response to the /jobs/status endpoint consists of 2 fields: - status: a string describing the status of the job submitted. Possible return values include: - completed: Job has been completed. - active: Job is currently being executed. - wait: Job is on the queue and is waiting to be executed by the server. - delayed: Job has been delayed due to current server constraints, and will be submitted into the job queue as soon as possible. - failed: Job has failed. Please review request that has been made and modify as appropriate. - results: an array of objects where each object corresponds to a row of results from the query provided. Note, only when the status is “completed” will results be a non-empty array. The specifics as to how each object is structured are outlined below.\nExample successful response:\n{\n    status: \"completed\",\n    results: [\n        {year: 2018, live_weight_t: 180}\n        {year: 2019, live_weight_t: 250},\n        {year: 2020, live_weight_t: 100}\n    ]\n}\nExample job is currently in being executed:\n{\n    status: \"active\",\n    results: []\n}\nExample job is currently waiting to be executed:\n{\n    status: \"wait\",\n    results: []\n}\nExample job failed:\n{\n    status: \"failed\",\n    results: []\n}\n\n\n\nNote: The /snet/query endpoint only submits a job request and will not provide results from the ARTIS seafood trade network table. All requests to the /snet/query endpoint provide an immediate response with a job id, along with information about the API request that was submitted. You will still need to request the /jobs/status endpoint to get an update on the job submitted and if completed the results of the database request. Please refer to the /jobs endpoint documentation for more detail.\nThis section outlines how to make requests for data within the ARTIS snet table. Note that all requests require some kind of filtering criteria, if you would like to request the complete ARTIS snet data please send an email to [ENTER EMAIL HERE].\nA request for the main ARTIS snet table consists of 5 fields: - cols_wanted (REQUIRED): A string containing the names of the columns, comma separated without spaces, used to summarize the ARTIS data you are requesting. - weight_type (REQUIRED): A string either “live_weight_t” or “product_weight_t”, denoting what mass measurement you would like to use. Note you can only choose ONE weight_type. - start_year (REQUIRED): A year between 1996-2020 - end_year (REQUIRED): A year between 1996-2020 must be greater than or equal to start_year - search_criteria (REQUIRED): Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required. Note all optional parameters sent will be treated as filtering criteria if search_criteria is set to 1. If search_criteria is set to 0, then all optional parameters will be ignored.\n\n\nIf you wanted to send a request for all ARTIS snet data summarize by year you would send a GET request to the /snet/query endpoint, with the following parameters:\nNote: In this request we are NOT performing any filtering on the ARTIS snet (with exception of the start and end years).\n{\n    \"cols_wanted\": \"year\",\n    \"weight_type\": \"live_weight_t\",\n    \"start_year\": 2015,\n    \"end_year\": 2020,\n    \"search_criteria\": 0\n}\nThe endpoint with parameters would look like this:\n/snet/query?cols_wanted=year&weight_t=live_weight_t&start_year=2015&end_year=2020&search_criteria=0\nHere is a sample of the results list that would be returned:\n[\n    {\n        \"year\": 2020,\n        \"live_weight_t\": 44166033.00951629\n    },\n    {\n        \"year\": 2015,\n        \"live_weight_t\": 40891197.02143691\n    },\n    {\n        \"year\": 2018,\n        \"live_weight_t\": 46537958.721290946\n    },\n    {\n        \"year\": 2017,\n        \"live_weight_t\": 44763794.736794\n    },\n    {\n        \"year\": 2019,\n        \"live_weight_t\": 45180309.70194027\n    },\n    {\n        \"year\": 2016,\n        \"live_weight_t\": 41962089.008993454\n    }\n]\nIf you wanted to send the same request but only for US and China capture trade from 2017 - 2019 you would include the following parameters:\n{\n    \"cols_wanted\": \"exporter_iso3c,year\",\n    \"weight_type\": \"live_weight_t\",\n    \"start_year\": 2017,\n    \"end_year\": 2019,\n    \"search_criteria\": 1,\n    \"exporter_iso3c\": \"CHN,USA\",\n    \"method\": \"capture\"\n}\nThe final url would look like this:\n/snet/query?cols_wanted=exporter_iso3c,year&weight_type=live_weight_t&start_year=2017&end_year=2019&search_criteria=1&exporter_iso3c=CHN,USA&method=capture\nHere is a sample of the results list that would be returned:\n[\n    {\n        \"exporter_iso3c\": \"USA\",\n        \"year\": 2017,\n        \"live_weight_t\": 1975004.9836222837\n    },\n    {\n        \"exporter_iso3c\": \"CHN\",\n        \"year\": 2018,\n        \"live_weight_t\": 3086899.0878980905\n    },\n    {\n        \"exporter_iso3c\": \"USA\",\n        \"year\": 2019,\n        \"live_weight_t\": 1699555.4157051465\n    },\n    {\n        \"exporter_iso3c\": \"CHN\",\n        \"year\": 2019,\n        \"live_weight_t\": 2960335.342349926\n    },\n    {\n        \"exporter_iso3c\": \"CHN\",\n        \"year\": 2017,\n        \"live_weight_t\": 3256231.7707880796\n    },\n    {\n        \"exporter_iso3c\": \"USA\",\n        \"year\": 2018,\n        \"live_weight_t\": 1807505.2796781566\n    }\n]\nIf you wanted to explore bilateral trade relationships for a specific species (salmo salar) in 2019, you would send the following request:\n{\n    \"cols_wanted\": \"exporter_iso3c,importer_iso3c,year\",\n    \"weight_type\": \"live_weight_t\",\n    \"start_year\": 2019,\n    \"end_year\": 2019,\n    \"search_criteria\": 1,\n    \"sciname\": \"salmo salar\"\n}\nThe final URL would look like this: /snet/query?cols_wanted=exporter_iso3c,importer_iso3c,year&weight_type=live_weight_t&start_year=2019&end_year=2019&search_criteria=1&sciname=salmo salar\nHere is a sample of the results list that would be returned:\n[\n    {\n        \"exporter_iso3c\": \"AUS\",\n        \"importer_iso3c\": \"CHN\",\n        \"year\": 2019,\n        \"live_weight_t\": 7919.87229991926\n    },\n    {\n        \"exporter_iso3c\": \"BEL\",\n        \"importer_iso3c\": \"COG\",\n        \"year\": 2019,\n        \"live_weight_t\": 5205.84567743523\n    },\n    {\n        \"exporter_iso3c\": \"BEL\",\n        \"importer_iso3c\": \"NLD\",\n        \"year\": 2019,\n        \"live_weight_t\": 2997.68945007121\n    },\n\n\n\n\nNote: The /consumption/query endpoint only submits a job request and will not provide results from the ARTIS consumption table. All requests to the /consumption/query endpoint provide an immediate response with a job id, along with information about the API request that was submitted. You will still need to request the /jobs/status endpoint to get an update on the job submitted and, if completed, the results of the database request. Please refer to the /jobs endpoint documentation for more detail.\nThis section outlines how to make requests for data within the ARTIS consumption table. Note that all requests require some kind of filtering criteria, if you would like to request the complete ARTIS consumption data please send an email to [ENTER EMAIL HERE].\nA request for the main ARTIS consumption table consists of 4 fields: - cols_wanted (REQUIRED): A string containing the names of the columns, comma separated without spaces, used to summarize the ARTIS data you are requesting. Note that all consumption results are returned in live weight equivalent tonnes. - start_year (REQUIRED): A year between 1996-2020 - end_year (REQUIRED): A year between 1996-2020 must be greater than or equal to start_year - search_criteria (REQUIRED): Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required. Note all optional parameters sent will be treated as filtering criteria if search_criteria is set to 1. If search_criteria is set to 0, then all optional parameters will be ignored.\n\n\nIf you wanted to send a request for all ARTIS consumption data summarized by year you would send a GET request to the /consumption/query endpoint, with the following parameters:\nNote: In this request we are NOT performing any filtering on the ARTIS snet (with exception of the start and end years).\n{\n    \"cols_wanted\": \"year\",\n    \"start_year\": 2015,\n    \"end_year\": 2020,\n    \"search_criteria\": 0\n}\nThe endpoint with parameters would look like this:\n/consumption/query?cols_wanted=year&start_year=2015&end_year=2020&search_criteria=0\n[\n    {\n        \"year\": 2015,\n        \"consumption_live_t\": 150051627.61692697\n    },\n    {\n        \"year\": 2016,\n        \"consumption_live_t\": 152188217.6477852\n    },\n    {\n        \"year\": 2017,\n        \"consumption_live_t\": 156667487.22123533\n    },\n    {\n        \"year\": 2018,\n        \"consumption_live_t\": 162506861.25704896\n    },\n    {\n        \"year\": 2019,\n        \"consumption_live_t\": 161828981.64790905\n    },\n    {\n        \"year\": 2020,\n        \"consumption_live_t\": 160382274.5401494\n    }\n]\nIf you wanted to explore consumption of a specific species (salmo salar) in 2019, you would send the following request:\n{\n    \"cols_wanted\": \"consumer_iso3c,year\",\n    \"start_year\": 2019,\n    \"end_year\": 2019,\n    \"search_criteria\": 1,\n    \"sciname\": \"salmo salar\"\n}\nThe final URL would look like this: /consumption/query?cols_wanted=consumer_iso3c,year&start_year=2019&end_year=2019&search_criteria=1&sciname=salmo%20salar\nHere is a sample of the results list that would be returned:\n[\n    {\n        \"consumer_iso3c\": \"URY\",\n        \"year\": 2019,\n        \"consumption_live_t\": 1268.9479577486022\n    },\n    {\n        \"consumer_iso3c\": \"USA\",\n        \"year\": 2019,\n        \"consumption_live_t\": 479888.35767397494\n    },\n    {\n        \"consumer_iso3c\": \"UZB\",\n        \"year\": 2019,\n        \"consumption_live_t\": 179.3410667585069\n    },\n    {\n        \"consumer_iso3c\": \"VCT\",\n        \"year\": 2019,\n        \"consumption_live_t\": 98.57772699218599\n    },\n    {\n        \"consumer_iso3c\": \"VEN\",\n        \"year\": 2019,\n        \"consumption_live_t\": 117.01599593460986\n    },\n    {\n        \"consumer_iso3c\": \"VNM\",\n        \"year\": 2019,\n        \"consumption_live_t\": 15550.574977568014\n    }\n]\n\n\n\n\n\nThis is section outline how to make requests for any of the supplemental data tables: - baci - countries - production - products - sciname\nYou can make 2 kinds of requests for data in supplemental tables: 1. Getting all the unique values for a specific table. These requests are made to the url: /supplemental/ - This type of request has two REQUIRED parameters: - table: A string corresponding to the name of the supplemental table you want information from. - variable: A string corresponding to the name of the column. Will return all unique values in this column. 2. Getting all rows based on a filtered search criteria. These requests are made to the url: /supplemental/query/ - This type of the request has the uses the following parameters: - table (REQUIRED): A string corresponding to the name of the supplemental table you want information from. - cols_wanted (REQUIRED): A string corresponding to the columns, comma separated, that will be returned for each row. - search_criteria (REQUIRED): Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required. Note all optional parameters sent will be treated as filtering criteria if search_criteria is set to 1. If search_criteria is set to 0, then all optional parameters will be ignored.\n\n\nIf you wanted to get all scientific names for all the species / species groups in ARTIS you would send a request to the /supplemental endpoint with the following parameters:\n{\n    \"table\": \"sciname\",\n    \"variable\": \"sciname\"\n}\nThe final URL would be: /supplemental/?table=sciname&variable=sciname\nHere is sample response:\n{\n    \"sciname\": [\n        \"eucinostomus melanopterus\",\n        \"nassarius\",\n        \"merlucciidae\",\n        \"stellifer minor\",\n        \"malacocephalus occidentalis\",\n        \"patagonotothen ramsayi\",\n        \"pomadasys kaakan\",\n        \"carangoides malabaricus\"\n    ]\n}\nIf you wanted to get all common names for all the species / species groups in ARTIS, you would send a request to the /supplemental endpoint with the following parameters:\n{\n    \"table\": \"sciname\",\n    \"variable\": \"common_name\"\n}\nThe final URL would be: /supplemental/?table=sciname&variable=common_name\nHere is a sample response:\n\"common_name\": [\n        \"scomber mackerels nei\",\n        \"mango tilapia\",\n        \"ocellated wedge sole\",\n        \"spotted porcupinefish\",\n        \"shi drum\",\n        \"leopard fish\",\n        \"globose clam\",\n        \"geelbek croaker\"\n]\nIf you wanted to get all scientific, common names and ISSCAAP groups for a specific genus (for example thunnus) you would send a request to the /supplemental/query endpoint with the following parameters:\n{\n    \"table\": \"sciname\",\n    \"cols_wanted\": \"sciname,common_name,isscaap\",\n    \"search_criteria\": 1,\n    \"genus\": \"thunnus\"\n}\nThe final URL would be: /supplemental/query?table=sciname&cols_wanted=sciname,common_name,isscaap&search_criteria=1&genus=thunnus\nHere is a sample response:\n[\n    {\n        \"sciname\": \"thunnus\",\n        \"common_name\": \"tunas nei\",\n        \"isscaap\": \"Tunas, bonitos, billfishes\"\n    },\n    {\n        \"sciname\": \"thunnus alalunga\",\n        \"common_name\": \"albacore\",\n        \"isscaap\": \"Tunas, bonitos, billfishes\"\n    }\n]\nIf you wanted to get production of all USA and Chilean salmo salar by production method and year, you would send a request to the /supplemental/query endpoint with the following parameters:\n{\n    \"table\": \"production\",\n    \"cols_wanted\": \"iso3c,method,year,live_weight_t\",\n    \"search_criteria\": 1,\n    \"sciname\": \"salmo salar\",\n    \"iso3c\": \"USA,CHL\"\n}\nThe final URL would be: /supplemental/query?table=production&cols_wanted=iso3c,method,year,live_weight_t&search_criteria=1&sciname=salmo salar&iso3c=USA,CHL\nHere is a sample response:\n[\n    {\n        \"iso3c\": \"CHL\",\n        \"sciname\": \"salmo salar\",\n        \"method\": \"aquaculture\",\n        \"habitat\": \"inland\",\n        \"live_weight_t\": 493.49,\n        \"year\": 2018\n    },\n    {\n        \"iso3c\": \"CHL\",\n        \"sciname\": \"salmo salar\",\n        \"method\": \"aquaculture\",\n        \"habitat\": \"marine\",\n        \"live_weight_t\": 660644.9,\n        \"year\": 2018\n    },\n    {\n        \"iso3c\": \"USA\",\n        \"sciname\": \"salmo salar\",\n        \"method\": \"aquaculture\",\n        \"habitat\": \"marine\",\n        \"live_weight_t\": 16107,\n        \"year\": 2018\n    }\n]"
  },
  {
    "objectID": "external-content/artis-api-making_requests.html#permission-to-use-artis-api",
    "href": "external-content/artis-api-making_requests.html#permission-to-use-artis-api",
    "title": "How to make Requests to the ARTIS API",
    "section": "",
    "text": "All API users require an API key to submit requests to the ARTIS API. Any requests not containing an API key will automatically be rejected. Please place your API key in the HTTP request header under the name “X-API-KEY”."
  },
  {
    "objectID": "external-content/artis-api-making_requests.html#summary",
    "href": "external-content/artis-api-making_requests.html#summary",
    "title": "How to make Requests to the ARTIS API",
    "section": "",
    "text": "HTTP Request Type\n\n\nEndpoint\n\n\nParameters\n\n\nResponse\n\n\n\n\nGET\n\n\n/snet/query\n\n\nRequired Parameters:\n\n\ncols_wanted (at least one value required): “exporter_iso3c”, “importer_iso3c”, “source_country_iso3c”, “hs6”, “sciname”, “habitat”, “method”, “dom_source”, “year”.\n\n\nweight_type (only one value can be selected): “product_weight_t” or “live_weight_t”\n\n\nstart_year: year from 1996-2020 (inclusive)\n\n\nend_year: year from 1996-2020 (inclusive)\n\n\nsearch_criteria: Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required.\n\nNote: start_year cannot be greater than end_year. For results for just 1 year, make start_year equal to end_year.\n\nOptional Parameters:\n\n\nSend in additional column names as parameters, with strings as values for the values you want to filter for.\n\n\n\n\n\nAn ID number corresponding to the job that has been submitted containing your query request.\n\n\nA summary of the data request sent to the endpoint.\n\n\n\n\n\nGET\n\n\n/consumption/query\n\n\nRequired Parameters:\n\n\ncols_wanted (at least one value required): “consumer_iso3c”, “exporter_iso3c”, “source_country_iso3”, “year”, “hs_version”, “dom_source”, “sciname”, “habitat”, “method”, “consumption_source”, “sciname_hs_modified”\n\n\nstart_year: year from 1996-2020 (inclusive)\n\n\nend_year: year from 1996-2020 (inclusive)\n\n\nsearch_criteria: Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required.\n\n\nOptional Parameters:\n\n\nSend in additional column names as parameters, with strings as values for the values you want to filter for.\n\n\n\n\n\n\n\n\nGET\n\n\n/jobs/status\n\n\nRequired Parameters:\n\n\nid: Job id number returned from an snet or consumption query\n\n\n\n\n\n\nstatus: job status (wait/active/completed etc)\n\n\nresult: a list of objects where each object would correspond to a row of results for the query submitted\n\n\n\n\n\n\nGET\n\n\n/supplemental\n\n\nRequired Parameters:\n\n\ntable: supplemental table name.\n\n\nvariable: column name within supplemental table.\n\n\n\n\nA list of strings representing unique values in the column requested.\n\n\n\n\nGET\n\n\n/supplemental/query\n\n\nRequired Parameters:\n\n\ntable: supplemental table name you want information from.\n\n\ncols_wanted: columns include in the table requested.\n\n\nsearch_criteria: Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required.\n\n\nOptional Parameters:\n\n\nSend in additional column names as parameters, with strings as values for the values you want to filter for.\n\n\n\n\nA list of object where each object represents a row of data in the supplemental table requested.\n\n\n\n\nAn indepth review of the API requests and responses is available below."
  },
  {
    "objectID": "external-content/artis-api-making_requests.html#getting-updates-on-jobs-that-have-been-submitted",
    "href": "external-content/artis-api-making_requests.html#getting-updates-on-jobs-that-have-been-submitted",
    "title": "How to make Requests to the ARTIS API",
    "section": "",
    "text": "To get an update on jobs that have been submitted send a GET request to the /jobs/status endpoint.\nNote: only requests to the /snet/query, /consumption/query, /baci/query, /production/query endpoints will generate a job ID. Requests to /supplemental endpoints will generate an immediate JSON response with data back to the requester.\nA correct request to the /jobs/status endpoint consists of 1 query parameter: - id (REQUIRED): a job ID number. This job ID was provided by the /snet/query endpoint.\nA response to the /jobs/status endpoint consists of 2 fields: - status: a string describing the status of the job submitted. Possible return values include: - completed: Job has been completed. - active: Job is currently being executed. - wait: Job is on the queue and is waiting to be executed by the server. - delayed: Job has been delayed due to current server constraints, and will be submitted into the job queue as soon as possible. - failed: Job has failed. Please review request that has been made and modify as appropriate. - results: an array of objects where each object corresponds to a row of results from the query provided. Note, only when the status is “completed” will results be a non-empty array. The specifics as to how each object is structured are outlined below.\nExample successful response:\n{\n    status: \"completed\",\n    results: [\n        {year: 2018, live_weight_t: 180}\n        {year: 2019, live_weight_t: 250},\n        {year: 2020, live_weight_t: 100}\n    ]\n}\nExample job is currently in being executed:\n{\n    status: \"active\",\n    results: []\n}\nExample job is currently waiting to be executed:\n{\n    status: \"wait\",\n    results: []\n}\nExample job failed:\n{\n    status: \"failed\",\n    results: []\n}"
  },
  {
    "objectID": "external-content/artis-api-making_requests.html#requests-for-main-artis-snet-data",
    "href": "external-content/artis-api-making_requests.html#requests-for-main-artis-snet-data",
    "title": "How to make Requests to the ARTIS API",
    "section": "",
    "text": "Note: The /snet/query endpoint only submits a job request and will not provide results from the ARTIS seafood trade network table. All requests to the /snet/query endpoint provide an immediate response with a job id, along with information about the API request that was submitted. You will still need to request the /jobs/status endpoint to get an update on the job submitted and if completed the results of the database request. Please refer to the /jobs endpoint documentation for more detail.\nThis section outlines how to make requests for data within the ARTIS snet table. Note that all requests require some kind of filtering criteria, if you would like to request the complete ARTIS snet data please send an email to [ENTER EMAIL HERE].\nA request for the main ARTIS snet table consists of 5 fields: - cols_wanted (REQUIRED): A string containing the names of the columns, comma separated without spaces, used to summarize the ARTIS data you are requesting. - weight_type (REQUIRED): A string either “live_weight_t” or “product_weight_t”, denoting what mass measurement you would like to use. Note you can only choose ONE weight_type. - start_year (REQUIRED): A year between 1996-2020 - end_year (REQUIRED): A year between 1996-2020 must be greater than or equal to start_year - search_criteria (REQUIRED): Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required. Note all optional parameters sent will be treated as filtering criteria if search_criteria is set to 1. If search_criteria is set to 0, then all optional parameters will be ignored.\n\n\nIf you wanted to send a request for all ARTIS snet data summarize by year you would send a GET request to the /snet/query endpoint, with the following parameters:\nNote: In this request we are NOT performing any filtering on the ARTIS snet (with exception of the start and end years).\n{\n    \"cols_wanted\": \"year\",\n    \"weight_type\": \"live_weight_t\",\n    \"start_year\": 2015,\n    \"end_year\": 2020,\n    \"search_criteria\": 0\n}\nThe endpoint with parameters would look like this:\n/snet/query?cols_wanted=year&weight_t=live_weight_t&start_year=2015&end_year=2020&search_criteria=0\nHere is a sample of the results list that would be returned:\n[\n    {\n        \"year\": 2020,\n        \"live_weight_t\": 44166033.00951629\n    },\n    {\n        \"year\": 2015,\n        \"live_weight_t\": 40891197.02143691\n    },\n    {\n        \"year\": 2018,\n        \"live_weight_t\": 46537958.721290946\n    },\n    {\n        \"year\": 2017,\n        \"live_weight_t\": 44763794.736794\n    },\n    {\n        \"year\": 2019,\n        \"live_weight_t\": 45180309.70194027\n    },\n    {\n        \"year\": 2016,\n        \"live_weight_t\": 41962089.008993454\n    }\n]\nIf you wanted to send the same request but only for US and China capture trade from 2017 - 2019 you would include the following parameters:\n{\n    \"cols_wanted\": \"exporter_iso3c,year\",\n    \"weight_type\": \"live_weight_t\",\n    \"start_year\": 2017,\n    \"end_year\": 2019,\n    \"search_criteria\": 1,\n    \"exporter_iso3c\": \"CHN,USA\",\n    \"method\": \"capture\"\n}\nThe final url would look like this:\n/snet/query?cols_wanted=exporter_iso3c,year&weight_type=live_weight_t&start_year=2017&end_year=2019&search_criteria=1&exporter_iso3c=CHN,USA&method=capture\nHere is a sample of the results list that would be returned:\n[\n    {\n        \"exporter_iso3c\": \"USA\",\n        \"year\": 2017,\n        \"live_weight_t\": 1975004.9836222837\n    },\n    {\n        \"exporter_iso3c\": \"CHN\",\n        \"year\": 2018,\n        \"live_weight_t\": 3086899.0878980905\n    },\n    {\n        \"exporter_iso3c\": \"USA\",\n        \"year\": 2019,\n        \"live_weight_t\": 1699555.4157051465\n    },\n    {\n        \"exporter_iso3c\": \"CHN\",\n        \"year\": 2019,\n        \"live_weight_t\": 2960335.342349926\n    },\n    {\n        \"exporter_iso3c\": \"CHN\",\n        \"year\": 2017,\n        \"live_weight_t\": 3256231.7707880796\n    },\n    {\n        \"exporter_iso3c\": \"USA\",\n        \"year\": 2018,\n        \"live_weight_t\": 1807505.2796781566\n    }\n]\nIf you wanted to explore bilateral trade relationships for a specific species (salmo salar) in 2019, you would send the following request:\n{\n    \"cols_wanted\": \"exporter_iso3c,importer_iso3c,year\",\n    \"weight_type\": \"live_weight_t\",\n    \"start_year\": 2019,\n    \"end_year\": 2019,\n    \"search_criteria\": 1,\n    \"sciname\": \"salmo salar\"\n}\nThe final URL would look like this: /snet/query?cols_wanted=exporter_iso3c,importer_iso3c,year&weight_type=live_weight_t&start_year=2019&end_year=2019&search_criteria=1&sciname=salmo salar\nHere is a sample of the results list that would be returned:\n[\n    {\n        \"exporter_iso3c\": \"AUS\",\n        \"importer_iso3c\": \"CHN\",\n        \"year\": 2019,\n        \"live_weight_t\": 7919.87229991926\n    },\n    {\n        \"exporter_iso3c\": \"BEL\",\n        \"importer_iso3c\": \"COG\",\n        \"year\": 2019,\n        \"live_weight_t\": 5205.84567743523\n    },\n    {\n        \"exporter_iso3c\": \"BEL\",\n        \"importer_iso3c\": \"NLD\",\n        \"year\": 2019,\n        \"live_weight_t\": 2997.68945007121\n    },"
  },
  {
    "objectID": "external-content/artis-api-making_requests.html#requests-for-consumption-data",
    "href": "external-content/artis-api-making_requests.html#requests-for-consumption-data",
    "title": "How to make Requests to the ARTIS API",
    "section": "",
    "text": "Note: The /consumption/query endpoint only submits a job request and will not provide results from the ARTIS consumption table. All requests to the /consumption/query endpoint provide an immediate response with a job id, along with information about the API request that was submitted. You will still need to request the /jobs/status endpoint to get an update on the job submitted and, if completed, the results of the database request. Please refer to the /jobs endpoint documentation for more detail.\nThis section outlines how to make requests for data within the ARTIS consumption table. Note that all requests require some kind of filtering criteria, if you would like to request the complete ARTIS consumption data please send an email to [ENTER EMAIL HERE].\nA request for the main ARTIS consumption table consists of 4 fields: - cols_wanted (REQUIRED): A string containing the names of the columns, comma separated without spaces, used to summarize the ARTIS data you are requesting. Note that all consumption results are returned in live weight equivalent tonnes. - start_year (REQUIRED): A year between 1996-2020 - end_year (REQUIRED): A year between 1996-2020 must be greater than or equal to start_year - search_criteria (REQUIRED): Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required. Note all optional parameters sent will be treated as filtering criteria if search_criteria is set to 1. If search_criteria is set to 0, then all optional parameters will be ignored.\n\n\nIf you wanted to send a request for all ARTIS consumption data summarized by year you would send a GET request to the /consumption/query endpoint, with the following parameters:\nNote: In this request we are NOT performing any filtering on the ARTIS snet (with exception of the start and end years).\n{\n    \"cols_wanted\": \"year\",\n    \"start_year\": 2015,\n    \"end_year\": 2020,\n    \"search_criteria\": 0\n}\nThe endpoint with parameters would look like this:\n/consumption/query?cols_wanted=year&start_year=2015&end_year=2020&search_criteria=0\n[\n    {\n        \"year\": 2015,\n        \"consumption_live_t\": 150051627.61692697\n    },\n    {\n        \"year\": 2016,\n        \"consumption_live_t\": 152188217.6477852\n    },\n    {\n        \"year\": 2017,\n        \"consumption_live_t\": 156667487.22123533\n    },\n    {\n        \"year\": 2018,\n        \"consumption_live_t\": 162506861.25704896\n    },\n    {\n        \"year\": 2019,\n        \"consumption_live_t\": 161828981.64790905\n    },\n    {\n        \"year\": 2020,\n        \"consumption_live_t\": 160382274.5401494\n    }\n]\nIf you wanted to explore consumption of a specific species (salmo salar) in 2019, you would send the following request:\n{\n    \"cols_wanted\": \"consumer_iso3c,year\",\n    \"start_year\": 2019,\n    \"end_year\": 2019,\n    \"search_criteria\": 1,\n    \"sciname\": \"salmo salar\"\n}\nThe final URL would look like this: /consumption/query?cols_wanted=consumer_iso3c,year&start_year=2019&end_year=2019&search_criteria=1&sciname=salmo%20salar\nHere is a sample of the results list that would be returned:\n[\n    {\n        \"consumer_iso3c\": \"URY\",\n        \"year\": 2019,\n        \"consumption_live_t\": 1268.9479577486022\n    },\n    {\n        \"consumer_iso3c\": \"USA\",\n        \"year\": 2019,\n        \"consumption_live_t\": 479888.35767397494\n    },\n    {\n        \"consumer_iso3c\": \"UZB\",\n        \"year\": 2019,\n        \"consumption_live_t\": 179.3410667585069\n    },\n    {\n        \"consumer_iso3c\": \"VCT\",\n        \"year\": 2019,\n        \"consumption_live_t\": 98.57772699218599\n    },\n    {\n        \"consumer_iso3c\": \"VEN\",\n        \"year\": 2019,\n        \"consumption_live_t\": 117.01599593460986\n    },\n    {\n        \"consumer_iso3c\": \"VNM\",\n        \"year\": 2019,\n        \"consumption_live_t\": 15550.574977568014\n    }\n]"
  },
  {
    "objectID": "external-content/artis-api-making_requests.html#requests-for-supplemental-data",
    "href": "external-content/artis-api-making_requests.html#requests-for-supplemental-data",
    "title": "How to make Requests to the ARTIS API",
    "section": "",
    "text": "This is section outline how to make requests for any of the supplemental data tables: - baci - countries - production - products - sciname\nYou can make 2 kinds of requests for data in supplemental tables: 1. Getting all the unique values for a specific table. These requests are made to the url: /supplemental/ - This type of request has two REQUIRED parameters: - table: A string corresponding to the name of the supplemental table you want information from. - variable: A string corresponding to the name of the column. Will return all unique values in this column. 2. Getting all rows based on a filtered search criteria. These requests are made to the url: /supplemental/query/ - This type of the request has the uses the following parameters: - table (REQUIRED): A string corresponding to the name of the supplemental table you want information from. - cols_wanted (REQUIRED): A string corresponding to the columns, comma separated, that will be returned for each row. - search_criteria (REQUIRED): Either 1 or 0. 1 notes that additional filtering criteria will be included, 0 notes no filter criteria is required. Note all optional parameters sent will be treated as filtering criteria if search_criteria is set to 1. If search_criteria is set to 0, then all optional parameters will be ignored.\n\n\nIf you wanted to get all scientific names for all the species / species groups in ARTIS you would send a request to the /supplemental endpoint with the following parameters:\n{\n    \"table\": \"sciname\",\n    \"variable\": \"sciname\"\n}\nThe final URL would be: /supplemental/?table=sciname&variable=sciname\nHere is sample response:\n{\n    \"sciname\": [\n        \"eucinostomus melanopterus\",\n        \"nassarius\",\n        \"merlucciidae\",\n        \"stellifer minor\",\n        \"malacocephalus occidentalis\",\n        \"patagonotothen ramsayi\",\n        \"pomadasys kaakan\",\n        \"carangoides malabaricus\"\n    ]\n}\nIf you wanted to get all common names for all the species / species groups in ARTIS, you would send a request to the /supplemental endpoint with the following parameters:\n{\n    \"table\": \"sciname\",\n    \"variable\": \"common_name\"\n}\nThe final URL would be: /supplemental/?table=sciname&variable=common_name\nHere is a sample response:\n\"common_name\": [\n        \"scomber mackerels nei\",\n        \"mango tilapia\",\n        \"ocellated wedge sole\",\n        \"spotted porcupinefish\",\n        \"shi drum\",\n        \"leopard fish\",\n        \"globose clam\",\n        \"geelbek croaker\"\n]\nIf you wanted to get all scientific, common names and ISSCAAP groups for a specific genus (for example thunnus) you would send a request to the /supplemental/query endpoint with the following parameters:\n{\n    \"table\": \"sciname\",\n    \"cols_wanted\": \"sciname,common_name,isscaap\",\n    \"search_criteria\": 1,\n    \"genus\": \"thunnus\"\n}\nThe final URL would be: /supplemental/query?table=sciname&cols_wanted=sciname,common_name,isscaap&search_criteria=1&genus=thunnus\nHere is a sample response:\n[\n    {\n        \"sciname\": \"thunnus\",\n        \"common_name\": \"tunas nei\",\n        \"isscaap\": \"Tunas, bonitos, billfishes\"\n    },\n    {\n        \"sciname\": \"thunnus alalunga\",\n        \"common_name\": \"albacore\",\n        \"isscaap\": \"Tunas, bonitos, billfishes\"\n    }\n]\nIf you wanted to get production of all USA and Chilean salmo salar by production method and year, you would send a request to the /supplemental/query endpoint with the following parameters:\n{\n    \"table\": \"production\",\n    \"cols_wanted\": \"iso3c,method,year,live_weight_t\",\n    \"search_criteria\": 1,\n    \"sciname\": \"salmo salar\",\n    \"iso3c\": \"USA,CHL\"\n}\nThe final URL would be: /supplemental/query?table=production&cols_wanted=iso3c,method,year,live_weight_t&search_criteria=1&sciname=salmo salar&iso3c=USA,CHL\nHere is a sample response:\n[\n    {\n        \"iso3c\": \"CHL\",\n        \"sciname\": \"salmo salar\",\n        \"method\": \"aquaculture\",\n        \"habitat\": \"inland\",\n        \"live_weight_t\": 493.49,\n        \"year\": 2018\n    },\n    {\n        \"iso3c\": \"CHL\",\n        \"sciname\": \"salmo salar\",\n        \"method\": \"aquaculture\",\n        \"habitat\": \"marine\",\n        \"live_weight_t\": 660644.9,\n        \"year\": 2018\n    },\n    {\n        \"iso3c\": \"USA\",\n        \"sciname\": \"salmo salar\",\n        \"method\": \"aquaculture\",\n        \"habitat\": \"marine\",\n        \"live_weight_t\": 16107,\n        \"year\": 2018\n    }\n]"
  },
  {
    "objectID": "external-content/artis-api-table_outlines.html",
    "href": "external-content/artis-api-table_outlines.html",
    "title": "Table Structures and References",
    "section": "",
    "text": "Main table linking trade records with production.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3C code for direct exporter country\n\n\nimporter_iso3c\nISO3C code for direct importer country\n\n\nsource_country_iso3c\nISO3C code for the country that produced the specific product\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”.\n\n\nhs6\nHS 6 digit code used to identify what product is being traded.\n\n\nsciname\nspecies name traded under the specific HS product and 6-digit code.\n\n\nhabitat\nclassifies whether the specific species’ habitat (marine/inland/unknown).\n\n\nmethod\ndefines method of production (aquaculture/capture/unknown).\n\n\nproduct_weight_t\nproduct weight in tonnes.\n\n\nlive_weight_t\nlive weight in tonnes.\n\n\nyear\nyear in which trade occured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexporter_iso3c\nimporter_iso3c\nsource_country_iso3c\ndom_source\nhs6\nsciname\nhabitat\nmethod\nproduct_weight_t\nlive_weight_t\nyear\n\n\n\n\nCAN\nUSA\nCAN\ndomestic export\n030212\noncorhynchus keta\nmarine\ncapture\n870.34\n1131.45\n2017\n\n\nCHL\nITA\nPER\nforeign export\n230120\nengraulis ringens\nmarine\ncapture\n344.889\n1026.11\n2017\n\n\n\nNote: - Domestic Export: An export where the specific product was produced in the same country as it was exported from. - Foreign Export: An export where a specific product is imported from a source country and then re-exported by another country. - Error Export: An export that cannot be explained by domestic or foreign export records nor production records.\n\n\n\n\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nsource_country_iso3c\nISO3C code for the country that produced the specific species\n\n\nexporter_iso3c\nISO3C code for the country that was the final exporter of the species before it arrived at the consuming country\n\n\nconsumer_iso3c\nISO3C code for the country consuming the species\n\n\nconsumption_source\n“domestic” or “foreign”, domestic consumption is when the country consumes species it produced itself. Foreign consumption is when a country consumes species imported from another country\n\n\nyear\nyear 1996-2020\n\n\ndom_source\nspecifies whether trade flow was “domestic” (exporting country produced the good that it is sending to the consuming country) or “foreign” (exporting country imported the good that it is sending to the consuming country)\n\n\nsciname\nspecies/species group name\n\n\nsciname_hs_modified\nmost resolved version of the species/species group name based on the underlying HS code that it was traded under\n\n\nhabitat\nproduction habitat\n\n\nmethod\nproduction method\n\n\nconsumption_live_t\nconsumption in live weight equivalent tonnes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource_country_iso3c\nexporter_iso3c\nconsumer_iso3c\nconsumption_source\nyear\ndom_source\nsciname\nsciname_hs_modified\nhabitat\nmethod\nconsumption_live_t\n\n\n\n\nCAN\nCAN\nUSA\nforeign\n2020\ndomestic\nsalmo salar\nmarine\ncapture\n200.00\n\n\n\nUSA\nNA\nUSA\ndomestic\n2020\nNA\nsalmo salar\nmarine\ncapture\n350.00\n\n\n\n\n\n\n\n\n\n\n\n\nThis table holds all the taxonomic information for all the species traded in the main ARTIS table.\n\n\n\nColumn Name\nDescription\n\n\n\n\nsciname\nSpecies name\n\n\ncommon_name\nCommon name to refer to species.\n\n\ngenus\nTaxonomic genus\n\n\nsubfamily\nTaxonomic subfamily\n\n\nfamily\nTaxonomic family\n\n\norder\nTaxonomic order\n\n\nclass\nTaxonomic class\n\n\nsuperclass\nTaxonomic superclass\n\n\nphylum\nTaxonomic phylum\n\n\nkingdom\nTaxonomic kingdom\n\n\nisscaap\nISSCAAP group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsciname\ncommon_name\ngenus\nsubfamily\nfamily\norder\nclass\nsuperclass\nphylum\nkingdom\nisscaap group\n\n\n\n\nsalmo salar\natlantic salmon\nsalmo\nsalmoninae\nsalmonidae\nsalmoniformes\nactinopteri\nosteichthyes\nchordata\nanimalia\nSalmons, trouts, smelts\n\n\n\n\n\n\n\n\nThis table has all FAO production records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3C code for the producin country\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nlive_weight_t\nLive weight in tonnes.\n\n\nyear\nYear species was produced.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niso3c\nsciname\nmethod\nhabitat\nlive_weight_t\nyear\n\n\n\n\nSWE\nabramis brama\ncapture\ninland\n7\n2006\n\n\n\n\n\n\n\n\nThis table contains information about what each HS 6-digit code represents.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs6\nHS 6-digit product code\n\n\ndescription\nProduct description\n\n\npresentation\nProduct form (fillet, whole, fats and oils, non-fish, non-fmp form, other body parts, other meat, livers and roes)\n\n\nstate\nProduct state (live, frozen, preserved, fresh, not for humans, reduced)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhs6\ndescription\npresentation\nstate\n\n\n\n\n030212\nFish; Pacific salmon (oncorhynchus spp.), Atlantic salmon (salmo salar), Danube salmon (hucho hucho), fresh or chilled (excluding fillets, livers, roes and other fish meat of heading no. 0304)\nwhole\nfresh"
  },
  {
    "objectID": "external-content/artis-api-table_outlines.html#main-artis-snet",
    "href": "external-content/artis-api-table_outlines.html#main-artis-snet",
    "title": "Table Structures and References",
    "section": "",
    "text": "Main table linking trade records with production.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3C code for direct exporter country\n\n\nimporter_iso3c\nISO3C code for direct importer country\n\n\nsource_country_iso3c\nISO3C code for the country that produced the specific product\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”.\n\n\nhs6\nHS 6 digit code used to identify what product is being traded.\n\n\nsciname\nspecies name traded under the specific HS product and 6-digit code.\n\n\nhabitat\nclassifies whether the specific species’ habitat (marine/inland/unknown).\n\n\nmethod\ndefines method of production (aquaculture/capture/unknown).\n\n\nproduct_weight_t\nproduct weight in tonnes.\n\n\nlive_weight_t\nlive weight in tonnes.\n\n\nyear\nyear in which trade occured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexporter_iso3c\nimporter_iso3c\nsource_country_iso3c\ndom_source\nhs6\nsciname\nhabitat\nmethod\nproduct_weight_t\nlive_weight_t\nyear\n\n\n\n\nCAN\nUSA\nCAN\ndomestic export\n030212\noncorhynchus keta\nmarine\ncapture\n870.34\n1131.45\n2017\n\n\nCHL\nITA\nPER\nforeign export\n230120\nengraulis ringens\nmarine\ncapture\n344.889\n1026.11\n2017\n\n\n\nNote: - Domestic Export: An export where the specific product was produced in the same country as it was exported from. - Foreign Export: An export where a specific product is imported from a source country and then re-exported by another country. - Error Export: An export that cannot be explained by domestic or foreign export records nor production records."
  },
  {
    "objectID": "external-content/artis-api-table_outlines.html#consumption-table",
    "href": "external-content/artis-api-table_outlines.html#consumption-table",
    "title": "Table Structures and References",
    "section": "",
    "text": "Column Name\nDescription\n\n\n\n\nsource_country_iso3c\nISO3C code for the country that produced the specific species\n\n\nexporter_iso3c\nISO3C code for the country that was the final exporter of the species before it arrived at the consuming country\n\n\nconsumer_iso3c\nISO3C code for the country consuming the species\n\n\nconsumption_source\n“domestic” or “foreign”, domestic consumption is when the country consumes species it produced itself. Foreign consumption is when a country consumes species imported from another country\n\n\nyear\nyear 1996-2020\n\n\ndom_source\nspecifies whether trade flow was “domestic” (exporting country produced the good that it is sending to the consuming country) or “foreign” (exporting country imported the good that it is sending to the consuming country)\n\n\nsciname\nspecies/species group name\n\n\nsciname_hs_modified\nmost resolved version of the species/species group name based on the underlying HS code that it was traded under\n\n\nhabitat\nproduction habitat\n\n\nmethod\nproduction method\n\n\nconsumption_live_t\nconsumption in live weight equivalent tonnes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource_country_iso3c\nexporter_iso3c\nconsumer_iso3c\nconsumption_source\nyear\ndom_source\nsciname\nsciname_hs_modified\nhabitat\nmethod\nconsumption_live_t\n\n\n\n\nCAN\nCAN\nUSA\nforeign\n2020\ndomestic\nsalmo salar\nmarine\ncapture\n200.00\n\n\n\nUSA\nNA\nUSA\ndomestic\n2020\nNA\nsalmo salar\nmarine\ncapture\n350.00"
  },
  {
    "objectID": "external-content/artis-api-table_outlines.html#sciname-table",
    "href": "external-content/artis-api-table_outlines.html#sciname-table",
    "title": "Table Structures and References",
    "section": "",
    "text": "This table holds all the taxonomic information for all the species traded in the main ARTIS table.\n\n\n\nColumn Name\nDescription\n\n\n\n\nsciname\nSpecies name\n\n\ncommon_name\nCommon name to refer to species.\n\n\ngenus\nTaxonomic genus\n\n\nsubfamily\nTaxonomic subfamily\n\n\nfamily\nTaxonomic family\n\n\norder\nTaxonomic order\n\n\nclass\nTaxonomic class\n\n\nsuperclass\nTaxonomic superclass\n\n\nphylum\nTaxonomic phylum\n\n\nkingdom\nTaxonomic kingdom\n\n\nisscaap\nISSCAAP group\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsciname\ncommon_name\ngenus\nsubfamily\nfamily\norder\nclass\nsuperclass\nphylum\nkingdom\nisscaap group\n\n\n\n\nsalmo salar\natlantic salmon\nsalmo\nsalmoninae\nsalmonidae\nsalmoniformes\nactinopteri\nosteichthyes\nchordata\nanimalia\nSalmons, trouts, smelts"
  },
  {
    "objectID": "external-content/artis-api-table_outlines.html#production-table",
    "href": "external-content/artis-api-table_outlines.html#production-table",
    "title": "Table Structures and References",
    "section": "",
    "text": "This table has all FAO production records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3C code for the producin country\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nlive_weight_t\nLive weight in tonnes.\n\n\nyear\nYear species was produced.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niso3c\nsciname\nmethod\nhabitat\nlive_weight_t\nyear\n\n\n\n\nSWE\nabramis brama\ncapture\ninland\n7\n2006"
  },
  {
    "objectID": "external-content/artis-api-table_outlines.html#products-table",
    "href": "external-content/artis-api-table_outlines.html#products-table",
    "title": "Table Structures and References",
    "section": "",
    "text": "This table contains information about what each HS 6-digit code represents.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs6\nHS 6-digit product code\n\n\ndescription\nProduct description\n\n\npresentation\nProduct form (fillet, whole, fats and oils, non-fish, non-fmp form, other body parts, other meat, livers and roes)\n\n\nstate\nProduct state (live, frozen, preserved, fresh, not for humans, reduced)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhs6\ndescription\npresentation\nstate\n\n\n\n\n030212\nFish; Pacific salmon (oncorhynchus spp.), Atlantic salmon (salmo salar), Danube salmon (hucho hucho), fresh or chilled (excluding fillets, livers, roes and other fish meat of heading no. 0304)\nwhole\nfresh"
  },
  {
    "objectID": "artis-run-model/run-aws.html#update-artis-model-scripts-and-model-inputs",
    "href": "artis-run-model/run-aws.html#update-artis-model-scripts-and-model-inputs",
    "title": "Running on AWS",
    "section": "Update ARTIS model scripts and model inputs",
    "text": "Update ARTIS model scripts and model inputs\n\nCopy the most up-to-date set of model inputs to the project root directory artis-hpc\nCopy the most up-to-date ARTIS R package folder and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package NAMESPACE file and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package DESCRIPTION file and place within artis-hpc/docker_image_files_original/\n\nIf running on a new Apple chip arm64:\n\nCopy arm64_venv_requirements.txt file from the root directory to the artis-hpc/docker_image_files_original/\nRename the file artis-hpc/docker_image_files_original/arm64_venv_requirements.txt to artis-hpc/docker_image_files_original/requirements.txt\nOpen artis-hpc/docker_image_files_original/run_artis_hs12.R and uncomment line 20.",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#technologies-used",
    "href": "artis-run-model/run-aws.html#technologies-used",
    "title": "Running on AWS",
    "section": "Technologies used",
    "text": "Technologies used\n\nTerraform\n\nThis is a set of code scripts that create all the AWS infrastructure needed for the ARTIS HPC\nDestroy all AWS infrastructure for the ARTIS HPC after the ARTIS model has finished (save on unnecessary costs)\n\nDocker\n\nThis is used to create a docker image that our HPC jobs will use to run the ARTIS model code\n\nPython\n\nThrough the docker and AWS python (boto3) clients, this will provide code that:\n\nPush all model input data to AWS S3\nBuild docker image needed that the AWS Batch jobs will need to run ARTIS model\nPush docker image to AWS ECR\nSubmit jobs to ARTIS HPC\nPull all model outputs data",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#installation",
    "href": "artis-run-model/run-aws.html#installation",
    "title": "Running on AWS",
    "section": "Installation",
    "text": "Installation\n\nHomebrew\nAWS CLI\nTerraform CLI\nPython\n\nPython packages\n\ndocker\nboto3\n\n\n\n\nHomebrew installation\n\nInstall homebrew by running the terminal command /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nClose existing terminal window where installation command was run and open a new terminal window\nConfirm homebrew has been installed, run terminal command brew --version, no error messsage should appear.\n\nIf after homebrew installation you get a message stating brew command not found:\n\nEdit zsh config file, run terminal command: vim ~/.zshrc\nType i to enter edit mode\nCopy paste this line into the file you opened: export PATH=/opt/homebrew/bin:$PATH\nPress Shift and :\nType wq\nPress enter\nSource new config file, run terminal command source ~/.zshrc\n\n\n\nAWS CLI installation\nFollowing instructions from AWS\nNote: If you already have AWS CLI installed please still confirm by following step 3 below. Both instructions should run without an error message.\nThe following instructions are for MacOS users:\n\nRun terminal command curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\nRun terminal command sudo installer -pkg AWSCLIV2.pkg -target /\nConfirm AWS CLI has been installed:\n\nRun terminal command which aws\nRun terminal command aws --version\n\n\n\n\nTerraform CLI installation\nNote: If you already have homebrew installed please confirm by running brew --version, no error message should occur.\nTo install terraform on MacOS we will be using homebrew. If you do not have homebrew installed on your computer please follow the installation instructions here, before continuing.\nBased on Terraform CLI installation instructions provided here.\n\nRun terminal command brew tap hashicorp/tap\nRun terminal command brew install hashicorp/tap/terraform\nRun terminal command brew update\nRun terminal command brew upgrade hashicorp/tap/terraform\n\nIf this has been unsuccessful you might need to install xcode command line tools, try:\n\nRun terminal command: sudo xcode-select --install",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#assumptions",
    "href": "artis-run-model/run-aws.html#assumptions",
    "title": "Running on AWS",
    "section": "Assumptions:",
    "text": "Assumptions:\n\nAn AWS root user was created (To create an AWS root user visit )\nAWS root user has created an admin user group with “AdministratorAccess” permissions.\nAWS root user has created IAM users\nAWS root user has add IAM users to admin group\nAWS IAM users have their AWS AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY\n\nTo create an AWS IAM user: - FIXIT: include screenshots for creating an IAM user with the correct admin permissions.\nNote: If you created ANY AWS RESOURCES for ARTIS manually please delete these before continuing. These resources should",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#aws-cli-setup",
    "href": "artis-run-model/run-aws.html#aws-cli-setup",
    "title": "Running on AWS",
    "section": "AWS CLI Setup",
    "text": "AWS CLI Setup\nFIXIT: Explain what each terminal command is doing\n\nRun terminal command: export AWS_ACCESS_KEY=[YOUR_AWS_ACCESS_KEY]\nRun terminal command: export AWS_SECRET_ACCESS_KEY=[YOUR_AWS_SECRET_ACCESS_KEY]\nRun terminal command: export AWS_REGION=us-east-1\nRun terminal command aws configure set aws_access_key_id \"[YOUR_AWS_ACCESS_KEY]\"\nRun terminal command aws configure set aws_secret_access_key \"[YOUR_AWS_SECRET_KEY]\"",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#python-installation",
    "href": "artis-run-model/run-aws.html#python-installation",
    "title": "Running on AWS",
    "section": "Python Installation",
    "text": "Python Installation\nNote: Please make sure that your terminal is currently in your working directory that should end in artis-hpc, by running the terminal command pwd.\n\nCreate a virtual environment, run terminal command:python3 -m venv venv\nOpen virtual environment, run terminal command: source venv/bin/activate\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nCheck that all python modules have been downloaded, run terminal command pip freeze and check that all modules in the requirements.txt file are included.\n\nIf an error occurs please follow these instructions:\n\nUpgrade your version of pip by running terminal command: pip install --upgrade pip\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nIf errors still occur install each python package in the requirements.txt file individually, run terminal command pip3 install [PACKAGE NAME] ie pip3 install urllib3.",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#creating-aws-infrastructure-with-a-setup-file",
    "href": "artis-run-model/run-aws.html#creating-aws-infrastructure-with-a-setup-file",
    "title": "Running on AWS",
    "section": "Creating AWS Infrastructure with a setup file",
    "text": "Creating AWS Infrastructure with a setup file\nNote: the initial_setup.py script will create all necessary AWS infrastructure, upload all model inputs to an AWS S3 bucket, and create and upload a docker image based on the ARTIS codebase. It will also submit jobs to the ARTIS HPC.\n\nOpen Docker Desktop\nTake note of any existing docker images and container relating to other projects, and delete all docker container relating to ARTIS, delete all docker images relating to ARTIS.\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command:\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: If you are using an Apple Silicone chip (M1, M2, M3, etc) your chip will be “arm64”, otherwise for intel chips it will be “x86”\n\n\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command:\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: This will create the docker image from scratch. If you have an existing docker image you would like to use include the -di [existing docker image name] with the command.\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name] -di [existing docker image name]:latest\nExamples: - If you are creating the docker image from scratch (If you change any R code for ARTIS you will have to recreate a docker image):\npython3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image\n\nIf you have an existing docker image (for example only need to re-upload a new set of model inputs):\n\npython3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image -di myname-artis-image:latest\nNote: If terraform states that it created all resources however when you log into the AWS console to confirm cannot see them, they have most likely been created as part of another account. Run terraform destroy -auto-approveon the command line. Confirmed you have followed the AWS CLI set up instructions with the correct set of keys (AWS access key and AWS secret access key).",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#installation-guide",
    "href": "artis-run-model/run-locally.html#installation-guide",
    "title": "Running Locally",
    "section": "Installation Guide",
    "text": "Installation Guide\nThis project uses Python 3.10.9 which can be downloaded here and RStudio which can be downloaded here.\nIt should take approximately 10 minutes to run this full installation.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#creating-python-virtual-environment",
    "href": "artis-run-model/run-locally.html#creating-python-virtual-environment",
    "title": "Running Locally",
    "section": "Creating python virtual environment",
    "text": "Creating python virtual environment\nNOTE: This protocol may not be successful for every individual local machine. The interaction in package versions and computer architecture (i.e. arm64 M1, M2 chips) may complicate this virtual environment set up. We are working on setting up a portable docker image to increase the reproducibility of this code.\n\nOpen the artis-model repository in RStudio.\nClick on the terminal tab.\nType pwd in the terminal.\nCopy the result of the “pwd” terminal command.\nType python3 -m venv [RESULT FROM pwd]/venv (ie. python3 -m venv /home/artis-model/venv)\nType source venv/bin/activate in terminal.\nType pip3 install qpsolvers in terminal.\nType pip3 install quadprog in terminal.\nType pip3 install cvxopt in terminal.\nConfirm you have successfully installed the packages qpsolvers, quadprog, cvxopt by running pip list.\nType deactivate in terminal.\nClick on the Console tab.\n\nNote: You only need to install the solvers the first time you run this code. Warnings about the latest version of pip may also appear during the installation - these are okay, but errors are not.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#r-installation-instructions",
    "href": "artis-run-model/run-locally.html#r-installation-instructions",
    "title": "Running Locally",
    "section": "R installation instructions",
    "text": "R installation instructions\n\nClick “Build” on the build tab on the top right hand side corner of RStudio.\nClick on the dropdown arrow in the “Install” subtab within the “Build” window.\nClick the option “Configure Build Tools…”\nMake sure options mirror the image below and click OK.\n\n\n\nClick on the dropdown arrow in the “Install” subtab and select the option “Clean and Install”",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#running-the-model-demo",
    "href": "artis-run-model/run-locally.html#running-the-model-demo",
    "title": "Running Locally",
    "section": "Running the model demo",
    "text": "Running the model demo\nRunning the demo for the ARTIS model should take approximately 10 minutes. To run the demo for ARTIS run the 02-artis-pipeline.R script and then run the 04-build-artis-timeseries.R script.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#outputs",
    "href": "artis-run-model/run-locally.html#outputs",
    "title": "Running Locally",
    "section": "Outputs",
    "text": "Outputs\nThe outputs of the demo will appear in the demo/outputs directory. Within this folder demo/outputs/custom_ts will contain all the final files that if run on the full model inputs would be used to create the results of the ARTIS research paper.\nPlease find below descriptions of main files: - demo/outputs/custom_ts/mid_custom_ts.csv: This is the demo version of the main ARTIS trade records table. - demo/outputs/custom_ts/summary_consumption_midpoint.csv: This is the demo version of the main ARTIS seafood consumption records table.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#methods-and-workflow",
    "href": "artis-run-model/run-locally.html#methods-and-workflow",
    "title": "Running Locally",
    "section": "Methods and Workflow",
    "text": "Methods and Workflow\n\nHigh level overview\nThe following diagrams describes how ARTIS trade records are obtained.\n  \n\n\nCode workflows\nThe following diagrams describe the how the codebase follows the workflow illustrated above.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#system-requirements",
    "href": "artis-run-model/run-locally.html#system-requirements",
    "title": "Running Locally",
    "section": "System Requirements",
    "text": "System Requirements\n\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Ventura 13.3.1\nR version 4.2.2\nR packages:\n\nreadxl 1.4.1\njanitor 2.1.0\ncountrycode 1.4.0\ndoParallel 1.0.17\niterators 1.0.14\nforeach 1.5.2\nslam 0.1-50\nMatrix 1.5-1\nmagrittr 2.0.3\ndata.table 1.14.6\nforcats 0.5.2\nstringr 1.5.0\ndplyr 1.0.10\npurrr 1.0.1\nreadr 2.1.3\ntidyr 1.2.1\ntibble 3.1.8\nggplot2 3.4.0\ntidyverse 1.3.2\nreticulate 1.26\n\nPython version 3.10.9\nPython packages:\n\ncvxopt 1.3.0\ndaqp 0.5.1\necos 2.0.12\nnumpy 1.24.3\nosqp 0.6.2.post9\npip 22.3.1\nqdldl 0.1.7\nqpsolvers 3.4.0\nquadprog 0.1.11\nscipy 1.10.1\nscs 3.2.3\nsetuptools 65.6.3",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-our-model/est-consumption.html",
    "href": "artis-our-model/est-consumption.html",
    "title": "Estimating Consuption",
    "section": "",
    "text": "Estimating Consuption\nThe species trade network and FAO production data were used to calculate national apparent consumption by scientific name, habitat, and method for each year.\nFirst, domestic production of products is estimated by multiplying production data by the corresponding estimated X matrix. We then calculate domestic consumption by subtracting domestic exports by HS code from domestic production of products. Domestic consumption by species is then derived based on the volume of domestic consumption and the estimated species composition for the associated code.\nForeign consumption represents the quantity of product imported that was consumed in country (i.e., not subsequently exported). To calculate foreign consumption, we subtract foreign exports from the quantity of processed imports. Processed imports represent the quantity of each product, by HS code, available after accounting for processing, by multiplying the appropriate estimated W by the import vector i. We convert processed imports to live weight by multiplying by the live weight conversion factor. To distinguish human consumable products, we filter out all processed product consumption of fishmeal, fish oil and ornamental products.\nFinally, we disaggregated foreign consumption of processed HS products to species. We assume the species and trade sourcing distribution of foreign consumption of a given code is proportional to the species distribution of the original imported HS products from which a final code was sourced based on the estimated species trade network. We therefore disaggregate foreign consumption by multiplying foreign consumption by the trade flow proportions of imports across all trade partners and species information.\nSince apparent consumption is based on a disappearance model, estimated values are subject to multiple sources of error. Due to discrepancies in production and trade reporting for select countries (e.g., as arises with joint ventures), a few countries had unrealistically large estimated per capita consumption. For country specific consumption estimates, we capped total per capita consumption to 100 kg, as this is slightly above the upper estimate FAOSTAT25. We then adjusted the supply by export partners, scientific name, production method, habitat, and source country for those countries proportionally. A second factor that influences estimated apparent consumption relates to the approach for removing non-human consumable products, particularly the domestic production and use of fishmeal. While we base our estimates on the species mix entering HS code 230120, this is estimated based on exports and domestic use patterns could diverge, leading to errors in the volume removed. Additionally, since many species can enter code 230120 and there is limited empirical data to inform volumes of species entering fishmeal by country, there is greater uncertainty in the exact species mix, and therefore greater uncertainty in the species volumes to exclude from direct human consumption calculations.\n\n\n\n\n Back to top",
    "crumbs": [
      "Our Model",
      "Estimating Consuption"
    ]
  },
  {
    "objectID": "artis-our-model/est-trade-net.html",
    "href": "artis-our-model/est-trade-net.html",
    "title": "Disaggregating Reported Trade",
    "section": "",
    "text": "Estimating species bilateral trade flows occurs in two steps:\n\nSolving the national production-trade mass balance\nConverting reported commodity trade flow estimates to species trade flow estimates based on the estimated species mix going into each domestic and foreign exported commodity.\n\n\n\n\n\n\nStylized representation of mass-balance problem\n\n\nWe start with the fact that exports must equal production and imports, minus consumption. Since exports are reported as commodities, we solve this mass balance problem in terms of commodities. Production data are reported for each species, so we estimate the elements of a matrix that represents the proportion of production going into each commodity. Since an imported commodity can be processed and exported as a different commodity, we also estimate the proportion of each import being converted into a different commodity. Then for a given country,\n\\(e = V_{1} \\circ X \\cdot p + V_{2} \\circ W \\cdot g - c + \\epsilon\\)\nIf n is the number of species and m is the number of commodities, then: \\(V_{1}\\) is a sparse (\\(m\\)×\\(n\\)) matrix with product conversion factors corresponding to the unknowns in \\(X\\); \\(X\\) is a sparse (\\(m\\)×\\(n\\)) matrix of the proportion of each species in each commodity; \\(p\\) is a vector of domestic species production (\\(n\\)×\\(1\\)); \\(V_{2}\\) is a sparse (\\(m\\)×\\(m\\)) matrix with product conversion factors corresponding to the entries of \\(W\\); \\(W\\) is a (\\(m\\)×\\(m\\)) matrix of the processed imported commodities; \\(g\\) be a vector of imports (\\(m\\)×\\(1\\)), \\(c\\) is a vector of domestic consumption (\\(m\\)×\\(1\\)), and; \\(\\epsilon\\) is a vector of error terms (\\(m\\)×\\(1\\)).\nWe compiled reported values for V_1, V_2, e, p and g, and estimate the entries of X, W, c, and ϵ. We first converted this problem to a system of linear equations. Using the property that vec(ABC)=(C^T⊗A)vec(B), we can create A_b=(y^T⊗D_m)D_V, where D_m is a diagonal matrix of ones, with dimension m and D_V is a diagonal matrix with the elements of vec(V). The vector of unknowns is then x_b=vec(Z). We then solve this system of equations with a quadratic optimization solver such that the mass balance equalities are satisfied, trade codes with higher species resolution in X are prioritized, the elements of X, W, and c are otherwise relatively even (i.e., we assume an even distribution of production among commodities unless the data suggests otherwise), that ϵ is as small as possible (i.e., minimize the error), and all unknowns are greater than or equal to zero.\nPositive error terms represent situations where reported production and imports cannot explain exports. This can occur due to under- or un-reported production or imports, over-reporting of exports, errors in the live weight conversion factors, or inconsistencies in the year production and trade are attributed to.\nWe solve the mass-balance problem for each country-year-HS version combination using the Python package “solve_qp.” The estimated species mixes in national production (X), processing of imports (W) and the error term (ϵ) are passed to the next stage of the analysis.\n\n\n\nFirst, we compute the mix of species going into each trade code for each country’s domestic exports. To do this, we reweight X so it represents the proportion of each species in each code rather than the proportion of production of a species going into each product. Each country’s estimated X matrix is multiplied by p to get the mass of each species in each commodity. The total mass of each commodity is found by summing all the species volume grouped by commodity and the proportion of each species within a commodity is then calculated by dividing all volumes by their respective commodity mass totals.\nEach country’s exports can be sourced from domestic production, imported products that are subsequently exported, with or without processing (i.e., foreign exports), or from an unknown source (i.e., error exports). Since the mix of these sources cannot be derived from the mass balance equation alone, we calculate a range for sourcing following33. We calculate the maximum possible domestic exports by taking the minimum between the domestic production and total exports. Similarly, we calculated the maximum volume of exports sourced from imports, by taking the minimum between each product’s imports (accounting for processing estimated by W) and exports. The minimum domestic exports are calculated as the minimum between production and the difference in exports and the maximum calculated foreign exports, with the remainder as error exports (minimum foreign exports are calculated in an analogous way). The above results represent midpoint estimates.\n\nmax domestic exportse_(domestic,max) = min(p_domestic domestic production,etotal exports)\nmax foreign exportse_(foreign,max) = min(importsg,total exportse)\nmin domestic exports e_(domestic,min)= min(pdomestic production,etotal exports - e_(foreign,max) max foreign exports)\ne_(foreign,min) min foreign exports = min(importsg,total exportse - me_(domestic,max) ax domestic exports)\ne_(domestic,mid) midpoint domestic exports = (e_(domestic,max) max domestic exports + e_(domestic,min) min domestic exports)/2\ne_(foreign,mid) midpoint foreign exports = (e_(foreign,max) max foreign exports + e_(foreign,min) min foreign exports)/2\n\nFor these three estimates (maximum, minimum and midpoint) we calculate the domestic and foreign weights by dividing domestic export values and foreign export values by total export. We then distribute each country’s exports into domestic, foreign and error exports by multiplying exports by domestic, foreign and error proportions. For each export source, we apply a different species mix to each HS code based on the estimated source country. For domestic exports, we use the exporting country’s estimated X matrix. For error exports, the geographical origin is unknown and may arise from unreported production, so we cannot meaningfully assign a species mix to the code. Consequently, we identify the lowest taxonomic resolution common to all species within the code and assign that name to the trade flow.\nFor foreign exports, we trace the origins back in the supply chain a maximum of three steps (i.e., producer to intermediate exporter to final exporter to final importer), with any remaining foreign export or flows less than 1 tonne left as “unknown” source. The small flows left unresolved comprise around 1% of total trade.\n\n\n\nDiagram of identifying source of exported products\n\n\nTo link an export of foreign origin to its source country, we use a reweighted version of W to estimate the original imported product codes and connect those to their source country, using a proportional breakdown of each country’s imports of that code. Foreign exports of one country that originated from foreign exports of another country are isolated and undergo the process above to identify the source country. The species mix for foreign trade flows are based on either the source country’s estimated X matrix or the method described above for error exports.\n\n\n\nDiagram of linking source country and species mix to ARTIS records\n\n\n\n\n\nStylized representation of conversion of product trade network to species trade network\n\n\n\n\n\nOnce the species trade flow network is built, we remove all volumes traded below 0.1 tonnes, as the multiplication by small proportions generates overly specific, and likely unrealistic, small flows.\nNext, to generate a complete time series, we need to compile estimates from across the HS versions. All HS versions are reported since they have been created, for example HS96 reports trade from 1996 until the present. However, the more recent HS versions generally include more specific trade codes and therefore are preferred over older versions. It takes a few years before an HS version is fully adopted, resulting in lower total trade volumes for the first few years an HS version is available compared to the previous HS versions. To provide the most accurate representation of trade, we create a continuous time series by adopting the most recent HS version available after its total trade has met up with the total trade reported under previous HS versions. This results in HS96 being used for 1996 - 2004, HS02 for 2004 - 2009, HS07 for 2010 - 2012 and HS12 for 2013 - 2020.\n\n\n\nComparison of trade totals by HS version\n\n\nTo check the reasonability of estimated trade flows, we first confirmed that all trade flows sum to the original BACI trade flows when grouped by exporter, importer, year, HS code and expressed as product weight. Note that some flows are slightly lower due to the 0.1 tonne threshold (maximum difference of 72 tonnes representing a percent difference of 0.19%). Second, we confirmed that the estimates from the mass balance problem satisfy the problem constraints. Third, we checked that domestic exports of species in live weight equivalent do not exceed production of that species. Fourth, we confirmed that exports of foreign source do not exceed imports of that species. Only 1.4% of cases across all years showed a country’s foreign export of a species exceeded the total import of that species.\n\n\n\n\n\n\nNote\n\n\n\nThe original text this section is based on is from Gephart et al. (2024) Nature Communications [add link]. Please reference that paper when referencing this information: [Insert reference]",
    "crumbs": [
      "Our Model",
      "Disaggregating Reported Trade"
    ]
  },
  {
    "objectID": "artis-our-model/est-trade-net.html#national-mass-balance",
    "href": "artis-our-model/est-trade-net.html#national-mass-balance",
    "title": "Disaggregating Reported Trade",
    "section": "",
    "text": "Stylized representation of mass-balance problem\n\n\nWe start with the fact that exports must equal production and imports, minus consumption. Since exports are reported as commodities, we solve this mass balance problem in terms of commodities. Production data are reported for each species, so we estimate the elements of a matrix that represents the proportion of production going into each commodity. Since an imported commodity can be processed and exported as a different commodity, we also estimate the proportion of each import being converted into a different commodity. Then for a given country,\n\\(e = V_{1} \\circ X \\cdot p + V_{2} \\circ W \\cdot g - c + \\epsilon\\)\nIf n is the number of species and m is the number of commodities, then: \\(V_{1}\\) is a sparse (\\(m\\)×\\(n\\)) matrix with product conversion factors corresponding to the unknowns in \\(X\\); \\(X\\) is a sparse (\\(m\\)×\\(n\\)) matrix of the proportion of each species in each commodity; \\(p\\) is a vector of domestic species production (\\(n\\)×\\(1\\)); \\(V_{2}\\) is a sparse (\\(m\\)×\\(m\\)) matrix with product conversion factors corresponding to the entries of \\(W\\); \\(W\\) is a (\\(m\\)×\\(m\\)) matrix of the processed imported commodities; \\(g\\) be a vector of imports (\\(m\\)×\\(1\\)), \\(c\\) is a vector of domestic consumption (\\(m\\)×\\(1\\)), and; \\(\\epsilon\\) is a vector of error terms (\\(m\\)×\\(1\\)).\nWe compiled reported values for V_1, V_2, e, p and g, and estimate the entries of X, W, c, and ϵ. We first converted this problem to a system of linear equations. Using the property that vec(ABC)=(C^T⊗A)vec(B), we can create A_b=(y^T⊗D_m)D_V, where D_m is a diagonal matrix of ones, with dimension m and D_V is a diagonal matrix with the elements of vec(V). The vector of unknowns is then x_b=vec(Z). We then solve this system of equations with a quadratic optimization solver such that the mass balance equalities are satisfied, trade codes with higher species resolution in X are prioritized, the elements of X, W, and c are otherwise relatively even (i.e., we assume an even distribution of production among commodities unless the data suggests otherwise), that ϵ is as small as possible (i.e., minimize the error), and all unknowns are greater than or equal to zero.\nPositive error terms represent situations where reported production and imports cannot explain exports. This can occur due to under- or un-reported production or imports, over-reporting of exports, errors in the live weight conversion factors, or inconsistencies in the year production and trade are attributed to.\nWe solve the mass-balance problem for each country-year-HS version combination using the Python package “solve_qp.” The estimated species mixes in national production (X), processing of imports (W) and the error term (ϵ) are passed to the next stage of the analysis.",
    "crumbs": [
      "Our Model",
      "Disaggregating Reported Trade"
    ]
  },
  {
    "objectID": "artis-our-model/est-trade-net.html#converting-the-product-trade-network-to-a-species-trade-network",
    "href": "artis-our-model/est-trade-net.html#converting-the-product-trade-network-to-a-species-trade-network",
    "title": "Disaggregating Reported Trade",
    "section": "",
    "text": "First, we compute the mix of species going into each trade code for each country’s domestic exports. To do this, we reweight X so it represents the proportion of each species in each code rather than the proportion of production of a species going into each product. Each country’s estimated X matrix is multiplied by p to get the mass of each species in each commodity. The total mass of each commodity is found by summing all the species volume grouped by commodity and the proportion of each species within a commodity is then calculated by dividing all volumes by their respective commodity mass totals.\nEach country’s exports can be sourced from domestic production, imported products that are subsequently exported, with or without processing (i.e., foreign exports), or from an unknown source (i.e., error exports). Since the mix of these sources cannot be derived from the mass balance equation alone, we calculate a range for sourcing following33. We calculate the maximum possible domestic exports by taking the minimum between the domestic production and total exports. Similarly, we calculated the maximum volume of exports sourced from imports, by taking the minimum between each product’s imports (accounting for processing estimated by W) and exports. The minimum domestic exports are calculated as the minimum between production and the difference in exports and the maximum calculated foreign exports, with the remainder as error exports (minimum foreign exports are calculated in an analogous way). The above results represent midpoint estimates.\n\nmax domestic exportse_(domestic,max) = min(p_domestic domestic production,etotal exports)\nmax foreign exportse_(foreign,max) = min(importsg,total exportse)\nmin domestic exports e_(domestic,min)= min(pdomestic production,etotal exports - e_(foreign,max) max foreign exports)\ne_(foreign,min) min foreign exports = min(importsg,total exportse - me_(domestic,max) ax domestic exports)\ne_(domestic,mid) midpoint domestic exports = (e_(domestic,max) max domestic exports + e_(domestic,min) min domestic exports)/2\ne_(foreign,mid) midpoint foreign exports = (e_(foreign,max) max foreign exports + e_(foreign,min) min foreign exports)/2\n\nFor these three estimates (maximum, minimum and midpoint) we calculate the domestic and foreign weights by dividing domestic export values and foreign export values by total export. We then distribute each country’s exports into domestic, foreign and error exports by multiplying exports by domestic, foreign and error proportions. For each export source, we apply a different species mix to each HS code based on the estimated source country. For domestic exports, we use the exporting country’s estimated X matrix. For error exports, the geographical origin is unknown and may arise from unreported production, so we cannot meaningfully assign a species mix to the code. Consequently, we identify the lowest taxonomic resolution common to all species within the code and assign that name to the trade flow.\nFor foreign exports, we trace the origins back in the supply chain a maximum of three steps (i.e., producer to intermediate exporter to final exporter to final importer), with any remaining foreign export or flows less than 1 tonne left as “unknown” source. The small flows left unresolved comprise around 1% of total trade.\n\n\n\nDiagram of identifying source of exported products\n\n\nTo link an export of foreign origin to its source country, we use a reweighted version of W to estimate the original imported product codes and connect those to their source country, using a proportional breakdown of each country’s imports of that code. Foreign exports of one country that originated from foreign exports of another country are isolated and undergo the process above to identify the source country. The species mix for foreign trade flows are based on either the source country’s estimated X matrix or the method described above for error exports.\n\n\n\nDiagram of linking source country and species mix to ARTIS records\n\n\n\n\n\nStylized representation of conversion of product trade network to species trade network",
    "crumbs": [
      "Our Model",
      "Disaggregating Reported Trade"
    ]
  },
  {
    "objectID": "artis-our-model/est-trade-net.html#network-post-estimation-processing",
    "href": "artis-our-model/est-trade-net.html#network-post-estimation-processing",
    "title": "Disaggregating Reported Trade",
    "section": "",
    "text": "Once the species trade flow network is built, we remove all volumes traded below 0.1 tonnes, as the multiplication by small proportions generates overly specific, and likely unrealistic, small flows.\nNext, to generate a complete time series, we need to compile estimates from across the HS versions. All HS versions are reported since they have been created, for example HS96 reports trade from 1996 until the present. However, the more recent HS versions generally include more specific trade codes and therefore are preferred over older versions. It takes a few years before an HS version is fully adopted, resulting in lower total trade volumes for the first few years an HS version is available compared to the previous HS versions. To provide the most accurate representation of trade, we create a continuous time series by adopting the most recent HS version available after its total trade has met up with the total trade reported under previous HS versions. This results in HS96 being used for 1996 - 2004, HS02 for 2004 - 2009, HS07 for 2010 - 2012 and HS12 for 2013 - 2020.\n\n\n\nComparison of trade totals by HS version\n\n\nTo check the reasonability of estimated trade flows, we first confirmed that all trade flows sum to the original BACI trade flows when grouped by exporter, importer, year, HS code and expressed as product weight. Note that some flows are slightly lower due to the 0.1 tonne threshold (maximum difference of 72 tonnes representing a percent difference of 0.19%). Second, we confirmed that the estimates from the mass balance problem satisfy the problem constraints. Third, we checked that domestic exports of species in live weight equivalent do not exceed production of that species. Fourth, we confirmed that exports of foreign source do not exceed imports of that species. Only 1.4% of cases across all years showed a country’s foreign export of a species exceeded the total import of that species.\n\n\n\n\n\n\nNote\n\n\n\nThe original text this section is based on is from Gephart et al. (2024) Nature Communications [add link]. Please reference that paper when referencing this information: [Insert reference]",
    "crumbs": [
      "Our Model",
      "Disaggregating Reported Trade"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aquatic Resource Trade in Species (ARTIS) Manual",
    "section": "",
    "text": "Food systems have become increasingly globalized, with over a quarter of all food now traded internationally. Seafood is among the most highly traded foods and it is becoming increasingly globalized, with trade doubling in recent decades. At the same time, seafood is now widely recognized as a critical source of nutrition. Thus, social and environmental threats to local seafood production, including environmental extremes, price impacts of market integration, networked risks, and increased availability of processed foods, must be evaluated in the context of global trade. These issues are paralleled by similar questions for other natural resources and are central to global food systems research. However, our collective understanding of the environmental and human outcomes of food system globalization is limited by a fundamental gap between production and trade data. We bridge this gap in the Aquatic Resource Trade in Species (ARTIS) database by providing the first global estimates of seafood species and nutrient trade flows from 1996–2020.\n\n\n\nConceptual representation of problem ARTIS aims to address\n\n\n\n\n\n\n\n\nMore resources\n\n\n\nSee the ARTIS website for additional information: ARTIS Website\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "artis-infrastructure/archive.html#purpose",
    "href": "artis-infrastructure/archive.html#purpose",
    "title": "Archived Versions (KNB)",
    "section": "Purpose",
    "text": "Purpose\n\nLong-term stable archive of model inpusts, model, database, and metadata\nOpen-acess distribution point\n\nARTIS uses the The Knowledge Network for Biocomplexity KNB data repository to archive and distribute stable releases of the model codebase and resulting database. Archiving, documenting and openly distributing ARTIS is a critical component in contributing to the larger open-science and reproducible science community. ARTIS uses KNB as an access point for anyone to download the ARTIS model codebase and clean inputs and output ARTIS database\nKNB is guided by FAIR (findable, accessible, interoperable, resuble) principles of data sharing and preservation and issues unique DOIs (digital object identifier) to each data package and every version of the package for long term access, transparency, and informative citations.",
    "crumbs": [
      "Distribution Infrastructure",
      "Archived Versions (KNB)"
    ]
  },
  {
    "objectID": "artis-infrastructure/archive.html#further-details-resources",
    "href": "artis-infrastructure/archive.html#further-details-resources",
    "title": "Archived Versions (KNB)",
    "section": "Further details & resources",
    "text": "Further details & resources\nKNB is a member of DataONE (Data Observation Network for Earth); a network of data repositories. KNB uses EML (Ecological Metadata Language) to document objects within a data packages which can be created via the website GUI (graphical user interface) or through a series of R packages rdataone and arcticdatautils.\nNote: a user will need an ORCiD to log into KNB.\nFor additional resources on how to submit data to KNB see the links below from NCEAS (National Center for Ecological Analysis and Synthesis) who develops and maintains DataONE and KNB.\n\nKNB and ADC Data Team Training\nData Team Reference Guide",
    "crumbs": [
      "Distribution Infrastructure",
      "Archived Versions (KNB)"
    ]
  },
  {
    "objectID": "artis-infrastructure/index.html",
    "href": "artis-infrastructure/index.html",
    "title": "Distribution Infrastructure",
    "section": "",
    "text": "Several computing resources and infrastructure are used in the ARTIS data pipeline and distribution. We utilize cloud high-performance computing, cloud-based storage, and local machines to integrate and analyze aquatic resource trade data. These computing resources create the ARTIS database that is openly available at two access points: KNB data repository and the ARTIS website (under development).\nThe schematic below depicts how components of the ARTIS pipeline fit together.\n\nBlue boxes – processes on local machines\ngreen boxes – access points\nyellow boxes – cloud HPC\npurple – cloud database storage\n\n\n\n\n\nOriginal Raw Data\n\nFAO global production, UN Comtrade, BACI, Sea Around Us, FishBase, and SeaLifeBase data contained in the artis-model/model_inputs_raw/ directory\n\nClean Data\n\nData cleaned and standardized in R and Python scripts and used in artis-model/model_inputs/ directory\n\nARTIS Model\n\nA model that estimates the aquatic resource trade flows and consumption\n\nARTIS Outputs\n\nModel results of trade flows and consumption tables stored in the ARTIS database found in the artis-model/outputs/ directory\n\npgAdmin\n\nOpensource application for managing PostgreSQL (a open source object-relational database system)\n\nKNB\n\nKnowledge Network for Biocomplexity data repository - database and model distribution point\n\n\n\nARTIS dataset - https://doi.org/10.5063/F1CZ35N7\nARTIS inputs and model - https://doi.org/10.5063/F1862DXT\n\n\nHeroku\n\nCloud hosting platform supporting the ARTIS API - being migrated to Vercel\n\n\n\n\n\n\nartis-model\n\nRelease version of model code link here\n\nartis-development\n\nPrivate development version of model code link here\n\nartis-hpc\n\nHigh-performance computing scripts for running model on Amazon Web Services link here\n\nartis-database\n\nScripts for creating and maintaining the Postgres database in the cloud and locally link here\n\nartis-API\n\nScripts for creating and maintaining the ARTIS API to website link here\n\nknb-submit\n\nScripts for submitting data to the KNB data repository link here",
    "crumbs": [
      "Distribution Infrastructure"
    ]
  },
  {
    "objectID": "artis-infrastructure/index.html#pipeline-component-details",
    "href": "artis-infrastructure/index.html#pipeline-component-details",
    "title": "Distribution Infrastructure",
    "section": "",
    "text": "Original Raw Data\n\nFAO global production, UN Comtrade, BACI, Sea Around Us, FishBase, and SeaLifeBase data contained in the artis-model/model_inputs_raw/ directory\n\nClean Data\n\nData cleaned and standardized in R and Python scripts and used in artis-model/model_inputs/ directory\n\nARTIS Model\n\nA model that estimates the aquatic resource trade flows and consumption\n\nARTIS Outputs\n\nModel results of trade flows and consumption tables stored in the ARTIS database found in the artis-model/outputs/ directory\n\npgAdmin\n\nOpensource application for managing PostgreSQL (a open source object-relational database system)\n\nKNB\n\nKnowledge Network for Biocomplexity data repository - database and model distribution point\n\n\n\nARTIS dataset - https://doi.org/10.5063/F1CZ35N7\nARTIS inputs and model - https://doi.org/10.5063/F1862DXT\n\n\nHeroku\n\nCloud hosting platform supporting the ARTIS API - being migrated to Vercel",
    "crumbs": [
      "Distribution Infrastructure"
    ]
  },
  {
    "objectID": "artis-infrastructure/index.html#github-code-repositories",
    "href": "artis-infrastructure/index.html#github-code-repositories",
    "title": "Distribution Infrastructure",
    "section": "",
    "text": "artis-model\n\nRelease version of model code link here\n\nartis-development\n\nPrivate development version of model code link here\n\nartis-hpc\n\nHigh-performance computing scripts for running model on Amazon Web Services link here\n\nartis-database\n\nScripts for creating and maintaining the Postgres database in the cloud and locally link here\n\nartis-API\n\nScripts for creating and maintaining the ARTIS API to website link here\n\nknb-submit\n\nScripts for submitting data to the KNB data repository link here",
    "crumbs": [
      "Distribution Infrastructure"
    ]
  },
  {
    "objectID": "artis-faq.html",
    "href": "artis-faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "The ARTIS model disaggregates reported aquatic resource products (e.g., frozen salmon filets) into species/species groups based on two pieces of information: 1) reported production data and 2) the lowest taxonomic resolution of the code.\nFor the first piece, the mass balance portion of the model estimates the species/species group mix within each domestically-produced product code. We also estimate the processing and re-export of imported products. This allows us to estimate the share of exports sourced from domestic versus foreign production and by using the bilateral trade data, we can trace products back to the country of harvest. The species/species group mix within the code can then be assigned. Consequently, if a country reports production of “bivalvia,” then “bivalvia” will appear in the trade network.\nFor the second source of taxonomic information, we look at all species that fall within a code and identify the lowest common taxonomic name. All exports identified as “error” receive this name. We also have an additional column that can be joined to ARTIS so that these names can be used in place of the model specified name when the lowest taxonomic name for the code is more detailed than the name based on the production data. Note though that if these improved names are used, domestic exports can exceed domestic production. These names (sciname_hs_modified) can be obtained for the trade data by joining the appropriate additional table by the original scientific name, the code, and the HS version. The column “sciname_hs_modified” is provided in the consumption table.\n\n\n\nReported trade data can (and often does) disagree across sources and it is generally not easy to determine why. Recall that ARTIS represents a disaggregation of BACI data. BACI is a reconciled version of UN Comtrade data, which is the official international database for bilateral trade. A given country’s trade reported to UN Comtrade can differ from what appears in BACI due to the method for reconciling the mirror trade records (see Data or CEPII BACI for more).\nIn addition to differences between BACI and UN Comtrade, sometimes national statistics available through a government’s website differ from what appears in UN Comtrade. There are likely multiple reasons why this could occur and we cannot explain these in all cases. A few factors that can be relevant include differences in the national trade codes (which often expand upon the 6-digit HS system), differences in when the data was last updated (which is particularly relevant for trade data for recent years) and how national statistics report products in-transit through the country.\nAnother source of trade data is bill of ladings data, which is generally not publicly available (though it can be purchased). Bill of ladings data includes finer spatial and temporal resolution and also includes information on the companies involved in trade. We have observed some large differences in the bill of ladings data that we have worked with compared to national and international statistics. Understanding for any individual country and bill of ladings data source why requires researching the specifics of the data source itself (i.e., we cannot provide a general explanation for this).\nOne other point to confirm when comparing data sources is that both sources are reporting the values in the same units (e.g., gross product weight in kg) as there are often multiple weight fields, in addition to value.\n\n\n\nWhen developing the ARTIS model, we assume that countries report data as it should be reported under existing international standards. According to the Coordinating Working Party on Fishery Statistics, catch and landings should be assigned to the country of the flag flown by the fishing vessel irrespective of the location of the fishing. This means that production resulting from a country operating a fishing vessel in a foreign country’s territory should be recorded in the national statistics of the foreign fishing vessel. However, if the vessel is chartered by a company based in the home country or the vessel is fishing for the country under a joint venture contract or similar agreement and the operation is integral to the economy of the host country, this does not apply. Consequently, our estimates of source country generally represent who harvested or caught the aquatic resource regardless of where it was produced (i.e., distant water fishing would generally be attributed to the flag state). In cases of exceptions related to select chartered foreign vessels, joint ventures, or other similar agreements, catch by a foreign vessel but reported by the host country may not match trade reporting if catch does not move through the customs boundary. These instances generate excess apparent consumption.\n\n\n\nUnreported or underreported catch results in lower production data than what actually occurred. Since the ARTIS model is searching for a solution that explains a country’s exports while minimizing the error term (among other objectives), the first impact of un/under-reported catch is that the country’s apparent consumption will be lower. However, since no terms can go negative, if the exports still cannot be explained by the production and imports, then it will result in a positive error term for the product. It can be helpful to think about the mass-balance problem to understand the effects of changes in production data:\n\n\n\nDiagram representation of mass-balance problem solved in ARTIS model\n\n\n\n\n\nTrade statistics are managed by each territory and generally guided by the Kyoto Convention. For the purposes of trade data reporting, imports and exports represent all goods which add or subtract, respectively, from the stock of material resources within an economic territory, but not goods which merely pass through a country’s economic territory. The economic territory generally coincides with the customs territory, which refers to the territory in which the country’s custom laws apply. Goods which enter a country for processing are included within trade statistics. Goods which pass through a country “in transit,” including those which are transshipped, are not recommended to be reported in trade statistics, though there are exceptions and known instances where one country reports trade which is “in transit” but the partner does not, which creates discrepancies that are not corrected for within BACI. Fishery products from within the country, the country’s waters, or obtained by a vessel of that country are considered goods wholly produced in that country. Catch by foreign vessels and catch by national vessels on the high seas landed in a country’s ports are recorded as imports by the country the products are landed in and as exports by the foreign nation, where economically or environmentally significant. For further trade statistic guideline details, see International Merchandise Trade Statistics: Concepts and Definitions 2010.\n\n\n\nThere are two common causes of artificial spikes. The first is related to improved resolution in a country’s production data. If a country previously reported production at the genus level, for example, but then starts reporting production at the species level, it could create a spike in the trade associated with the species name. In this case, one could look for a corresponding drop in the genus level data.\nThe second reason is related to change in the HS code system. HS codes are updated every 5 years. To create a continuous time series, we default to selecting one HS version for each year. There are slight lags in reporting under each new HS version (see figure below), so our custom time series adopts the most recent version once the newer version’s reporting approximately sums to the previous version’s reporting. However, changes in the set of HS product codes can create artificial spikes in time series. For example, if a species is traded under a broad code, it may not be well identified in the model, but if that species then gets a more specific code under a new HS version, the model will be able to distinguish that it is in fact that species being traded.\n\n\n\nComparison of trade totals by HS version\n\n\n\n\n\nProducts table check why there are duplicates (usually caused by multiple presentations or states associated with one code, 030719, H5 probably HS12, 20 codes with 2 states or duplicates). Duplicates should be removed before joining.\n\n\n\nThe primary set of seafood products that are not included are those considered byproducts or co-products of another seafood product (e.g., caviar, shark fins, offal, etc.). This is necessary to avoid double counting. Since all products must be converted into its live weight equivalent to solve the mass-balance problem, converting both a primary product and its coproduct back to its full live weight equivalent would result in the fish being counted twice. Of course in principle, if one knew how often the primary product and coproduct are both produced (over space and time), then both products could be included without double counting. However, to the best of our knowledge such information is not currently available.\n\n\n\nPrice data generally refers to a single transaction, as the price of a product varies throughout a year (or month or even a day). True price data does exist for some products (e.g., through Urner Barry), but people sometimes mean they are looking for unit value data (value/volume) as a proxy for the average price over the period. This can be generated from UN Comtrade or BACI data and we do have an ancillary file containing these values. However, we have not yet included this information in the core database because the data has not been fully explored yet.\nAnother point to note when it comes to price (or unit value data) is that prices change along the supply chain, particularly with value added processing. So the unit value derived from trade data is not necessarily indicative of the price a landing or the final price consumers face.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#what-does-it-mean-when-a-scientific-name-is-not-a-true-species-name",
    "href": "artis-faq.html#what-does-it-mean-when-a-scientific-name-is-not-a-true-species-name",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "The ARTIS model disaggregates reported aquatic resource products (e.g., frozen salmon filets) into species/species groups based on two pieces of information: 1) reported production data and 2) the lowest taxonomic resolution of the code.\nFor the first piece, the mass balance portion of the model estimates the species/species group mix within each domestically-produced product code. We also estimate the processing and re-export of imported products. This allows us to estimate the share of exports sourced from domestic versus foreign production and by using the bilateral trade data, we can trace products back to the country of harvest. The species/species group mix within the code can then be assigned. Consequently, if a country reports production of “bivalvia,” then “bivalvia” will appear in the trade network.\nFor the second source of taxonomic information, we look at all species that fall within a code and identify the lowest common taxonomic name. All exports identified as “error” receive this name. We also have an additional column that can be joined to ARTIS so that these names can be used in place of the model specified name when the lowest taxonomic name for the code is more detailed than the name based on the production data. Note though that if these improved names are used, domestic exports can exceed domestic production. These names (sciname_hs_modified) can be obtained for the trade data by joining the appropriate additional table by the original scientific name, the code, and the HS version. The column “sciname_hs_modified” is provided in the consumption table.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#why-dont-the-trade-totals-match-what-a-country-reports",
    "href": "artis-faq.html#why-dont-the-trade-totals-match-what-a-country-reports",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Reported trade data can (and often does) disagree across sources and it is generally not easy to determine why. Recall that ARTIS represents a disaggregation of BACI data. BACI is a reconciled version of UN Comtrade data, which is the official international database for bilateral trade. A given country’s trade reported to UN Comtrade can differ from what appears in BACI due to the method for reconciling the mirror trade records (see Data or CEPII BACI for more).\nIn addition to differences between BACI and UN Comtrade, sometimes national statistics available through a government’s website differ from what appears in UN Comtrade. There are likely multiple reasons why this could occur and we cannot explain these in all cases. A few factors that can be relevant include differences in the national trade codes (which often expand upon the 6-digit HS system), differences in when the data was last updated (which is particularly relevant for trade data for recent years) and how national statistics report products in-transit through the country.\nAnother source of trade data is bill of ladings data, which is generally not publicly available (though it can be purchased). Bill of ladings data includes finer spatial and temporal resolution and also includes information on the companies involved in trade. We have observed some large differences in the bill of ladings data that we have worked with compared to national and international statistics. Understanding for any individual country and bill of ladings data source why requires researching the specifics of the data source itself (i.e., we cannot provide a general explanation for this).\nOne other point to confirm when comparing data sources is that both sources are reporting the values in the same units (e.g., gross product weight in kg) as there are often multiple weight fields, in addition to value.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#how-is-distant-water-fishing-captured",
    "href": "artis-faq.html#how-is-distant-water-fishing-captured",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "When developing the ARTIS model, we assume that countries report data as it should be reported under existing international standards. According to the Coordinating Working Party on Fishery Statistics, catch and landings should be assigned to the country of the flag flown by the fishing vessel irrespective of the location of the fishing. This means that production resulting from a country operating a fishing vessel in a foreign country’s territory should be recorded in the national statistics of the foreign fishing vessel. However, if the vessel is chartered by a company based in the home country or the vessel is fishing for the country under a joint venture contract or similar agreement and the operation is integral to the economy of the host country, this does not apply. Consequently, our estimates of source country generally represent who harvested or caught the aquatic resource regardless of where it was produced (i.e., distant water fishing would generally be attributed to the flag state). In cases of exceptions related to select chartered foreign vessels, joint ventures, or other similar agreements, catch by a foreign vessel but reported by the host country may not match trade reporting if catch does not move through the customs boundary. These instances generate excess apparent consumption.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#what-is-the-effect-of-underreported-catch",
    "href": "artis-faq.html#what-is-the-effect-of-underreported-catch",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Unreported or underreported catch results in lower production data than what actually occurred. Since the ARTIS model is searching for a solution that explains a country’s exports while minimizing the error term (among other objectives), the first impact of un/under-reported catch is that the country’s apparent consumption will be lower. However, since no terms can go negative, if the exports still cannot be explained by the production and imports, then it will result in a positive error term for the product. It can be helpful to think about the mass-balance problem to understand the effects of changes in production data:\n\n\n\nDiagram representation of mass-balance problem solved in ARTIS model",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#what-is-the-effect-of-products-in-transit-through-an-intermediate-country",
    "href": "artis-faq.html#what-is-the-effect-of-products-in-transit-through-an-intermediate-country",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Trade statistics are managed by each territory and generally guided by the Kyoto Convention. For the purposes of trade data reporting, imports and exports represent all goods which add or subtract, respectively, from the stock of material resources within an economic territory, but not goods which merely pass through a country’s economic territory. The economic territory generally coincides with the customs territory, which refers to the territory in which the country’s custom laws apply. Goods which enter a country for processing are included within trade statistics. Goods which pass through a country “in transit,” including those which are transshipped, are not recommended to be reported in trade statistics, though there are exceptions and known instances where one country reports trade which is “in transit” but the partner does not, which creates discrepancies that are not corrected for within BACI. Fishery products from within the country, the country’s waters, or obtained by a vessel of that country are considered goods wholly produced in that country. Catch by foreign vessels and catch by national vessels on the high seas landed in a country’s ports are recorded as imports by the country the products are landed in and as exports by the foreign nation, where economically or environmentally significant. For further trade statistic guideline details, see International Merchandise Trade Statistics: Concepts and Definitions 2010.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#why-are-there-sudden-spikes",
    "href": "artis-faq.html#why-are-there-sudden-spikes",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "There are two common causes of artificial spikes. The first is related to improved resolution in a country’s production data. If a country previously reported production at the genus level, for example, but then starts reporting production at the species level, it could create a spike in the trade associated with the species name. In this case, one could look for a corresponding drop in the genus level data.\nThe second reason is related to change in the HS code system. HS codes are updated every 5 years. To create a continuous time series, we default to selecting one HS version for each year. There are slight lags in reporting under each new HS version (see figure below), so our custom time series adopts the most recent version once the newer version’s reporting approximately sums to the previous version’s reporting. However, changes in the set of HS product codes can create artificial spikes in time series. For example, if a species is traded under a broad code, it may not be well identified in the model, but if that species then gets a more specific code under a new HS version, the model will be able to distinguish that it is in fact that species being traded.\n\n\n\nComparison of trade totals by HS version",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#why-are-there-multiple-rows-for-a-single-hs-product-code-in-the-code-descriptions-metadata-file",
    "href": "artis-faq.html#why-are-there-multiple-rows-for-a-single-hs-product-code-in-the-code-descriptions-metadata-file",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Products table check why there are duplicates (usually caused by multiple presentations or states associated with one code, 030719, H5 probably HS12, 20 codes with 2 states or duplicates). Duplicates should be removed before joining.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#why-are-some-seafood-products-missing",
    "href": "artis-faq.html#why-are-some-seafood-products-missing",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "The primary set of seafood products that are not included are those considered byproducts or co-products of another seafood product (e.g., caviar, shark fins, offal, etc.). This is necessary to avoid double counting. Since all products must be converted into its live weight equivalent to solve the mass-balance problem, converting both a primary product and its coproduct back to its full live weight equivalent would result in the fish being counted twice. Of course in principle, if one knew how often the primary product and coproduct are both produced (over space and time), then both products could be included without double counting. However, to the best of our knowledge such information is not currently available.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#is-there-price-data-available",
    "href": "artis-faq.html#is-there-price-data-available",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Price data generally refers to a single transaction, as the price of a product varies throughout a year (or month or even a day). True price data does exist for some products (e.g., through Urner Barry), but people sometimes mean they are looking for unit value data (value/volume) as a proxy for the average price over the period. This can be generated from UN Comtrade or BACI data and we do have an ancillary file containing these values. However, we have not yet included this information in the core database because the data has not been fully explored yet.\nAnother point to note when it comes to price (or unit value data) is that prices change along the supply chain, particularly with value added processing. So the unit value derived from trade data is not necessarily indicative of the price a landing or the final price consumers face.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-infrastructure/api.html#about-artis",
    "href": "artis-infrastructure/api.html#about-artis",
    "title": "API",
    "section": "About ARTIS",
    "text": "About ARTIS\nFood systems have become increasingly globalized, with over a quarter of all food now traded internationally. Seafood is among the most highly traded foods and it is becoming increasingly globalized, with trade doubling in recent decades. At the same time, seafood is now widely recognized as a critical source of nutrition. Thus, social and environmental threats to local seafood production, including environmental extremes, price impacts of market integration, networked risks, and increased availability of processed foods, must be evaluated in the context of global trade. These issues are paralleled by similar questions for other natural resources and are central to global food systems research. However, our collective understanding of the environmental and human outcomes of food system globalization is limited by a fundamental gap between production and trade data. We bridge this gap in the Aquatic Resource Trade in Species (ARTIS) database by providing the first global estimates of seafood species and nutrient trade flows from 1996–2020.\n(Taken from https://artisdata.weebly.com/)",
    "crumbs": [
      "Distribution Infrastructure",
      "API"
    ]
  },
  {
    "objectID": "artis-infrastructure/api.html#description-files",
    "href": "artis-infrastructure/api.html#description-files",
    "title": "API",
    "section": "Description Files",
    "text": "Description Files\nThe files below contain the documentation for what types of requests (examples and sample responses) can be made to the ARTIS API.\n\ntable_outlines.md\n\nFormats and structures for all tables accessible from the ARTIS API\n\nmaking_requests.md\n\nStructure for how to make requests\nSample requests\nSample responses",
    "crumbs": [
      "Distribution Infrastructure",
      "API"
    ]
  },
  {
    "objectID": "artis-infrastructure/api.html#artis-database-tables",
    "href": "artis-infrastructure/api.html#artis-database-tables",
    "title": "API",
    "section": "ARTIS Database Tables",
    "text": "ARTIS Database Tables\n\nMain ARTIS snet\nSupplemental Tables\n\nsciname (species taxonomic information)\nproduction (FAO production data)\nproducts (HS code descriptions and classification)\ncountries (country metadata ie iso3 codes, regions)\nbaci (BACI trade bilateral trade data)",
    "crumbs": [
      "Distribution Infrastructure",
      "API"
    ]
  },
  {
    "objectID": "artis-infrastructure/sql.html#installations",
    "href": "artis-infrastructure/sql.html#installations",
    "title": "SQL Database",
    "section": "Installations",
    "text": "Installations\n\nDownload PostgreSQL: https://www.postgresql.org/download/\nDownload pgAdmin: https://www.pgadmin.org/download/",
    "crumbs": [
      "Distribution Infrastructure",
      "SQL Database"
    ]
  },
  {
    "objectID": "artis-infrastructure/sql.html#updating-cloud-heroku-database",
    "href": "artis-infrastructure/sql.html#updating-cloud-heroku-database",
    "title": "SQL Database",
    "section": "Updating Cloud (Heroku) Database",
    "text": "Updating Cloud (Heroku) Database\nReference: https://stackoverflow.com/questions/11769860/connect-to-a-heroku-database-with-pgadmin\n\nGathering Heroku database credentials:\n\nSign into the Heroku platform\nClick on the “artis” app\n\n\n\nClick on the “Resources” tab \nClick on “Heroku Postgres” in the list of resources available (this should open a new browser tab)\n\n\n\nClick on the “Credentials” tab\n\n\n\nClick on the arrow by “default 1 app” (this should provide a drop down set of options and details)\nClick on the “show” button to reveal the password for the database",
    "crumbs": [
      "Distribution Infrastructure",
      "SQL Database"
    ]
  },
  {
    "objectID": "artis-infrastructure/sql.html#setting-up-local-database",
    "href": "artis-infrastructure/sql.html#setting-up-local-database",
    "title": "SQL Database",
    "section": "Setting up local database",
    "text": "Setting up local database\n\nCreate a connection between the Heroku Database and local pgadmin:\n\nOpen pgAdmin\nRight click on the Server list on the left hand side\nSelect Servers &gt; Register &gt; Server (this will open a new smaller window with additional settings)\n\n\n\nEnter a server name (this will only be a local name reference) like “HEROKU_ARTIS”\n\n\n\nClick on the “Connection” tab\n\n\n\nThe details needed to fill in the following information can be found on the Heroku credentials page we found earlier:\nEnter the host name under the “Host name/address”\nEnter the port number under the “Port” field\nEnter the database name under the “Maintenance database” field\nEnter the user name under the “Username” field\nEnter the password (make sure to click reveal in Heroku) under the “Password”\nSelect Save password for future use\nClick on “Advanced” tab\n\n\n\nEnter the database name under the “DB restriction” field (There should now be a new database connection in your pgAdmin dropdown)\n\n\n\nTest connection to Heroku database:\n\nClick on server connection you created earlier (this will appear under the server name you wrote in earlier, ie “HEROKU_ARTIS”)\nClick the arrow by the server connection name (this should create provide a drop down with options and the 1 database)\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin)\n\n\n\nRun the SQL command “SELECT * FROM users;” (this should return immediately, with a table of the users that have access to the ARTIS API)\n\n\n\nAdd new data to cloud database\n\nFind the tables drop down under the database connection options on the left hand side of pgAdmin\n\n\n\n\nRepeat the following instructions for each table you want to update:\n\nIf the table already exists:\n\n\nRight click on the existing table and select the “Delete/Drop” option\n\n\n\n\n\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin)\nPaste and run the SQL script for creating the table you are interested in updating\nRight-click the “Tables” dropdown and select “Refresh”\n\n\n\nRight-click on the table you just re-created, and select “Import/Export Data” (this will open a new dialog box)\n\n\n\nConfirm the “Import” tab is selected and use the “Filename” field to find the table data you would like to include.\n\n\n\nSelect the “Options” tab\nConfirm the “Header” toggle is activated and the “NULL Strings” field has the value “NA”\n\n\n\nSelect the “Columns” tab\nMake sure the “record_id” column IS NOT part of the “Columns to import” field. If it is please delete this column from the list.",
    "crumbs": [
      "Distribution Infrastructure",
      "SQL Database"
    ]
  },
  {
    "objectID": "artis-infrastructure/sql.html#directory-and-file-structure",
    "href": "artis-infrastructure/sql.html#directory-and-file-structure",
    "title": "SQL Database",
    "section": "Directory and File Structure",
    "text": "Directory and File Structure\n\nprep_db_files.R\n\nTakes raw snet files to a database table\n\ncreate_sql_tables\n\nSQL files to create the different tables",
    "crumbs": [
      "Distribution Infrastructure",
      "SQL Database"
    ]
  },
  {
    "objectID": "artis-infrastructure/sql.html#database-structure",
    "href": "artis-infrastructure/sql.html#database-structure",
    "title": "SQL Database",
    "section": "Database Structure",
    "text": "Database Structure\n\nARTIS snet table\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3C code for direct exporter country\n\n\nimporter_iso3c\nISO3C code for direct importer country\n\n\nsource_country_iso3c\nISO3C code for the country that produced the specific product\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs6\nHS 6 digit code used to identify what product is being traded.\n\n\nsciname\nspecies name traded under the specific HS product and 6-digit code.\n\n\nhabitat\nclassifies whether the specific species’ habitat (marine/inland/unknown).\n\n\nmethod\ndefines method of production (aquaculture/capture/unknown).\n\n\nproduct_weight_t\nproduct weight in tonnes.\n\n\nlive_weight_t\nlive weight in tonnes.\n\n\nyear\nyear in which trade occured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexporter_iso3c\nimporter_iso3c\nsource_country_iso3c\ndom_source\nhs6\nsciname\nhabitat\nmethod\nproduct_weight_t\nlive_weight_t\nyear\n\n\n\n\nCAN\nUSA\nCAN\ndomestic export\n030212\noncorhynchus keta\nmarine\ncapture\n870.34\n1131.45\n2017\n\n\nCHL\nITA\nPER\nforeign export\n230120\nengraulis ringens\nmarine\ncapture\n344.889\n1026.11\n2017\n\n\n\nNote:\n\nDomestic Export: An export where the specific product was produced in the same country as it was exported from.\nForeign Export: An export where a specific product is imported from a source country and then re-exported by another country.\nError Export: An export that cannot be explained by domestic or foreign export records nor production records.\n\n\n\nProduction table\nThis table has all FAO production records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 code for the producing country\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nlive_weight_t\nLive weight in tonnes.\n\n\nyear\nYear species was produced.\n\n\n\n\nSample Production table entry\n\n\n\n\n\n\n\n\n\n\n\niso3c\nsciname\nmethod\nhabitat\nlive_weight_t\nyear\n\n\n\n\nSWE\nabramis brama\ncapture\ninland\n7\n2006\n\n\n\n\n\n\nSAU Production table\nThis table has all SAU production records for all countries in ARTIS for 1996 - 2019. Note all production is marine capture.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\ncountry_name_en\nProducing country name in english\n\n\ncountry_iso3_alpha\nProducing country ISO3 3 letter code\n\n\ncountry_iso3_numeric\nProducing country ISO3 numeric code\n\n\neez\nExclusive Economic Zone\n\n\nsector\neconomic sector\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nyear\nYear species was produced\n\n\nlive_weight_t\nLive weight in tonnes\n\n\n\n\n\nBaci table\nThis table has all BACI bilateral trade records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3 3 letter code for direct exporter country\n\n\nimporter_iso3c\nISO3 3 letter code for direct importer country\n\n\nhs6\nHS 6 digit code used to identify what product is being traded\n\n\nproduct_weight_t\nproduct weight in tonnes\n\n\nhs_version\nHS code version for the year used\n\n\nyear\nyear trade occured\n\n\n\n\n\nCountries table\nThis table contains metadata about countries in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code for country\n\n\ncountry_name\nCountry name in english\n\n\nowid_region\nCountry’s region as defined by Our World in Data\n\n\ncontinent\nCountry’s continent as defined by R countrycode package\n\n\n\n\n\nNutrient metadata table\nThis table contains the nutrient content per 100g of the species in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\nsciname\nspecies scientific name\n\n\ncalcium_mg\ncalcium content (mg) per 100 g of species\n\n\niron_mg\niron content (mg) per 100 g of species\n\n\nprotein_g\nprotein content (g) per 100 g of species\n\n\nfattyacids_g\nfatty acid content (g) per 100 g of species\n\n\nvitamina_mcg\nvitamin a content (mcg) per 100 g of species\n\n\nvitaminb12_mcg\nvitamin b12 content (mcg) per 100 g of species\n\n\nzinc_mg\nzinc content (mg) per 100 g of species\n\n\n\n\n\nComplete consumption table\nThis table contains consumption estimates from trade.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code of consuming country\n\n\nhs6\nHS 6 digit code used to identify what product is being consumed\n\n\nsciname\nspecies scientific name\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nsource_country_iso3c\nISO3 3 letter code of country of origin\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs_version\nHS code version for the year\n\n\npopulation\npopulation of consuming country (sourced from FAO)\n\n\nyear\nyear of consumption\n\n\n\n\n\nCode max resolved taxa table\nThis is a conversion table to resolve a scientific name from higher order taxa (family, order, class etc) to a more specific species based on the HS product and HS version used.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs_version\nHS code version for the year\n\n\nhs6\nspecies scientific name\n\n\nsciname\noriginal scientific name determined by production records\n\n\nsciname_hs_modified\nmore specific/resolved scientific name given hs version and hs product\n\n\n\n\n\n\nProducts table\nThis table contains information about what each HS 6-digit code represents.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs6\nHS 6-digit product code\n\n\ndescription\nProduct description\n\n\npresentation\nProduct form (fillet, whole, fats and oils, non-fish, non-fmp form, other body parts, other meat, livers and roes)\n\n\nstate\nProduct state (live, frozen, preserved, fresh, not for humans, reduced)\n\n\n\n\nSample products table entry\n\n\n\n\n\n\n\n\n\nhs6\ndescription\npresentation\nstate\n\n\n\n\n030212\nFish; Pacific salmon (oncorhynchus spp.), Atlantic salmon (salmo salar), Danube salmon (hucho hucho), fresh or chilled (excluding fillets, livers, roes and other fish meat of heading no. 0304)\nwhole\nfresh",
    "crumbs": [
      "Distribution Infrastructure",
      "SQL Database"
    ]
  },
  {
    "objectID": "explore-artis.html",
    "href": "explore-artis.html",
    "title": "exploreARTIS",
    "section": "",
    "text": "The exploreARTIS R package provides functions for filtering and visualizing trade and consumption data from the ARTIS (Aquatic Resource Trade In Species) database. This package is designed to facilitate and streamline investigation of the ARTIS database. Most functions are wrappers for ggplot2::ggplot() and can accept additional layers to further customize the figures, with the exception of exploreARTIS::plot_sankey() which is based on ggsankey.\n\n\n\n\nMac Users Run the following commands in terminal:\nbrew install pkg-config\nbrew install gdal\nOnce installed run the following command in the R console:\ninstall.packages(\"sf\", configure.args = \"--with-proj-lib=/usr/local/lib/\")\nWindows Users Please make sure you have Rtools installed first. Follow the instructions here. Then run the following command in the R console:\ninstall.packages(\"sf\")\n\n\n\nYou can install this package with the devtools package. The first time you do it you will have to run\ninstall.packages(\"devtools\")\nlibrary(devtools)\nThen, you can run\ndevtools::install_github(\"Seafood-Globalization-Lab/exploreARTIS\", dependencies = TRUE)\nAfter you install the exploreARTIS package, you can just load it with library(exploreARTIS). You will also need to reinstall the package whenever there are updates to the package code.\n\n\n\n\nARTIS data consists of the following variables:\n\nexporter_iso3c (string): Exporter Country ISO 3 code\nimporter_iso3c (string): Importer Country ISO 3 code\nsource_country_iso3c (string): Producer Country ISO 3 code\ndom_source (string): Domestic Export / Foreign Export / Error Export\nhs6 (string): 6-digit HS commodity code\nsciname (string): Species or Species group\nenvironment (string): Marine / Freshwater\nmethod (string): Capture / Aquaculture / Unknown\nproduct_weight_t (double): Product weight (tonnes)\nlive_weight_t (double): Live weight (tonnes)\nhs_version (string): version of HS codes\nyear (double): Year\n\n\n\n\n\n\nIf you have downloaded bulk ARTIS data, it is generally split into separate csv files for each HS version and year combination. This is because the combined file is large and slow to load, sometimes causing users’ R sessions to crash. If you would like to combine files into a single data frame, you will need to pick which HS version-year combinations you would like to include. Then, you can decide if you would like to filter down any of the variabales (e.g., keep select exporters, species, etc.). Once you have made these decisions, you can use the example script scripts/filter_bulk_artis.R as a starting place to loop through the desired ARTIS files and filter based on your specified criteria. Note that this is not a function, but rather an example script with comments to facilitate customization for your own project.\n\n\n\nOnce you have the ARTIS data frame you are using for your analysis, you may still want to filter it for specific visualizations. For example, you may be working with all trade for a given country but want to generate a plot for just one species. You can of course use any base R or tidyverse functions to filter the data, but we also provide a function in this package to filter any of the ARTIS variables: filter_artis(). The filtered data frame can then be passed to any visualization function.\n# loading library\nlibrary(exploreARTIS)\n\n# Filter ARTIS data to Chilean exports of Atlantic salmon in 2016-2020\nfilter_artis(mini_artis, year = 2016:2020, exporter = \"CHL\", species = \"salmo salar\")\n\n\n\n\nHere are examples of all types of plots that can be created with this package. mini_artis is dataframe with a subset of ARTIS data that is included in the exploreARTIS package.\n\n\nplot_bar() creates ranked bar plots, with bar categories indicated by the bar_group argument. The number of bars to display can be controlled with top_n. argument.\n# loading library\nlibrary(exploreARTIS)\n\n# Bar chart visualizing seafood trade volumes by exporter\nplot_bar(mini_artis, bar_group = \"exporter_iso3c\")\n\n\n\nBar charts can optionally be filled by an ARTIS variable.\n# loading library\nlibrary(exploreARTIS)\n\n# Bar chart visualizing seafood trade volumes by exporter and filling by export source\nplot_bar(mini_artis, bar_group = \"exporter_iso3c\", fill_type = \"dom_source\")\n\n\n\n\n\n\nplot_ts() creates time series line or area plots for any specified artis_var. The plot.type argument allows options of “line” (default) or “stacked” to change plot views. To keep the number of colors reasonable, the prop_flow_cutoff argument groups lines falling below the cut-off into “other” and this can be adjusted to show more or fewer lines/fills.\n# loading library\nlibrary(exploreARTIS)\n\nplot_ts(mini_artis, artis_var = \"exporter_iso3c\")\n\n\n\nA stacked line graph of all export partners in the ARTIS dataset\n# loading library\nlibrary(exploreARTIS)\n\nplot_ts(mini_artis, artis_var = \"exporter_iso3c\", plot.type = \"stacked\")\n\n\n\n\n\n\nplot_sankey() creates a sankey plot to display flows among nodes across columns. This function is flexible in allowing the user to specify which data columns should be used to produce the colunns of the sankey plot. This function includes the argument prop_flow_cutoff to control how many groups are included in “other” (which can help keep the larger flows readable). It also includes an argumen to drop the group “other” entirely if preferred.\n# loading library\nlibrary(exploreARTIS)\n\n# Sankey plot of all seafood trade\nplot_sankey(mini_artis, cols = c(\"sciname\", \"exporter_iso3c\", \"importer_iso3c\"))\n\n\n\n\n\n\nr plot_chord() creates a chord diagram for visualizing flows among countries/regions.\n# loading library\nlibrary(exploreARTIS)\n\n# Chord diagram of all seafood trade\nplot_chord(mini_artis, region_colors = region7_palette)\n\n\n\nChord Diagram - all seafood trade\n\n\nIndividual countries can be pulled out to highlight their trade by specifying the country/countries’ iso3c code(s) in the focal_country argument.\n# loading library\nlibrary(exploreARTIS)\n\n# Chord diagram of all seafood trade with Vietnam highlighted\nplot_chord(mini_artis, focal_country = \"VNM\", region_colors = region7_palette)\n\n\n\nChord Diagram - all seafood trade\n\n\n\n\n\nplot_map() creates maps that are optionally colored by country_fill and optionally include flow arrows colored by flow volume with flow_arrows. The number of arrows can be specified with n_flows.\n# loading library\nlibrary(exploreARTIS)\n\n# Map of top seafood exports and flows\nplot_map(mini_artis, country_fill = \"importer_iso3c\", flow_arrows = TRUE, arrow_label = \"Trade (live t)\", fill_label = \"Import (live t)\")\n\n\n\nIndividual country’s trade flows can be isolated by filtering the importer or exporter column before passing it to the plot function.\n# loading library\nlibrary(exploreARTIS)\n\n# Map of seafood exports from Chile\nmini_artis %&gt;% filter(exporter_iso3c == \"CHL\") %&gt;% plot_map(country_fill = \"importer_iso3c\", flow_arrows = TRUE, arrow_label = \"Trade (live t)\", fill_label = \"Import (live t)\")\n\n\n\n\n\n\nBoth plot_ts() and r plot_bar() allow facetting by an ARTIS variable with the facet_variable argument. If a facet variable is specified then facet_values must also be defined, either as a number (the number of facets to create) or a vector (the specific facets to create).\n# loading libraries\nlibrary(exploreARTIS)\n\n# Area plot of top importers facetted by method\nplot_ts(mini_artis, artis_var = \"importer_iso3c\", plot.type = \"stacked\", facet_variable = \"method\", facet_values = c(\"capture\", \"aquaculture\"))\n\n\n\n# loading libraries\nlibrary(exploreARTIS)\n\n# Bar plot of top importers facetted by method\nplot_bar(mini_artis, bar_group = \"importer_iso3c\", facet_variable = \"method\", facet_values = c(\"capture\", \"aquaculture\"))",
    "crumbs": [
      "exploreARTIS package"
    ]
  },
  {
    "objectID": "explore-artis.html#installation",
    "href": "explore-artis.html#installation",
    "title": "exploreARTIS",
    "section": "",
    "text": "Mac Users Run the following commands in terminal:\nbrew install pkg-config\nbrew install gdal\nOnce installed run the following command in the R console:\ninstall.packages(\"sf\", configure.args = \"--with-proj-lib=/usr/local/lib/\")\nWindows Users Please make sure you have Rtools installed first. Follow the instructions here. Then run the following command in the R console:\ninstall.packages(\"sf\")\n\n\n\nYou can install this package with the devtools package. The first time you do it you will have to run\ninstall.packages(\"devtools\")\nlibrary(devtools)\nThen, you can run\ndevtools::install_github(\"Seafood-Globalization-Lab/exploreARTIS\", dependencies = TRUE)\nAfter you install the exploreARTIS package, you can just load it with library(exploreARTIS). You will also need to reinstall the package whenever there are updates to the package code.",
    "crumbs": [
      "exploreARTIS package"
    ]
  },
  {
    "objectID": "explore-artis.html#artis-data-structure",
    "href": "explore-artis.html#artis-data-structure",
    "title": "exploreARTIS",
    "section": "",
    "text": "ARTIS data consists of the following variables:\n\nexporter_iso3c (string): Exporter Country ISO 3 code\nimporter_iso3c (string): Importer Country ISO 3 code\nsource_country_iso3c (string): Producer Country ISO 3 code\ndom_source (string): Domestic Export / Foreign Export / Error Export\nhs6 (string): 6-digit HS commodity code\nsciname (string): Species or Species group\nenvironment (string): Marine / Freshwater\nmethod (string): Capture / Aquaculture / Unknown\nproduct_weight_t (double): Product weight (tonnes)\nlive_weight_t (double): Live weight (tonnes)\nhs_version (string): version of HS codes\nyear (double): Year",
    "crumbs": [
      "exploreARTIS package"
    ]
  },
  {
    "objectID": "explore-artis.html#filtering-artis-data",
    "href": "explore-artis.html#filtering-artis-data",
    "title": "exploreARTIS",
    "section": "",
    "text": "If you have downloaded bulk ARTIS data, it is generally split into separate csv files for each HS version and year combination. This is because the combined file is large and slow to load, sometimes causing users’ R sessions to crash. If you would like to combine files into a single data frame, you will need to pick which HS version-year combinations you would like to include. Then, you can decide if you would like to filter down any of the variabales (e.g., keep select exporters, species, etc.). Once you have made these decisions, you can use the example script scripts/filter_bulk_artis.R as a starting place to loop through the desired ARTIS files and filter based on your specified criteria. Note that this is not a function, but rather an example script with comments to facilitate customization for your own project.\n\n\n\nOnce you have the ARTIS data frame you are using for your analysis, you may still want to filter it for specific visualizations. For example, you may be working with all trade for a given country but want to generate a plot for just one species. You can of course use any base R or tidyverse functions to filter the data, but we also provide a function in this package to filter any of the ARTIS variables: filter_artis(). The filtered data frame can then be passed to any visualization function.\n# loading library\nlibrary(exploreARTIS)\n\n# Filter ARTIS data to Chilean exports of Atlantic salmon in 2016-2020\nfilter_artis(mini_artis, year = 2016:2020, exporter = \"CHL\", species = \"salmo salar\")",
    "crumbs": [
      "exploreARTIS package"
    ]
  },
  {
    "objectID": "explore-artis.html#visualization-examples",
    "href": "explore-artis.html#visualization-examples",
    "title": "exploreARTIS",
    "section": "",
    "text": "Here are examples of all types of plots that can be created with this package. mini_artis is dataframe with a subset of ARTIS data that is included in the exploreARTIS package.\n\n\nplot_bar() creates ranked bar plots, with bar categories indicated by the bar_group argument. The number of bars to display can be controlled with top_n. argument.\n# loading library\nlibrary(exploreARTIS)\n\n# Bar chart visualizing seafood trade volumes by exporter\nplot_bar(mini_artis, bar_group = \"exporter_iso3c\")\n\n\n\nBar charts can optionally be filled by an ARTIS variable.\n# loading library\nlibrary(exploreARTIS)\n\n# Bar chart visualizing seafood trade volumes by exporter and filling by export source\nplot_bar(mini_artis, bar_group = \"exporter_iso3c\", fill_type = \"dom_source\")\n\n\n\n\n\n\nplot_ts() creates time series line or area plots for any specified artis_var. The plot.type argument allows options of “line” (default) or “stacked” to change plot views. To keep the number of colors reasonable, the prop_flow_cutoff argument groups lines falling below the cut-off into “other” and this can be adjusted to show more or fewer lines/fills.\n# loading library\nlibrary(exploreARTIS)\n\nplot_ts(mini_artis, artis_var = \"exporter_iso3c\")\n\n\n\nA stacked line graph of all export partners in the ARTIS dataset\n# loading library\nlibrary(exploreARTIS)\n\nplot_ts(mini_artis, artis_var = \"exporter_iso3c\", plot.type = \"stacked\")\n\n\n\n\n\n\nplot_sankey() creates a sankey plot to display flows among nodes across columns. This function is flexible in allowing the user to specify which data columns should be used to produce the colunns of the sankey plot. This function includes the argument prop_flow_cutoff to control how many groups are included in “other” (which can help keep the larger flows readable). It also includes an argumen to drop the group “other” entirely if preferred.\n# loading library\nlibrary(exploreARTIS)\n\n# Sankey plot of all seafood trade\nplot_sankey(mini_artis, cols = c(\"sciname\", \"exporter_iso3c\", \"importer_iso3c\"))\n\n\n\n\n\n\nr plot_chord() creates a chord diagram for visualizing flows among countries/regions.\n# loading library\nlibrary(exploreARTIS)\n\n# Chord diagram of all seafood trade\nplot_chord(mini_artis, region_colors = region7_palette)\n\n\n\nChord Diagram - all seafood trade\n\n\nIndividual countries can be pulled out to highlight their trade by specifying the country/countries’ iso3c code(s) in the focal_country argument.\n# loading library\nlibrary(exploreARTIS)\n\n# Chord diagram of all seafood trade with Vietnam highlighted\nplot_chord(mini_artis, focal_country = \"VNM\", region_colors = region7_palette)\n\n\n\nChord Diagram - all seafood trade\n\n\n\n\n\nplot_map() creates maps that are optionally colored by country_fill and optionally include flow arrows colored by flow volume with flow_arrows. The number of arrows can be specified with n_flows.\n# loading library\nlibrary(exploreARTIS)\n\n# Map of top seafood exports and flows\nplot_map(mini_artis, country_fill = \"importer_iso3c\", flow_arrows = TRUE, arrow_label = \"Trade (live t)\", fill_label = \"Import (live t)\")\n\n\n\nIndividual country’s trade flows can be isolated by filtering the importer or exporter column before passing it to the plot function.\n# loading library\nlibrary(exploreARTIS)\n\n# Map of seafood exports from Chile\nmini_artis %&gt;% filter(exporter_iso3c == \"CHL\") %&gt;% plot_map(country_fill = \"importer_iso3c\", flow_arrows = TRUE, arrow_label = \"Trade (live t)\", fill_label = \"Import (live t)\")\n\n\n\n\n\n\nBoth plot_ts() and r plot_bar() allow facetting by an ARTIS variable with the facet_variable argument. If a facet variable is specified then facet_values must also be defined, either as a number (the number of facets to create) or a vector (the specific facets to create).\n# loading libraries\nlibrary(exploreARTIS)\n\n# Area plot of top importers facetted by method\nplot_ts(mini_artis, artis_var = \"importer_iso3c\", plot.type = \"stacked\", facet_variable = \"method\", facet_values = c(\"capture\", \"aquaculture\"))\n\n\n\n# loading libraries\nlibrary(exploreARTIS)\n\n# Bar plot of top importers facetted by method\nplot_bar(mini_artis, bar_group = \"importer_iso3c\", facet_variable = \"method\", facet_values = c(\"capture\", \"aquaculture\"))",
    "crumbs": [
      "exploreARTIS package"
    ]
  },
  {
    "objectID": "artis-licenses.html",
    "href": "artis-licenses.html",
    "title": "Citing & Licenses",
    "section": "",
    "text": "The Seafood Globalization Lab recognizes the importance of publishing open-access datasets to accelerate high-impact solutions for complex human and environmental challenges. We are committed to making our data available to the public and striving to following FAIR (Findability, Accessibility, Interoperability, and Reuse) principles for digital assets and Force11 data citation principles.\n\n\nPlease cite the ARTIS database as:\nJessica Gephart, Rahul Agrawal Bejarano, Althea Marks, & Kelvin Gorospe. (2024). Aquatic Resource Trade in Species (ARTIS). Knowledge Network for Biocomplexity. doi:10.5063/F1CZ35N7.\nPlease cite the ARTIS input data and model as:\nJessica Gephart, Rahul Agrawal Bejarano, Althea Marks, & Kelvin Gorospe. (2024). ARTIS input data and model. Knowledge Network for Biocomplexity. doi:10.5063/F1862DXT.\nCiting datasets can:\n\nIncrease transparency and reproducibility of the research.\nPublicize that data are available for reuse.\nAttribute credit to data creators when data are reused.\n\n\n\n\nThe ARTIS model and database are licensed under a Creative Commons Attribution 4.0 International License (CC BY) license, which allows others to:\n\nShare: Copy and redistribute the material in any medium or format.\nAdapt: Remix, transform, and build upon the material for any purpose, even commercially.\n\nAttribution must be given to the original source, ensuring that credit is provided for the work.\n\n\n\n\nFindable: Easily discoverable by both humans and computers. We use rich metadata and assign unique identifiers to our datasets.\nAccessible: Retrievable using standard protocols, ensuring that users can access our data in a straightforward manner.\nInteroperable: Compatible with other datasets and tools, allowing for integration and analysis across different platforms and studies.\nReusable: Well-documented and licensed to facilitate reuse and repurposing by the scientific community and beyond.\n\n\n\n\nThese principles promote:\n\nCredit and Attribution: Acknowledging the efforts of data creators and contributors.\nEvidence: Providing a clear link between data and the claims they support.\nUnique Identifiers: Ensuring that data can be reliably located and referenced.",
    "crumbs": [
      "Citing & Licenses"
    ]
  },
  {
    "objectID": "artis-licenses.html#how-to-cite",
    "href": "artis-licenses.html#how-to-cite",
    "title": "Citing & Licenses",
    "section": "",
    "text": "Please cite the ARTIS database as:\nJessica Gephart, Rahul Agrawal Bejarano, Althea Marks, & Kelvin Gorospe. (2024). Aquatic Resource Trade in Species (ARTIS). Knowledge Network for Biocomplexity. doi:10.5063/F1CZ35N7.\nPlease cite the ARTIS input data and model as:\nJessica Gephart, Rahul Agrawal Bejarano, Althea Marks, & Kelvin Gorospe. (2024). ARTIS input data and model. Knowledge Network for Biocomplexity. doi:10.5063/F1862DXT.\nCiting datasets can:\n\nIncrease transparency and reproducibility of the research.\nPublicize that data are available for reuse.\nAttribute credit to data creators when data are reused.",
    "crumbs": [
      "Citing & Licenses"
    ]
  },
  {
    "objectID": "artis-licenses.html#licenses",
    "href": "artis-licenses.html#licenses",
    "title": "Citing & Licenses",
    "section": "",
    "text": "The ARTIS model and database are licensed under a Creative Commons Attribution 4.0 International License (CC BY) license, which allows others to:\n\nShare: Copy and redistribute the material in any medium or format.\nAdapt: Remix, transform, and build upon the material for any purpose, even commercially.\n\nAttribution must be given to the original source, ensuring that credit is provided for the work.",
    "crumbs": [
      "Citing & Licenses"
    ]
  },
  {
    "objectID": "artis-licenses.html#fair-principles",
    "href": "artis-licenses.html#fair-principles",
    "title": "Citing & Licenses",
    "section": "",
    "text": "Findable: Easily discoverable by both humans and computers. We use rich metadata and assign unique identifiers to our datasets.\nAccessible: Retrievable using standard protocols, ensuring that users can access our data in a straightforward manner.\nInteroperable: Compatible with other datasets and tools, allowing for integration and analysis across different platforms and studies.\nReusable: Well-documented and licensed to facilitate reuse and repurposing by the scientific community and beyond.",
    "crumbs": [
      "Citing & Licenses"
    ]
  },
  {
    "objectID": "artis-licenses.html#force11-joint-declaration-of-data-citation-principles",
    "href": "artis-licenses.html#force11-joint-declaration-of-data-citation-principles",
    "title": "Citing & Licenses",
    "section": "",
    "text": "These principles promote:\n\nCredit and Attribution: Acknowledging the efforts of data creators and contributors.\nEvidence: Providing a clear link between data and the claims they support.\nUnique Identifiers: Ensuring that data can be reliably located and referenced.",
    "crumbs": [
      "Citing & Licenses"
    ]
  },
  {
    "objectID": "artis-licenses.html#footnotes",
    "href": "artis-licenses.html#footnotes",
    "title": "Citing & Licenses",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWilkinson, M., Dumontier, M., Aalbersberg, I. et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018 (2016). https://doi.org/10.1038/sdata.2016.18↩︎\nData Citation Synthesis Group: Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11; 2014 https://doi.org/10.25490/a97f-egyk↩︎",
    "crumbs": [
      "Citing & Licenses"
    ]
  },
  {
    "objectID": "artis-our-model/index.html",
    "href": "artis-our-model/index.html",
    "title": "Our Model",
    "section": "",
    "text": "Our Model\nTo estimate the aquatic food species trade network, we compiled and aligned data on fishery and aquaculture production, live weight conversion factors, and bilateral global trade. The data span the globe and encompass decades of changes in country and species names and product forms. Over 4000 live weight conversion factors were compiled and matched to 2000+ farmed and wild capture aquatic species which in turn were matched to 900+ traded seafood product descriptions. Though we include nonfood (e.g., fish meal, bait, and ornamental trade) production and trade in the database, we exclude this from the analysis of aquatic food production and consumption. We also exclude mammals, reptiles, fowl, or seaweeds, along with co-products (e.g., caviar, shark fins, and fish meat) to avoid double counting, from the model and resulting database.\nSpecies trade flow estimates occur in two steps. First, we take a mass balance approach, where each country’s seafood exports must equal the domestic production, plus imports, minus domestic consumption, after accounting for processing losses. For each country, we estimate the proportion of seafood production going into each possible commodity, the proportion of each imported commodity processed and exported, and the domestic consumption of each commodity. We then use these estimates with bilateral trade data to solve for the global species flows. This approach substantially improves upon previous efforts by estimating species-level trade, covering all production environments (marine and freshwater) and production methods (farmed and wild caught), and including the processing and export of imported products.\nARTIS also differs from other available trade databases and models in important ways. Databases presenting reported bilateral trade, such as UN Comtrade, do not include source country and are reported in terms of products which generally obscures the species and production method. Closely related or derived databases, such as BACI, thus have the same issues when trying to understand aquatic food origin and species (see below for more details on reported trade data). FAO also provides bilateral trade data on fishery products, with products at a greater disaggregation than what the 6-digit Harmonized System offers, though this still does not provide sourcing or full species/production method information and the data is currently only available back to 2019.\n\n\n\nOverview of the ARTIS data processing and model\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Our Model"
    ]
  },
  {
    "objectID": "artis-our-model/data-inputs.html",
    "href": "artis-our-model/data-inputs.html",
    "title": "Data",
    "section": "",
    "text": "Aquatic resource production comes from the Food and Agriculture Organization (FAO), which provides national capture and aquaculture production5. The Food and Agriculture Organization provides annual capture and aquaculture production data for around 240 countries, territories, or land areas from 1950 to 2020. The FAO data reports production in tonnes (live weight equivalent) of around 550 farmed and 1600 wild capture species and species groups. FAO production data consists primarily of official national statistics, with some verifiable supplemental information from academic reviews, consultant reports, and other specialist literature. Data reported by nations are checked by the FAO for consistency and questionable values are verified with the reporting offices. When countries fail to report production, FAO uses past values to estimate production. For the purposes of this analysis, we do not distinguish between nationally reported, and FAO estimated values.\nAccording to the Coordinating Working Party on Fishery Statistics, catch and landings should be assigned to the country of the flag flown by the fishing vessel irrespective of the location of the fishing. This means that production resulting from a country operating a fishing vessel in a foreign country’s territory should be recorded in the national statistics of the foreign fishing vessel. However, if the vessel is chartered by a company based in the home country or the vessel is fishing for the country under a joint venture contract or similar agreement and the operation is integral to the economy of the host country, this does not apply. Consequently, our estimates of source country generally represent who harvested or caught the aquatic resource regardless of where it was produced (i.e., distant water fishing would generally be attributed to the flag state). In cases of exceptions related to select chartered foreign vessels, joint ventures, or other similar agreements, catch by a foreign vessel but reported by the host country may not match trade reporting if catch does not move through the customs boundary. These instances generate excess apparent consumption.\n\n\nWe use the CEPII BACI world trade database, which is a reconciled version of the UN Comtrade database49. Trade data are reported to the UN by both importers and exporters following the Harmonized System (HS) codes. The HS trade code system organizes traded goods into a hierarchy, with the highest level represented by two-digit codes (e.g., Chapter 03 covers “Fish and Crustaceans, Molluscs and Other Aquatic Invertebrates”), which are broken down into 4-digit headings (e.g., heading 0301 covers “Live fish”), which are then subdivided into 6-digit subheadings (e.g., subheading 030111 covers “Live ornamental freshwater fish”). National statistics offices may further subdivide HS codes into 7- to 12-digit codes but since these are not standard across countries, the HS 6-digit codes are the most highly resolved trade codes available globally. HS codes are administered by the World Customs Organization, which updates the codes every five years. HS versions can be used from their introduction through the present, meaning that the HS 2002 version provides a time series of trade from 2002 to the present whereas the HS 2017 version only provides a time series back to 2017. Notably, HS version 2012 included major revisions to the HS codes relevant to fisheries and aquaculture products.\nCEPII reconciles discrepancies in mirror trade records, which occur in around 35% of observations (for all traded commodities), by first removing transportation costs and using a weighting scheme based on each country’s reporting reliability to average discrepancies in reported mirror flows. BACI data focuses on trade flows between individual countries since 1994 and therefore drops flows within some groups of countries (e.g., Belgium-Luxembourg) to ensure consistent geographies. The resulting data set covers trade for over 200 countries and 5,000 products. Further details on the BACI data set are available in49. While BACI resolves many data issues contained in the raw UN Comtrade database, it does not correct for all implausible trade flows, which can especially arise if one country misreports a value and the partner country does not report a value50. Further, there are instances where one country reports on trade that is optional to report, and the partner country does not. Here, we do not identify and re-estimate any values reported in BACI. Excessively large exports will generally result in high error terms, while high imports will result in high apparent consumption.\nTrade statistics are managed by each territory and generally guided by the Kyoto Convention. For the purposes of trade data reporting, imports and exports represent all goods which add or subtract, respectively, from the stock of material resources within an economic territory, but not goods which merely pass through a country’s economic territory. The economic territory generally coincides with the customs territory, which refers to the territory in which the country’s custom laws apply. Goods which enter a country for processing are included within trade statistics. Goods which pass through a country “in transit,” including those which are transshipped, are not recommended to be reported in trade statistics, though there are exceptions and known instances where one country reports trade which is “in transit” but the partner does not, which creates discrepancies that are not corrected for within BACI. Fishery products from within the country, the country’s waters, or obtained by a vessel of that country are considered goods wholly produced in that country. Catch by foreign vessels and catch by national vessels on the high seas landed in a country’s ports are recorded as imports by the country the products are landed in and as exports by the foreign nation, where economically or environmentally significant. For further trade statistic guideline details, see International Merchandise Trade Statistics: Concepts and Definitions 2010.\n\n\n\nGlobal trade data is reported in terms of the product weight. To convert from product weight (i.e., net weight) to the live weight equivalent, a live weight conversion factor must be applied for each HS code. Live weight conversion factors are sourced from the European Market Observatory for Fisheries and Aquaculture Products (EUMOFA) Annex 7, along with various national and international governmental report values. EUMOFA data reports live weight conversion factors by CN-8 codes, so the mean of the live weight conversion factors falling within each HS 6-digit code are used. The EUMOFA data assigns products primarily destined for industrial purposes (e.g., fish meal and fish oil), co-products (e.g., caviar) and live trade a value of zero. In this analysis, co-products retained a live weight conversion factor value of zero to avoid double counting, but live animal trade was assigned a live weight conversion factor of 1 and fish meal and fish oil was assigned an average value of 2.9853. Data compiled from national and international reports were categorized into taxa types (mollusks, crustaceans, fishes, and other aquatic invertebrates), FAO ISSCAAP groups, species or taxon name, type of processing, and country of processing.\nLive weight conversion factors applied to trade data introduce a source of uncertainty and error due to uncertainty in conversion factors is not reported and a single live weight conversion factor is often presented per code, regardless of the species or region of origin. This is a limitation given that there are geographical and temporal variation in live weight conversion factors due to differences in processing technology. Despite this limitation, EUMOFA data offers better documentation and alignment with HS commodity codes than other live weight conversion factor data sources and is updated annually, providing documentation for changes in live weight conversion factors. Additionally, by supplementing the EUMOFA data with the other reported values we can better capture specific species processing into various product forms and some regional variability.\nAll conversion factors were reported as live weight to product weight ratios. These conversion factors were mapped onto possible species to commodity or commodity to commodity conversions, described below. For commodity-to-commodity conversions, we estimate the conversion factors (i.e., processing loss rate) as the additional mass lost when converting from the live weight to the original product form relative to converting from live weight to the processed product form. This can be calculated as the live weight conversion factor for the original product form divided by the live weight factor for the processed product form. We assume that mass cannot be gained through processing and therefore impose a maximum value of one to this ratio.\n\n\n\nFor each country-year-HS version combination, we estimate the proportion of each species going into each commodity and the proportion of each imported commodity processed into each other commodity. Each species can only be converted into a subset of the commodities. For example, Atlantic salmon, Salmo salar, can be converted into whole frozen salmon or frozen salmon filets, but cannot be converted to a frozen tilapia filet. Similarly, each commodity can only be converted to a subset of other commodities through processing. For example, whole frozen salmon can be processed into frozen salmon filets, but not vice versa and neither salmon commodity can be converted to a tilapia commodity through processing. Defining possible conversions restricts the solution space to realistic results and improves estimation by reducing the number of unknowns. We describe this assignment process in detail below.\n\n\nSpecies production to commodity assignment is a many-to-many matching problem, wherein one commodity can consist of multiple species and one species can be converted to multiple commodities. All taxonomic names reported in the FAO production data were matched to HS 6-digit codes based on the code descriptions and HS system hierarchy. The first matching step required dividing all taxonomic groups into the broad commodity groups at the 4-digit level (fish, crustaceans, molluscs and aquatic invertebrates). Within each of these groups, taxonomic groups were matched based on 6 types of matching categories:\n1. Explicit taxa match - Scientific names are matched based on taxonomic information provided in the code description 2. NEC match - All remaining unmatched species within the 4-digit level are assigned to the “not elsewhere considered” (NEC) code 3. NEC by taxa match - When a code description signifies an NEC group, but limits this based on a taxonomic category (e.g., Salmonidae, N.E.C.), the NEC grouping occurs at this level, rather than the broad NEC match 4. Broad commodity match - Only the broad taxonomic groups inform this assignment since no further taxonomic information is provided 5. Aquarium trade match - Assigned to ornamental species trade based on species found in the aquarium/ornamental trade54 6. Fishmeal - Assigned to fishmeal codes if at least 1% of production goes to fishmeal production globally during the study period based on the end use designation from Sea Around Us production data55. Although an estimated 27% of fishmeal is derived from processing by-products3, the species, geographical, and temporal variation in that estimate is currently unknown. Consequently, fishmeal is currently treated as sourced from whole fish reduction. This does not affect the total trade or trade patterns of fishmeal but does result in an overestimate of the proportion of production going to fishmeal in cases where by-products are used.\nAfter all species are matched to the appropriate HS codes, we use the list of species to define codes as inland, marine, diadromous, or mixed. Higher order taxonomic groups are then only matched with HS codes that include their habitat. For example, production of inland actinopterygii is matched with codes that include inland species that fall within actinopterygii, but not with exclusively marine codes, even if they contain species that fall within actinopterygii.\n\n\n\nAs with the species to commodity assignment, the commodity-to-commodity assignment is a many-to-many data problem. Here, one commodity can be processed into multiple other commodities (i.e., frozen salmon can be processed into salmon filets or canned salmon), which also means one commodity could have come from multiple other commodities. To create these assignments, we established rules for which product transformations are technically possible. First, a product cannot transfer outside of its broad commodity group (e.g., fish, crustaceans, mollusc, aquatic invertebrate). Second, where a more refined species or species group was given (e.g., tunas, salmons, etc.) a product cannot be transformed outside that group. Third, products are classified in terms of their state (e.g., alive, fresh, frozen, etc.) and presentation (e.g., e.g., whole, fileted, salted/dried/preserved meats, reductions such as fish meal and fish oil, etc.) and cannot be converted into less processed forms (e.g., frozen salmon filets cannot turn into a frozen whole salmon). Fourth, specific commodities (i.e. mentioning specific species) and NEC commodities can become broad commodities (where appropriate), however broad commodities cannot become more specific or NEC commodities.\n\n\n\n\nThe FAO production and BACI trade datasets do not share the same set of countries and territories. For the production and trade data to balance, it is important for the set of territories falling under a given name to align across the datasets. To avoid instances where, for example, production is reported under a territory, but trade is reported under the sovereign nation, we generally group all territories with the sovereign nation. As countries gain independence, they are added as a trade partner in the database. Due to this country standardization circular flows may occur when a sovereign nation trades with their territory. These circular flows are filtered out of the standardized BACI trade flows (i.e., internal trade is not included).\n\n\n\n\n\n\nNote\n\n\n\nThe original text this section is based on is from Gephart et al. (2024) Nature Communications [add link]. Please reference that paper when referencing this information: [Insert reference]",
    "crumbs": [
      "Our Model",
      "Data"
    ]
  },
  {
    "objectID": "artis-our-model/data-inputs.html#bilateral-trade-data",
    "href": "artis-our-model/data-inputs.html#bilateral-trade-data",
    "title": "Data",
    "section": "",
    "text": "We use the CEPII BACI world trade database, which is a reconciled version of the UN Comtrade database49. Trade data are reported to the UN by both importers and exporters following the Harmonized System (HS) codes. The HS trade code system organizes traded goods into a hierarchy, with the highest level represented by two-digit codes (e.g., Chapter 03 covers “Fish and Crustaceans, Molluscs and Other Aquatic Invertebrates”), which are broken down into 4-digit headings (e.g., heading 0301 covers “Live fish”), which are then subdivided into 6-digit subheadings (e.g., subheading 030111 covers “Live ornamental freshwater fish”). National statistics offices may further subdivide HS codes into 7- to 12-digit codes but since these are not standard across countries, the HS 6-digit codes are the most highly resolved trade codes available globally. HS codes are administered by the World Customs Organization, which updates the codes every five years. HS versions can be used from their introduction through the present, meaning that the HS 2002 version provides a time series of trade from 2002 to the present whereas the HS 2017 version only provides a time series back to 2017. Notably, HS version 2012 included major revisions to the HS codes relevant to fisheries and aquaculture products.\nCEPII reconciles discrepancies in mirror trade records, which occur in around 35% of observations (for all traded commodities), by first removing transportation costs and using a weighting scheme based on each country’s reporting reliability to average discrepancies in reported mirror flows. BACI data focuses on trade flows between individual countries since 1994 and therefore drops flows within some groups of countries (e.g., Belgium-Luxembourg) to ensure consistent geographies. The resulting data set covers trade for over 200 countries and 5,000 products. Further details on the BACI data set are available in49. While BACI resolves many data issues contained in the raw UN Comtrade database, it does not correct for all implausible trade flows, which can especially arise if one country misreports a value and the partner country does not report a value50. Further, there are instances where one country reports on trade that is optional to report, and the partner country does not. Here, we do not identify and re-estimate any values reported in BACI. Excessively large exports will generally result in high error terms, while high imports will result in high apparent consumption.\nTrade statistics are managed by each territory and generally guided by the Kyoto Convention. For the purposes of trade data reporting, imports and exports represent all goods which add or subtract, respectively, from the stock of material resources within an economic territory, but not goods which merely pass through a country’s economic territory. The economic territory generally coincides with the customs territory, which refers to the territory in which the country’s custom laws apply. Goods which enter a country for processing are included within trade statistics. Goods which pass through a country “in transit,” including those which are transshipped, are not recommended to be reported in trade statistics, though there are exceptions and known instances where one country reports trade which is “in transit” but the partner does not, which creates discrepancies that are not corrected for within BACI. Fishery products from within the country, the country’s waters, or obtained by a vessel of that country are considered goods wholly produced in that country. Catch by foreign vessels and catch by national vessels on the high seas landed in a country’s ports are recorded as imports by the country the products are landed in and as exports by the foreign nation, where economically or environmentally significant. For further trade statistic guideline details, see International Merchandise Trade Statistics: Concepts and Definitions 2010.",
    "crumbs": [
      "Our Model",
      "Data"
    ]
  },
  {
    "objectID": "artis-our-model/data-inputs.html#live-weight-conversions",
    "href": "artis-our-model/data-inputs.html#live-weight-conversions",
    "title": "Data",
    "section": "",
    "text": "Global trade data is reported in terms of the product weight. To convert from product weight (i.e., net weight) to the live weight equivalent, a live weight conversion factor must be applied for each HS code. Live weight conversion factors are sourced from the European Market Observatory for Fisheries and Aquaculture Products (EUMOFA) Annex 7, along with various national and international governmental report values. EUMOFA data reports live weight conversion factors by CN-8 codes, so the mean of the live weight conversion factors falling within each HS 6-digit code are used. The EUMOFA data assigns products primarily destined for industrial purposes (e.g., fish meal and fish oil), co-products (e.g., caviar) and live trade a value of zero. In this analysis, co-products retained a live weight conversion factor value of zero to avoid double counting, but live animal trade was assigned a live weight conversion factor of 1 and fish meal and fish oil was assigned an average value of 2.9853. Data compiled from national and international reports were categorized into taxa types (mollusks, crustaceans, fishes, and other aquatic invertebrates), FAO ISSCAAP groups, species or taxon name, type of processing, and country of processing.\nLive weight conversion factors applied to trade data introduce a source of uncertainty and error due to uncertainty in conversion factors is not reported and a single live weight conversion factor is often presented per code, regardless of the species or region of origin. This is a limitation given that there are geographical and temporal variation in live weight conversion factors due to differences in processing technology. Despite this limitation, EUMOFA data offers better documentation and alignment with HS commodity codes than other live weight conversion factor data sources and is updated annually, providing documentation for changes in live weight conversion factors. Additionally, by supplementing the EUMOFA data with the other reported values we can better capture specific species processing into various product forms and some regional variability.\nAll conversion factors were reported as live weight to product weight ratios. These conversion factors were mapped onto possible species to commodity or commodity to commodity conversions, described below. For commodity-to-commodity conversions, we estimate the conversion factors (i.e., processing loss rate) as the additional mass lost when converting from the live weight to the original product form relative to converting from live weight to the processed product form. This can be calculated as the live weight conversion factor for the original product form divided by the live weight factor for the processed product form. We assume that mass cannot be gained through processing and therefore impose a maximum value of one to this ratio.",
    "crumbs": [
      "Our Model",
      "Data"
    ]
  },
  {
    "objectID": "artis-our-model/data-inputs.html#seafood-production-and-commodity-conversion",
    "href": "artis-our-model/data-inputs.html#seafood-production-and-commodity-conversion",
    "title": "Data",
    "section": "",
    "text": "For each country-year-HS version combination, we estimate the proportion of each species going into each commodity and the proportion of each imported commodity processed into each other commodity. Each species can only be converted into a subset of the commodities. For example, Atlantic salmon, Salmo salar, can be converted into whole frozen salmon or frozen salmon filets, but cannot be converted to a frozen tilapia filet. Similarly, each commodity can only be converted to a subset of other commodities through processing. For example, whole frozen salmon can be processed into frozen salmon filets, but not vice versa and neither salmon commodity can be converted to a tilapia commodity through processing. Defining possible conversions restricts the solution space to realistic results and improves estimation by reducing the number of unknowns. We describe this assignment process in detail below.\n\n\nSpecies production to commodity assignment is a many-to-many matching problem, wherein one commodity can consist of multiple species and one species can be converted to multiple commodities. All taxonomic names reported in the FAO production data were matched to HS 6-digit codes based on the code descriptions and HS system hierarchy. The first matching step required dividing all taxonomic groups into the broad commodity groups at the 4-digit level (fish, crustaceans, molluscs and aquatic invertebrates). Within each of these groups, taxonomic groups were matched based on 6 types of matching categories:\n1. Explicit taxa match - Scientific names are matched based on taxonomic information provided in the code description 2. NEC match - All remaining unmatched species within the 4-digit level are assigned to the “not elsewhere considered” (NEC) code 3. NEC by taxa match - When a code description signifies an NEC group, but limits this based on a taxonomic category (e.g., Salmonidae, N.E.C.), the NEC grouping occurs at this level, rather than the broad NEC match 4. Broad commodity match - Only the broad taxonomic groups inform this assignment since no further taxonomic information is provided 5. Aquarium trade match - Assigned to ornamental species trade based on species found in the aquarium/ornamental trade54 6. Fishmeal - Assigned to fishmeal codes if at least 1% of production goes to fishmeal production globally during the study period based on the end use designation from Sea Around Us production data55. Although an estimated 27% of fishmeal is derived from processing by-products3, the species, geographical, and temporal variation in that estimate is currently unknown. Consequently, fishmeal is currently treated as sourced from whole fish reduction. This does not affect the total trade or trade patterns of fishmeal but does result in an overestimate of the proportion of production going to fishmeal in cases where by-products are used.\nAfter all species are matched to the appropriate HS codes, we use the list of species to define codes as inland, marine, diadromous, or mixed. Higher order taxonomic groups are then only matched with HS codes that include their habitat. For example, production of inland actinopterygii is matched with codes that include inland species that fall within actinopterygii, but not with exclusively marine codes, even if they contain species that fall within actinopterygii.\n\n\n\nAs with the species to commodity assignment, the commodity-to-commodity assignment is a many-to-many data problem. Here, one commodity can be processed into multiple other commodities (i.e., frozen salmon can be processed into salmon filets or canned salmon), which also means one commodity could have come from multiple other commodities. To create these assignments, we established rules for which product transformations are technically possible. First, a product cannot transfer outside of its broad commodity group (e.g., fish, crustaceans, mollusc, aquatic invertebrate). Second, where a more refined species or species group was given (e.g., tunas, salmons, etc.) a product cannot be transformed outside that group. Third, products are classified in terms of their state (e.g., alive, fresh, frozen, etc.) and presentation (e.g., e.g., whole, fileted, salted/dried/preserved meats, reductions such as fish meal and fish oil, etc.) and cannot be converted into less processed forms (e.g., frozen salmon filets cannot turn into a frozen whole salmon). Fourth, specific commodities (i.e. mentioning specific species) and NEC commodities can become broad commodities (where appropriate), however broad commodities cannot become more specific or NEC commodities.",
    "crumbs": [
      "Our Model",
      "Data"
    ]
  },
  {
    "objectID": "artis-our-model/data-inputs.html#country-standardization-and-regions",
    "href": "artis-our-model/data-inputs.html#country-standardization-and-regions",
    "title": "Data",
    "section": "",
    "text": "The FAO production and BACI trade datasets do not share the same set of countries and territories. For the production and trade data to balance, it is important for the set of territories falling under a given name to align across the datasets. To avoid instances where, for example, production is reported under a territory, but trade is reported under the sovereign nation, we generally group all territories with the sovereign nation. As countries gain independence, they are added as a trade partner in the database. Due to this country standardization circular flows may occur when a sovereign nation trades with their territory. These circular flows are filtered out of the standardized BACI trade flows (i.e., internal trade is not included).\n\n\n\n\n\n\nNote\n\n\n\nThe original text this section is based on is from Gephart et al. (2024) Nature Communications [add link]. Please reference that paper when referencing this information: [Insert reference]",
    "crumbs": [
      "Our Model",
      "Data"
    ]
  },
  {
    "objectID": "artis-run-model/index.html",
    "href": "artis-run-model/index.html",
    "title": "Running the Model",
    "section": "",
    "text": "Local computer\n\nSee the next pages for instructions (from the artis-model Github repo)\n\nAmazon Web Services (AWS)\n\nSee the next pages for instructions (from artis-hpc Github repo).\n\n\n\n\n\n\nThe Knowledge Network for Biocomplexity (KNB) data repository\n\nhttps://doi.org/10.5063/F1862DXT\nRecomended access / download point\n.zip file containing the model code and cleaned input data\nStable model release (with DOI) used for research products\nWill include Docker image in future version\n\nGithub Seafood-Globalization-Lab/artis-model repository\n\nartis-model Contains the latest released model codebase\nDoes NOT contain model inputs",
    "crumbs": [
      "Running the Model"
    ]
  },
  {
    "objectID": "artis-run-model/index.html#two-avenues-to-run-artis-model",
    "href": "artis-run-model/index.html#two-avenues-to-run-artis-model",
    "title": "Running the Model",
    "section": "",
    "text": "Local computer\n\nSee the next pages for instructions (from the artis-model Github repo)\n\nAmazon Web Services (AWS)\n\nSee the next pages for instructions (from artis-hpc Github repo).",
    "crumbs": [
      "Running the Model"
    ]
  },
  {
    "objectID": "artis-run-model/index.html#two-ways-to-obtain-artis-model-and-model-inputs",
    "href": "artis-run-model/index.html#two-ways-to-obtain-artis-model-and-model-inputs",
    "title": "Running the Model",
    "section": "",
    "text": "The Knowledge Network for Biocomplexity (KNB) data repository\n\nhttps://doi.org/10.5063/F1862DXT\nRecomended access / download point\n.zip file containing the model code and cleaned input data\nStable model release (with DOI) used for research products\nWill include Docker image in future version\n\nGithub Seafood-Globalization-Lab/artis-model repository\n\nartis-model Contains the latest released model codebase\nDoes NOT contain model inputs",
    "crumbs": [
      "Running the Model"
    ]
  },
  {
    "objectID": "external-content/artis-hpc-readme.html",
    "href": "external-content/artis-hpc-readme.html",
    "title": "ARTIS HPC",
    "section": "",
    "text": "This repository outlines the instructions and scripts needed to create the ARTIS High Performance Computer (HPC) on Amazon Web Services (AWS).\n\n\n\nCopy the most up-to-date set of model inputs to the project root directory artis-hpc\nCopy the most up-to-date ARTIS R package folder and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package NAMESPACE file and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package DESCRIPTION file and place within artis-hpc/docker_image_files_original/\n\nIf running on a new Apple chip arm64:\n\nCopy arm64_venv_requirements.txt file from the root directory to the artis-hpc/docker_image_files_original/\nRename the file artis-hpc/docker_image_files_original/arm64_venv_requirements.txt to artis-hpc/docker_image_files_original/requirements.txt\nOpen artis-hpc/docker_image_files_original/run_artis_hs12.R and uncomment line 20.\n\n\n\n\n\nTerraform\n\nThis is a set of code scripts that create all the AWS infrastructure needed for the ARTIS HPC\nDestroy all AWS infrastructure for the ARTIS HPC after the ARTIS model has finished (save on unnecessary costs)\n\nDocker\n\nThis is used to create a docker image that our HPC jobs will use to run the ARTIS model code\n\nPython\n\nThrough the docker and AWS python (boto3) clients, this will provide code that:\n\nPush all model input data to AWS S3\nBuild docker image needed that the AWS Batch jobs will need to run ARTIS model\nPush docker image to AWS ECR\nSubmit jobs to ARTIS HPC\nPull all model outputs data\n\n\n\n\n\n\n\nHomebrew\nAWS CLI\nTerraform CLI\nPython\n\nPython packages\n\ndocker\nboto3\n\n\n\n\n\n\nInstall homebrew by running the terminal command /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nClose existing terminal window where installation command was run and open a new terminal window\nConfirm homebrew has been installed, run terminal command brew --version, no error messsage should appear.\n\nIf after homebrew installation you get a message stating brew command not found:\n\nEdit zsh config file, run terminal command: vim ~/.zshrc\nType i to enter edit mode\nCopy paste this line into the file you opened: export PATH=/opt/homebrew/bin:$PATH\nPress Shift and :\nType wq\nPress enter\nSource new config file, run terminal command source ~/.zshrc\n\n\n\n\nFollowing instructions from AWS\nNote: If you already have AWS CLI installed please still confirm by following step 3 below. Both instructions should run without an error message.\nThe following instructions are for MacOS users:\n\nRun terminal command curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\nRun terminal command sudo installer -pkg AWSCLIV2.pkg -target /\nConfirm AWS CLI has been installed:\n\nRun terminal command which aws\nRun terminal command aws --version\n\n\n\n\n\nNote: If you already have homebrew installed please confirm by running brew --version, no error message should occur.\nTo install terraform on MacOS we will be using homebrew. If you do not have homebrew installed on your computer please follow the installation instructions here, before continuing.\nBased on Terraform CLI installation instructions provided here.\n\nRun terminal command brew tap hashicorp/tap\nRun terminal command brew install hashicorp/tap/terraform\nRun terminal command brew update\nRun terminal command brew upgrade hashicorp/tap/terraform\n\nIf this has been unsuccessful you might need to install xcode command line tools, try:\n\nRun terminal command: sudo xcode-select --install\n\n\n\n\n\n\nAn AWS root user was created (To create an AWS root user visit )\nAWS root user has created an admin user group with “AdministratorAccess” permissions.\nAWS root user has created IAM users\nAWS root user has add IAM users to admin group\nAWS IAM users have their AWS AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY\n\nTo create an AWS IAM user: - FIXIT: include screenshots for creating an IAM user with the correct admin permissions.\nNote: If you created ANY AWS RESOURCES for ARTIS manually please delete these before continuing. These resources should\n\n\n\nFIXIT: Explain what each terminal command is doing\n\nRun terminal command: export AWS_ACCESS_KEY=[YOUR_AWS_ACCESS_KEY]\nRun terminal command: export AWS_SECRET_ACCESS_KEY=[YOUR_AWS_SECRET_ACCESS_KEY]\nRun terminal command: export AWS_REGION=us-east-1\nRun terminal command aws configure set aws_access_key_id \"[YOUR_AWS_ACCESS_KEY]\"\nRun terminal command aws configure set aws_secret_access_key \"[YOUR_AWS_SECRET_KEY]\"\n\n\n\n\nNote: Please make sure that your terminal is currently in your working directory that should end in artis-hpc, by running the terminal command pwd.\n\nCreate a virtual environment, run terminal command:python3 -m venv venv\nOpen virtual environment, run terminal command: source venv/bin/activate\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nCheck that all python modules have been downloaded, run terminal command pip freeze and check that all modules in the requirements.txt file are included.\n\nIf an error occurs please follow these instructions:\n\nUpgrade your version of pip by running terminal command: pip install --upgrade pip\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nIf errors still occur install each python package in the requirements.txt file individually, run terminal command pip3 install [PACKAGE NAME] ie pip3 install urllib3.\n\n\n\n\nNote: the initial_setup.py script will create all necessary AWS infrastructure, upload all model inputs to an AWS S3 bucket, and create and upload a docker image based on the ARTIS codebase. It will also submit jobs to the ARTIS HPC.\n\nOpen Docker Desktop\nTake note of any existing docker images and container relating to other projects, and delete all docker container relating to ARTIS, delete all docker images relating to ARTIS.\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command:\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: If you are using an Apple Silicone chip (M1, M2, M3, etc) your chip will be “arm64”, otherwise for intel chips it will be “x86”\n\n\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command:\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: This will create the docker image from scratch. If you have an existing docker image you would like to use include the -di [existing docker image name] with the command.\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name] -di [existing docker image name]:latest\nExamples: - If you are creating the docker image from scratch (If you change any R code for ARTIS you will have to recreate a docker image):\npython3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image\n\nIf you have an existing docker image (for example only need to re-upload a new set of model inputs):\n\npython3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image -di myname-artis-image:latest\nNote: If terraform states that it created all resources however when you log into the AWS console to confirm cannot see them, they have most likely been created as part of another account. Run terraform destroy -auto-approveon the command line. Confirmed you have followed the AWS CLI set up instructions with the correct set of keys (AWS access key and AWS secret access key)."
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#update-artis-model-scripts-and-model-inputs",
    "href": "external-content/artis-hpc-readme.html#update-artis-model-scripts-and-model-inputs",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Copy the most up-to-date set of model inputs to the project root directory artis-hpc\nCopy the most up-to-date ARTIS R package folder and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package NAMESPACE file and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package DESCRIPTION file and place within artis-hpc/docker_image_files_original/\n\nIf running on a new Apple chip arm64:\n\nCopy arm64_venv_requirements.txt file from the root directory to the artis-hpc/docker_image_files_original/\nRename the file artis-hpc/docker_image_files_original/arm64_venv_requirements.txt to artis-hpc/docker_image_files_original/requirements.txt\nOpen artis-hpc/docker_image_files_original/run_artis_hs12.R and uncomment line 20."
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#technologies-used",
    "href": "external-content/artis-hpc-readme.html#technologies-used",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Terraform\n\nThis is a set of code scripts that create all the AWS infrastructure needed for the ARTIS HPC\nDestroy all AWS infrastructure for the ARTIS HPC after the ARTIS model has finished (save on unnecessary costs)\n\nDocker\n\nThis is used to create a docker image that our HPC jobs will use to run the ARTIS model code\n\nPython\n\nThrough the docker and AWS python (boto3) clients, this will provide code that:\n\nPush all model input data to AWS S3\nBuild docker image needed that the AWS Batch jobs will need to run ARTIS model\nPush docker image to AWS ECR\nSubmit jobs to ARTIS HPC\nPull all model outputs data"
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#installation",
    "href": "external-content/artis-hpc-readme.html#installation",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Homebrew\nAWS CLI\nTerraform CLI\nPython\n\nPython packages\n\ndocker\nboto3\n\n\n\n\n\n\nInstall homebrew by running the terminal command /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nClose existing terminal window where installation command was run and open a new terminal window\nConfirm homebrew has been installed, run terminal command brew --version, no error messsage should appear.\n\nIf after homebrew installation you get a message stating brew command not found:\n\nEdit zsh config file, run terminal command: vim ~/.zshrc\nType i to enter edit mode\nCopy paste this line into the file you opened: export PATH=/opt/homebrew/bin:$PATH\nPress Shift and :\nType wq\nPress enter\nSource new config file, run terminal command source ~/.zshrc\n\n\n\n\nFollowing instructions from AWS\nNote: If you already have AWS CLI installed please still confirm by following step 3 below. Both instructions should run without an error message.\nThe following instructions are for MacOS users:\n\nRun terminal command curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\nRun terminal command sudo installer -pkg AWSCLIV2.pkg -target /\nConfirm AWS CLI has been installed:\n\nRun terminal command which aws\nRun terminal command aws --version\n\n\n\n\n\nNote: If you already have homebrew installed please confirm by running brew --version, no error message should occur.\nTo install terraform on MacOS we will be using homebrew. If you do not have homebrew installed on your computer please follow the installation instructions here, before continuing.\nBased on Terraform CLI installation instructions provided here.\n\nRun terminal command brew tap hashicorp/tap\nRun terminal command brew install hashicorp/tap/terraform\nRun terminal command brew update\nRun terminal command brew upgrade hashicorp/tap/terraform\n\nIf this has been unsuccessful you might need to install xcode command line tools, try:\n\nRun terminal command: sudo xcode-select --install"
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#assumptions",
    "href": "external-content/artis-hpc-readme.html#assumptions",
    "title": "ARTIS HPC",
    "section": "",
    "text": "An AWS root user was created (To create an AWS root user visit )\nAWS root user has created an admin user group with “AdministratorAccess” permissions.\nAWS root user has created IAM users\nAWS root user has add IAM users to admin group\nAWS IAM users have their AWS AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY\n\nTo create an AWS IAM user: - FIXIT: include screenshots for creating an IAM user with the correct admin permissions.\nNote: If you created ANY AWS RESOURCES for ARTIS manually please delete these before continuing. These resources should"
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#aws-cli-setup",
    "href": "external-content/artis-hpc-readme.html#aws-cli-setup",
    "title": "ARTIS HPC",
    "section": "",
    "text": "FIXIT: Explain what each terminal command is doing\n\nRun terminal command: export AWS_ACCESS_KEY=[YOUR_AWS_ACCESS_KEY]\nRun terminal command: export AWS_SECRET_ACCESS_KEY=[YOUR_AWS_SECRET_ACCESS_KEY]\nRun terminal command: export AWS_REGION=us-east-1\nRun terminal command aws configure set aws_access_key_id \"[YOUR_AWS_ACCESS_KEY]\"\nRun terminal command aws configure set aws_secret_access_key \"[YOUR_AWS_SECRET_KEY]\""
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#python-installation",
    "href": "external-content/artis-hpc-readme.html#python-installation",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Note: Please make sure that your terminal is currently in your working directory that should end in artis-hpc, by running the terminal command pwd.\n\nCreate a virtual environment, run terminal command:python3 -m venv venv\nOpen virtual environment, run terminal command: source venv/bin/activate\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nCheck that all python modules have been downloaded, run terminal command pip freeze and check that all modules in the requirements.txt file are included.\n\nIf an error occurs please follow these instructions:\n\nUpgrade your version of pip by running terminal command: pip install --upgrade pip\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nIf errors still occur install each python package in the requirements.txt file individually, run terminal command pip3 install [PACKAGE NAME] ie pip3 install urllib3."
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#creating-aws-infrastructure-with-a-setup-file",
    "href": "external-content/artis-hpc-readme.html#creating-aws-infrastructure-with-a-setup-file",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Note: the initial_setup.py script will create all necessary AWS infrastructure, upload all model inputs to an AWS S3 bucket, and create and upload a docker image based on the ARTIS codebase. It will also submit jobs to the ARTIS HPC.\n\nOpen Docker Desktop\nTake note of any existing docker images and container relating to other projects, and delete all docker container relating to ARTIS, delete all docker images relating to ARTIS.\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command:\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: If you are using an Apple Silicone chip (M1, M2, M3, etc) your chip will be “arm64”, otherwise for intel chips it will be “x86”\n\n\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command:\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: This will create the docker image from scratch. If you have an existing docker image you would like to use include the -di [existing docker image name] with the command.\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name] -di [existing docker image name]:latest\nExamples: - If you are creating the docker image from scratch (If you change any R code for ARTIS you will have to recreate a docker image):\npython3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image\n\nIf you have an existing docker image (for example only need to re-upload a new set of model inputs):\n\npython3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image -di myname-artis-image:latest\nNote: If terraform states that it created all resources however when you log into the AWS console to confirm cannot see them, they have most likely been created as part of another account. Run terraform destroy -auto-approveon the command line. Confirmed you have followed the AWS CLI set up instructions with the correct set of keys (AWS access key and AWS secret access key)."
  },
  {
    "objectID": "external-content/artis-database-readme.html",
    "href": "external-content/artis-database-readme.html",
    "title": "Configuring cloud and local ARTIS databases",
    "section": "",
    "text": "Instructions for seting up and managing cloud and local instances of the ARTIS database.\n\nInstallations\nCloud Database\nLocal Database\nArchitecture\nDatabase Structure\n\nPlease Note: We are migrating our cloud database from Heroku to Vercel and will update these instructions (2024-07-26)\n\n\n\nDownload PostgreSQL: https://www.postgresql.org/download/\nDownload pgAdmin: https://www.pgadmin.org/download/\n\n\n\n\nReference: https://stackoverflow.com/questions/11769860/connect-to-a-heroku-database-with-pgadmin\n\n\n\nSign into the Heroku platform\nClick on the “artis” app\n\n\n\nClick on the “Resources” tab \nClick on “Heroku Postgres” in the list of resources available (this should open a new browser tab)\n\n\n\nClick on the “Credentials” tab\n\n\n\nClick on the arrow by “default 1 app” (this should provide a drop down set of options and details)\nClick on the “show” button to reveal the password for the database\n\n\n\n\n\n\n\n\nOpen pgAdmin\nRight click on the Server list on the left hand side\nSelect Servers &gt; Register &gt; Server (this will open a new smaller window with additional settings)\n\n\n\nEnter a server name (this will only be a local name reference) like “HEROKU_ARTIS”\n\n\n\nClick on the “Connection” tab\n\n\n\nThe details needed to fill in the following information can be found on the Heroku credentials page we found earlier:\nEnter the host name under the “Host name/address”\nEnter the port number under the “Port” field\nEnter the database name under the “Maintenance database” field\nEnter the user name under the “Username” field\nEnter the password (make sure to click reveal in Heroku) under the “Password”\nSelect Save password for future use\nClick on “Advanced” tab\n\n\n\nEnter the database name under the “DB restriction” field (There should now be a new database connection in your pgAdmin dropdown)\n\n\n\n\n\nClick on server connection you created earlier (this will appear under the server name you wrote in earlier, ie “HEROKU_ARTIS”)\nClick the arrow by the server connection name (this should create provide a drop down with options and the 1 database)\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin)\n\n\n\nRun the SQL command “SELECT * FROM users;” (this should return immediately, with a table of the users that have access to the ARTIS API)\n\n\n\n\n\nFind the tables drop down under the database connection options on the left hand side of pgAdmin\n\n\n\n\nRepeat the following instructions for each table you want to update:\n\nIf the table already exists:\n\n\nRight click on the existing table and select the “Delete/Drop” option\n\n\n\n\n\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin)\nPaste and run the SQL script for creating the table you are interested in updating\nRight-click the “Tables” dropdown and select “Refresh”\n\n\n\nRight-click on the table you just re-created, and select “Import/Export Data” (this will open a new dialog box)\n\n\n\nConfirm the “Import” tab is selected and use the “Filename” field to find the table data you would like to include.\n\n\n\nSelect the “Options” tab\nConfirm the “Header” toggle is activated and the “NULL Strings” field has the value “NA”\n\n\n\nSelect the “Columns” tab\nMake sure the “record_id” column IS NOT part of the “Columns to import” field. If it is please delete this column from the list.\n\n\n\n\n\n\n\n\n\nprep_db_files.R\n\nTakes raw snet files to a database table\n\ncreate_sql_tables\n\nSQL files to create the different tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3C code for direct exporter country\n\n\nimporter_iso3c\nISO3C code for direct importer country\n\n\nsource_country_iso3c\nISO3C code for the country that produced the specific product\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs6\nHS 6 digit code used to identify what product is being traded.\n\n\nsciname\nspecies name traded under the specific HS product and 6-digit code.\n\n\nhabitat\nclassifies whether the specific species’ habitat (marine/inland/unknown).\n\n\nmethod\ndefines method of production (aquaculture/capture/unknown).\n\n\nproduct_weight_t\nproduct weight in tonnes.\n\n\nlive_weight_t\nlive weight in tonnes.\n\n\nyear\nyear in which trade occured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexporter_iso3c\nimporter_iso3c\nsource_country_iso3c\ndom_source\nhs6\nsciname\nhabitat\nmethod\nproduct_weight_t\nlive_weight_t\nyear\n\n\n\n\nCAN\nUSA\nCAN\ndomestic export\n030212\noncorhynchus keta\nmarine\ncapture\n870.34\n1131.45\n2017\n\n\nCHL\nITA\nPER\nforeign export\n230120\nengraulis ringens\nmarine\ncapture\n344.889\n1026.11\n2017\n\n\n\nNote:\n\nDomestic Export: An export where the specific product was produced in the same country as it was exported from.\nForeign Export: An export where a specific product is imported from a source country and then re-exported by another country.\nError Export: An export that cannot be explained by domestic or foreign export records nor production records.\n\n\n\n\nThis table has all FAO production records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 code for the producing country\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nlive_weight_t\nLive weight in tonnes.\n\n\nyear\nYear species was produced.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niso3c\nsciname\nmethod\nhabitat\nlive_weight_t\nyear\n\n\n\n\nSWE\nabramis brama\ncapture\ninland\n7\n2006\n\n\n\n\n\n\n\nThis table has all SAU production records for all countries in ARTIS for 1996 - 2019. Note all production is marine capture.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\ncountry_name_en\nProducing country name in english\n\n\ncountry_iso3_alpha\nProducing country ISO3 3 letter code\n\n\ncountry_iso3_numeric\nProducing country ISO3 numeric code\n\n\neez\nExclusive Economic Zone\n\n\nsector\neconomic sector\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nyear\nYear species was produced\n\n\nlive_weight_t\nLive weight in tonnes\n\n\n\n\n\n\nThis table has all BACI bilateral trade records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3 3 letter code for direct exporter country\n\n\nimporter_iso3c\nISO3 3 letter code for direct importer country\n\n\nhs6\nHS 6 digit code used to identify what product is being traded\n\n\nproduct_weight_t\nproduct weight in tonnes\n\n\nhs_version\nHS code version for the year used\n\n\nyear\nyear trade occured\n\n\n\n\n\n\nThis table contains metadata about countries in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code for country\n\n\ncountry_name\nCountry name in english\n\n\nowid_region\nCountry’s region as defined by Our World in Data\n\n\ncontinent\nCountry’s continent as defined by R countrycode package\n\n\n\n\n\n\nThis table contains the nutrient content per 100g of the species in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\nsciname\nspecies scientific name\n\n\ncalcium_mg\ncalcium content (mg) per 100 g of species\n\n\niron_mg\niron content (mg) per 100 g of species\n\n\nprotein_g\nprotein content (g) per 100 g of species\n\n\nfattyacids_g\nfatty acid content (g) per 100 g of species\n\n\nvitamina_mcg\nvitamin a content (mcg) per 100 g of species\n\n\nvitaminb12_mcg\nvitamin b12 content (mcg) per 100 g of species\n\n\nzinc_mg\nzinc content (mg) per 100 g of species\n\n\n\n\n\n\nThis table contains consumption estimates from trade.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code of consuming country\n\n\nhs6\nHS 6 digit code used to identify what product is being consumed\n\n\nsciname\nspecies scientific name\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nsource_country_iso3c\nISO3 3 letter code of country of origin\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs_version\nHS code version for the year\n\n\npopulation\npopulation of consuming country (sourced from FAO)\n\n\nyear\nyear of consumption\n\n\n\n\n\n\nThis is a conversion table to resolve a scientific name from higher order taxa (family, order, class etc) to a more specific species based on the HS product and HS version used.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs_version\nHS code version for the year\n\n\nhs6\nspecies scientific name\n\n\nsciname\noriginal scientific name determined by production records\n\n\nsciname_hs_modified\nmore specific/resolved scientific name given hs version and hs product\n\n\n\n\n\n\n\nThis table contains information about what each HS 6-digit code represents.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs6\nHS 6-digit product code\n\n\ndescription\nProduct description\n\n\npresentation\nProduct form (fillet, whole, fats and oils, non-fish, non-fmp form, other body parts, other meat, livers and roes)\n\n\nstate\nProduct state (live, frozen, preserved, fresh, not for humans, reduced)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhs6\ndescription\npresentation\nstate\n\n\n\n\n030212\nFish; Pacific salmon (oncorhynchus spp.), Atlantic salmon (salmo salar), Danube salmon (hucho hucho), fresh or chilled (excluding fillets, livers, roes and other fish meat of heading no. 0304)\nwhole\nfresh"
  },
  {
    "objectID": "external-content/artis-database-readme.html#installations",
    "href": "external-content/artis-database-readme.html#installations",
    "title": "Configuring cloud and local ARTIS databases",
    "section": "",
    "text": "Download PostgreSQL: https://www.postgresql.org/download/\nDownload pgAdmin: https://www.pgadmin.org/download/"
  },
  {
    "objectID": "external-content/artis-database-readme.html#updating-cloud-heroku-database",
    "href": "external-content/artis-database-readme.html#updating-cloud-heroku-database",
    "title": "Configuring cloud and local ARTIS databases",
    "section": "",
    "text": "Reference: https://stackoverflow.com/questions/11769860/connect-to-a-heroku-database-with-pgadmin\n\n\n\nSign into the Heroku platform\nClick on the “artis” app\n\n\n\nClick on the “Resources” tab \nClick on “Heroku Postgres” in the list of resources available (this should open a new browser tab)\n\n\n\nClick on the “Credentials” tab\n\n\n\nClick on the arrow by “default 1 app” (this should provide a drop down set of options and details)\nClick on the “show” button to reveal the password for the database"
  },
  {
    "objectID": "external-content/artis-database-readme.html#setting-up-local-database",
    "href": "external-content/artis-database-readme.html#setting-up-local-database",
    "title": "Configuring cloud and local ARTIS databases",
    "section": "",
    "text": "Open pgAdmin\nRight click on the Server list on the left hand side\nSelect Servers &gt; Register &gt; Server (this will open a new smaller window with additional settings)\n\n\n\nEnter a server name (this will only be a local name reference) like “HEROKU_ARTIS”\n\n\n\nClick on the “Connection” tab\n\n\n\nThe details needed to fill in the following information can be found on the Heroku credentials page we found earlier:\nEnter the host name under the “Host name/address”\nEnter the port number under the “Port” field\nEnter the database name under the “Maintenance database” field\nEnter the user name under the “Username” field\nEnter the password (make sure to click reveal in Heroku) under the “Password”\nSelect Save password for future use\nClick on “Advanced” tab\n\n\n\nEnter the database name under the “DB restriction” field (There should now be a new database connection in your pgAdmin dropdown)\n\n\n\n\n\nClick on server connection you created earlier (this will appear under the server name you wrote in earlier, ie “HEROKU_ARTIS”)\nClick the arrow by the server connection name (this should create provide a drop down with options and the 1 database)\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin)\n\n\n\nRun the SQL command “SELECT * FROM users;” (this should return immediately, with a table of the users that have access to the ARTIS API)\n\n\n\n\n\nFind the tables drop down under the database connection options on the left hand side of pgAdmin\n\n\n\n\nRepeat the following instructions for each table you want to update:\n\nIf the table already exists:\n\n\nRight click on the existing table and select the “Delete/Drop” option\n\n\n\n\n\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin)\nPaste and run the SQL script for creating the table you are interested in updating\nRight-click the “Tables” dropdown and select “Refresh”\n\n\n\nRight-click on the table you just re-created, and select “Import/Export Data” (this will open a new dialog box)\n\n\n\nConfirm the “Import” tab is selected and use the “Filename” field to find the table data you would like to include.\n\n\n\nSelect the “Options” tab\nConfirm the “Header” toggle is activated and the “NULL Strings” field has the value “NA”\n\n\n\nSelect the “Columns” tab\nMake sure the “record_id” column IS NOT part of the “Columns to import” field. If it is please delete this column from the list."
  },
  {
    "objectID": "external-content/artis-database-readme.html#directory-and-file-structure",
    "href": "external-content/artis-database-readme.html#directory-and-file-structure",
    "title": "Configuring cloud and local ARTIS databases",
    "section": "",
    "text": "prep_db_files.R\n\nTakes raw snet files to a database table\n\ncreate_sql_tables\n\nSQL files to create the different tables"
  },
  {
    "objectID": "external-content/artis-database-readme.html#database-structure",
    "href": "external-content/artis-database-readme.html#database-structure",
    "title": "Configuring cloud and local ARTIS databases",
    "section": "",
    "text": "Column Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3C code for direct exporter country\n\n\nimporter_iso3c\nISO3C code for direct importer country\n\n\nsource_country_iso3c\nISO3C code for the country that produced the specific product\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs6\nHS 6 digit code used to identify what product is being traded.\n\n\nsciname\nspecies name traded under the specific HS product and 6-digit code.\n\n\nhabitat\nclassifies whether the specific species’ habitat (marine/inland/unknown).\n\n\nmethod\ndefines method of production (aquaculture/capture/unknown).\n\n\nproduct_weight_t\nproduct weight in tonnes.\n\n\nlive_weight_t\nlive weight in tonnes.\n\n\nyear\nyear in which trade occured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexporter_iso3c\nimporter_iso3c\nsource_country_iso3c\ndom_source\nhs6\nsciname\nhabitat\nmethod\nproduct_weight_t\nlive_weight_t\nyear\n\n\n\n\nCAN\nUSA\nCAN\ndomestic export\n030212\noncorhynchus keta\nmarine\ncapture\n870.34\n1131.45\n2017\n\n\nCHL\nITA\nPER\nforeign export\n230120\nengraulis ringens\nmarine\ncapture\n344.889\n1026.11\n2017\n\n\n\nNote:\n\nDomestic Export: An export where the specific product was produced in the same country as it was exported from.\nForeign Export: An export where a specific product is imported from a source country and then re-exported by another country.\nError Export: An export that cannot be explained by domestic or foreign export records nor production records.\n\n\n\n\nThis table has all FAO production records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 code for the producing country\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nlive_weight_t\nLive weight in tonnes.\n\n\nyear\nYear species was produced.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niso3c\nsciname\nmethod\nhabitat\nlive_weight_t\nyear\n\n\n\n\nSWE\nabramis brama\ncapture\ninland\n7\n2006\n\n\n\n\n\n\n\nThis table has all SAU production records for all countries in ARTIS for 1996 - 2019. Note all production is marine capture.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\ncountry_name_en\nProducing country name in english\n\n\ncountry_iso3_alpha\nProducing country ISO3 3 letter code\n\n\ncountry_iso3_numeric\nProducing country ISO3 numeric code\n\n\neez\nExclusive Economic Zone\n\n\nsector\neconomic sector\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nyear\nYear species was produced\n\n\nlive_weight_t\nLive weight in tonnes\n\n\n\n\n\n\nThis table has all BACI bilateral trade records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3 3 letter code for direct exporter country\n\n\nimporter_iso3c\nISO3 3 letter code for direct importer country\n\n\nhs6\nHS 6 digit code used to identify what product is being traded\n\n\nproduct_weight_t\nproduct weight in tonnes\n\n\nhs_version\nHS code version for the year used\n\n\nyear\nyear trade occured\n\n\n\n\n\n\nThis table contains metadata about countries in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code for country\n\n\ncountry_name\nCountry name in english\n\n\nowid_region\nCountry’s region as defined by Our World in Data\n\n\ncontinent\nCountry’s continent as defined by R countrycode package\n\n\n\n\n\n\nThis table contains the nutrient content per 100g of the species in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\nsciname\nspecies scientific name\n\n\ncalcium_mg\ncalcium content (mg) per 100 g of species\n\n\niron_mg\niron content (mg) per 100 g of species\n\n\nprotein_g\nprotein content (g) per 100 g of species\n\n\nfattyacids_g\nfatty acid content (g) per 100 g of species\n\n\nvitamina_mcg\nvitamin a content (mcg) per 100 g of species\n\n\nvitaminb12_mcg\nvitamin b12 content (mcg) per 100 g of species\n\n\nzinc_mg\nzinc content (mg) per 100 g of species\n\n\n\n\n\n\nThis table contains consumption estimates from trade.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code of consuming country\n\n\nhs6\nHS 6 digit code used to identify what product is being consumed\n\n\nsciname\nspecies scientific name\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nsource_country_iso3c\nISO3 3 letter code of country of origin\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs_version\nHS code version for the year\n\n\npopulation\npopulation of consuming country (sourced from FAO)\n\n\nyear\nyear of consumption\n\n\n\n\n\n\nThis is a conversion table to resolve a scientific name from higher order taxa (family, order, class etc) to a more specific species based on the HS product and HS version used.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs_version\nHS code version for the year\n\n\nhs6\nspecies scientific name\n\n\nsciname\noriginal scientific name determined by production records\n\n\nsciname_hs_modified\nmore specific/resolved scientific name given hs version and hs product\n\n\n\n\n\n\n\nThis table contains information about what each HS 6-digit code represents.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs6\nHS 6-digit product code\n\n\ndescription\nProduct description\n\n\npresentation\nProduct form (fillet, whole, fats and oils, non-fish, non-fmp form, other body parts, other meat, livers and roes)\n\n\nstate\nProduct state (live, frozen, preserved, fresh, not for humans, reduced)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhs6\ndescription\npresentation\nstate\n\n\n\n\n030212\nFish; Pacific salmon (oncorhynchus spp.), Atlantic salmon (salmo salar), Danube salmon (hucho hucho), fresh or chilled (excluding fillets, livers, roes and other fish meat of heading no. 0304)\nwhole\nfresh"
  },
  {
    "objectID": "external-content/artis-api-readme.html",
    "href": "external-content/artis-api-readme.html",
    "title": "ARTIS Aquatic Resource Trade in Species API",
    "section": "",
    "text": "This API processes requests for the ARTIS database.\n\n\nFood systems have become increasingly globalized, with over a quarter of all food now traded internationally. Seafood is among the most highly traded foods and it is becoming increasingly globalized, with trade doubling in recent decades. At the same time, seafood is now widely recognized as a critical source of nutrition. Thus, social and environmental threats to local seafood production, including environmental extremes, price impacts of market integration, networked risks, and increased availability of processed foods, must be evaluated in the context of global trade. These issues are paralleled by similar questions for other natural resources and are central to global food systems research. However, our collective understanding of the environmental and human outcomes of food system globalization is limited by a fundamental gap between production and trade data. We bridge this gap in the Aquatic Resource Trade in Species (ARTIS) database by providing the first global estimates of seafood species and nutrient trade flows from 1996–2020.\n(Taken from https://artisdata.weebly.com/)\n\n\n\n\nThe files below contain the documentation for what types of requests (examples and sample responses) can be made to the ARTIS API.\n\ntable_outlines.md\n\nFormats and structures for all tables accessible from the ARTIS API\n\nmaking_requests.md\n\nStructure for how to make requests\nSample requests\nSample responses\n\n\n\n\n\n\n\nMain ARTIS snet\nSupplemental Tables\n\nsciname (species taxonomic information)\nproduction (FAO production data)\nproducts (HS code descriptions and classification)\ncountries (country metadata ie iso3 codes, regions)\nbaci (BACI trade bilateral trade data)"
  },
  {
    "objectID": "external-content/artis-api-readme.html#about-artis",
    "href": "external-content/artis-api-readme.html#about-artis",
    "title": "ARTIS Aquatic Resource Trade in Species API",
    "section": "",
    "text": "Food systems have become increasingly globalized, with over a quarter of all food now traded internationally. Seafood is among the most highly traded foods and it is becoming increasingly globalized, with trade doubling in recent decades. At the same time, seafood is now widely recognized as a critical source of nutrition. Thus, social and environmental threats to local seafood production, including environmental extremes, price impacts of market integration, networked risks, and increased availability of processed foods, must be evaluated in the context of global trade. These issues are paralleled by similar questions for other natural resources and are central to global food systems research. However, our collective understanding of the environmental and human outcomes of food system globalization is limited by a fundamental gap between production and trade data. We bridge this gap in the Aquatic Resource Trade in Species (ARTIS) database by providing the first global estimates of seafood species and nutrient trade flows from 1996–2020.\n(Taken from https://artisdata.weebly.com/)"
  },
  {
    "objectID": "external-content/artis-api-readme.html#description-files",
    "href": "external-content/artis-api-readme.html#description-files",
    "title": "ARTIS Aquatic Resource Trade in Species API",
    "section": "",
    "text": "The files below contain the documentation for what types of requests (examples and sample responses) can be made to the ARTIS API.\n\ntable_outlines.md\n\nFormats and structures for all tables accessible from the ARTIS API\n\nmaking_requests.md\n\nStructure for how to make requests\nSample requests\nSample responses"
  },
  {
    "objectID": "external-content/artis-api-readme.html#artis-database-tables",
    "href": "external-content/artis-api-readme.html#artis-database-tables",
    "title": "ARTIS Aquatic Resource Trade in Species API",
    "section": "",
    "text": "Main ARTIS snet\nSupplemental Tables\n\nsciname (species taxonomic information)\nproduction (FAO production data)\nproducts (HS code descriptions and classification)\ncountries (country metadata ie iso3 codes, regions)\nbaci (BACI trade bilateral trade data)"
  },
  {
    "objectID": "external-content/exploreARTIS-readme.html",
    "href": "external-content/exploreARTIS-readme.html",
    "title": "exploreARTIS",
    "section": "",
    "text": "The exploreARTIS R package provides functions for filtering and visualizing trade and consumption data from the ARTIS (Aquatic Resource Trade In Species) database. This package is designed to facilitate and streamline investigation of the ARTIS database. Most functions are wrappers for ggplot2::ggplot() and can accept additional layers to further customize the figures, with the exception of exploreARTIS::plot_sankey() which is based on ggsankey.\n\n\n\n\nMac Users Run the following commands in terminal:\nbrew install pkg-config\nbrew install gdal\nOnce installed run the following command in the R console:\ninstall.packages(\"sf\", configure.args = \"--with-proj-lib=/usr/local/lib/\")\nWindows Users Please make sure you have Rtools installed first. Follow the instructions here. Then run the following command in the R console:\ninstall.packages(\"sf\")\n\n\n\nYou can install this package with the devtools package. The first time you do it you will have to run\ninstall.packages(\"devtools\")\nlibrary(devtools)\nThen, you can run\ndevtools::install_github(\"Seafood-Globalization-Lab/exploreARTIS\", dependencies = TRUE)\nAfter you install the exploreARTIS package, you can just load it with library(exploreARTIS). You will also need to reinstall the package whenever there are updates to the package code.\n\n\n\n\nARTIS data consists of the following variables:\n\nexporter_iso3c (string): Exporter Country ISO 3 code\nimporter_iso3c (string): Importer Country ISO 3 code\nsource_country_iso3c (string): Producer Country ISO 3 code\ndom_source (string): Domestic Export / Foreign Export / Error Export\nhs6 (string): 6-digit HS commodity code\nsciname (string): Species or Species group\nenvironment (string): Marine / Freshwater\nmethod (string): Capture / Aquaculture / Unknown\nproduct_weight_t (double): Product weight (tonnes)\nlive_weight_t (double): Live weight (tonnes)\nhs_version (string): version of HS codes\nyear (double): Year\n\n\n\n\n\n\nIf you have downloaded bulk ARTIS data, it is generally split into separate csv files for each HS version and year combination. This is because the combined file is large and slow to load, sometimes causing users’ R sessions to crash. If you would like to combine files into a single data frame, you will need to pick which HS version-year combinations you would like to include. Then, you can decide if you would like to filter down any of the variabales (e.g., keep select exporters, species, etc.). Once you have made these decisions, you can use the example script scripts/filter_bulk_artis.R as a starting place to loop through the desired ARTIS files and filter based on your specified criteria. Note that this is not a function, but rather an example script with comments to facilitate customization for your own project.\n\n\n\nOnce you have the ARTIS data frame you are using for your analysis, you may still want to filter it for specific visualizations. For example, you may be working with all trade for a given country but want to generate a plot for just one species. You can of course use any base R or tidyverse functions to filter the data, but we also provide a function in this package to filter any of the ARTIS variables: filter_artis(). The filtered data frame can then be passed to any visualization function.\n# loading library\nlibrary(exploreARTIS)\n\n# Filter ARTIS data to Chilean exports of Atlantic salmon in 2016-2020\nfilter_artis(mini_artis, year = 2016:2020, exporter = \"CHL\", species = \"salmo salar\")\n\n\n\n\nHere are examples of all types of plots that can be created with this package. mini_artis is dataframe with a subset of ARTIS data that is included in the exploreARTIS package.\n\n\nplot_bar() creates ranked bar plots, with bar categories indicated by the bar_group argument. The number of bars to display can be controlled with top_n. argument.\n# loading library\nlibrary(exploreARTIS)\n\n# Bar chart visualizing seafood trade volumes by exporter\nplot_bar(mini_artis, bar_group = \"exporter_iso3c\")\n\n\n\nBar charts can optionally be filled by an ARTIS variable.\n# loading library\nlibrary(exploreARTIS)\n\n# Bar chart visualizing seafood trade volumes by exporter and filling by export source\nplot_bar(mini_artis, bar_group = \"exporter_iso3c\", fill_type = \"dom_source\")\n\n\n\n\n\n\nplot_ts() creates time series line or area plots for any specified artis_var. The plot.type argument allows options of “line” (default) or “stacked” to change plot views. To keep the number of colors reasonable, the prop_flow_cutoff argument groups lines falling below the cut-off into “other” and this can be adjusted to show more or fewer lines/fills.\n# loading library\nlibrary(exploreARTIS)\n\nplot_ts(mini_artis, artis_var = \"exporter_iso3c\")\n\n\n\nA stacked line graph of all export partners in the ARTIS dataset\n# loading library\nlibrary(exploreARTIS)\n\nplot_ts(mini_artis, artis_var = \"exporter_iso3c\", plot.type = \"stacked\")\n\n\n\n\n\n\nplot_sankey() creates a sankey plot to display flows among nodes across columns. This function is flexible in allowing the user to specify which data columns should be used to produce the colunns of the sankey plot. This function includes the argument prop_flow_cutoff to control how many groups are included in “other” (which can help keep the larger flows readable). It also includes an argumen to drop the group “other” entirely if preferred.\n# loading library\nlibrary(exploreARTIS)\n\n# Sankey plot of all seafood trade\nplot_sankey(mini_artis, cols = c(\"sciname\", \"exporter_iso3c\", \"importer_iso3c\"))\n\n\n\n\n\n\nr plot_chord() creates a chord diagram for visualizing flows among countries/regions.\n# loading library\nlibrary(exploreARTIS)\n\n# Chord diagram of all seafood trade\nplot_chord(mini_artis, region_colors = region7_palette)\n\n\n\nChord Diagram - all seafood trade\n\n\nIndividual countries can be pulled out to highlight their trade by specifying the country/countries’ iso3c code(s) in the focal_country argument.\n# loading library\nlibrary(exploreARTIS)\n\n# Chord diagram of all seafood trade with Vietnam highlighted\nplot_chord(mini_artis, focal_country = \"VNM\", region_colors = region7_palette)\n\n\n\nChord Diagram - all seafood trade\n\n\n\n\n\nplot_map() creates maps that are optionally colored by country_fill and optionally include flow arrows colored by flow volume with flow_arrows. The number of arrows can be specified with n_flows.\n# loading library\nlibrary(exploreARTIS)\n\n# Map of top seafood exports and flows\nplot_map(mini_artis, country_fill = \"importer_iso3c\", flow_arrows = TRUE, arrow_label = \"Trade (live t)\", fill_label = \"Import (live t)\")\n\n\n\nIndividual country’s trade flows can be isolated by filtering the importer or exporter column before passing it to the plot function.\n# loading library\nlibrary(exploreARTIS)\n\n# Map of seafood exports from Chile\nmini_artis %&gt;% filter(exporter_iso3c == \"CHL\") %&gt;% plot_map(country_fill = \"importer_iso3c\", flow_arrows = TRUE, arrow_label = \"Trade (live t)\", fill_label = \"Import (live t)\")\n\n\n\n\n\n\nBoth plot_ts() and r plot_bar() allow facetting by an ARTIS variable with the facet_variable argument. If a facet variable is specified then facet_values must also be defined, either as a number (the number of facets to create) or a vector (the specific facets to create).\n# loading libraries\nlibrary(exploreARTIS)\n\n# Area plot of top importers facetted by method\nplot_ts(mini_artis, artis_var = \"importer_iso3c\", plot.type = \"stacked\", facet_variable = \"method\", facet_values = c(\"capture\", \"aquaculture\"))\n\n\n\n# loading libraries\nlibrary(exploreARTIS)\n\n# Bar plot of top importers facetted by method\nplot_bar(mini_artis, bar_group = \"importer_iso3c\", facet_variable = \"method\", facet_values = c(\"capture\", \"aquaculture\"))"
  },
  {
    "objectID": "external-content/exploreARTIS-readme.html#installation",
    "href": "external-content/exploreARTIS-readme.html#installation",
    "title": "exploreARTIS",
    "section": "",
    "text": "Mac Users Run the following commands in terminal:\nbrew install pkg-config\nbrew install gdal\nOnce installed run the following command in the R console:\ninstall.packages(\"sf\", configure.args = \"--with-proj-lib=/usr/local/lib/\")\nWindows Users Please make sure you have Rtools installed first. Follow the instructions here. Then run the following command in the R console:\ninstall.packages(\"sf\")\n\n\n\nYou can install this package with the devtools package. The first time you do it you will have to run\ninstall.packages(\"devtools\")\nlibrary(devtools)\nThen, you can run\ndevtools::install_github(\"Seafood-Globalization-Lab/exploreARTIS\", dependencies = TRUE)\nAfter you install the exploreARTIS package, you can just load it with library(exploreARTIS). You will also need to reinstall the package whenever there are updates to the package code."
  },
  {
    "objectID": "external-content/exploreARTIS-readme.html#artis-data-structure",
    "href": "external-content/exploreARTIS-readme.html#artis-data-structure",
    "title": "exploreARTIS",
    "section": "",
    "text": "ARTIS data consists of the following variables:\n\nexporter_iso3c (string): Exporter Country ISO 3 code\nimporter_iso3c (string): Importer Country ISO 3 code\nsource_country_iso3c (string): Producer Country ISO 3 code\ndom_source (string): Domestic Export / Foreign Export / Error Export\nhs6 (string): 6-digit HS commodity code\nsciname (string): Species or Species group\nenvironment (string): Marine / Freshwater\nmethod (string): Capture / Aquaculture / Unknown\nproduct_weight_t (double): Product weight (tonnes)\nlive_weight_t (double): Live weight (tonnes)\nhs_version (string): version of HS codes\nyear (double): Year"
  },
  {
    "objectID": "external-content/exploreARTIS-readme.html#filtering-artis-data",
    "href": "external-content/exploreARTIS-readme.html#filtering-artis-data",
    "title": "exploreARTIS",
    "section": "",
    "text": "If you have downloaded bulk ARTIS data, it is generally split into separate csv files for each HS version and year combination. This is because the combined file is large and slow to load, sometimes causing users’ R sessions to crash. If you would like to combine files into a single data frame, you will need to pick which HS version-year combinations you would like to include. Then, you can decide if you would like to filter down any of the variabales (e.g., keep select exporters, species, etc.). Once you have made these decisions, you can use the example script scripts/filter_bulk_artis.R as a starting place to loop through the desired ARTIS files and filter based on your specified criteria. Note that this is not a function, but rather an example script with comments to facilitate customization for your own project.\n\n\n\nOnce you have the ARTIS data frame you are using for your analysis, you may still want to filter it for specific visualizations. For example, you may be working with all trade for a given country but want to generate a plot for just one species. You can of course use any base R or tidyverse functions to filter the data, but we also provide a function in this package to filter any of the ARTIS variables: filter_artis(). The filtered data frame can then be passed to any visualization function.\n# loading library\nlibrary(exploreARTIS)\n\n# Filter ARTIS data to Chilean exports of Atlantic salmon in 2016-2020\nfilter_artis(mini_artis, year = 2016:2020, exporter = \"CHL\", species = \"salmo salar\")"
  },
  {
    "objectID": "external-content/exploreARTIS-readme.html#visualization-examples",
    "href": "external-content/exploreARTIS-readme.html#visualization-examples",
    "title": "exploreARTIS",
    "section": "",
    "text": "Here are examples of all types of plots that can be created with this package. mini_artis is dataframe with a subset of ARTIS data that is included in the exploreARTIS package.\n\n\nplot_bar() creates ranked bar plots, with bar categories indicated by the bar_group argument. The number of bars to display can be controlled with top_n. argument.\n# loading library\nlibrary(exploreARTIS)\n\n# Bar chart visualizing seafood trade volumes by exporter\nplot_bar(mini_artis, bar_group = \"exporter_iso3c\")\n\n\n\nBar charts can optionally be filled by an ARTIS variable.\n# loading library\nlibrary(exploreARTIS)\n\n# Bar chart visualizing seafood trade volumes by exporter and filling by export source\nplot_bar(mini_artis, bar_group = \"exporter_iso3c\", fill_type = \"dom_source\")\n\n\n\n\n\n\nplot_ts() creates time series line or area plots for any specified artis_var. The plot.type argument allows options of “line” (default) or “stacked” to change plot views. To keep the number of colors reasonable, the prop_flow_cutoff argument groups lines falling below the cut-off into “other” and this can be adjusted to show more or fewer lines/fills.\n# loading library\nlibrary(exploreARTIS)\n\nplot_ts(mini_artis, artis_var = \"exporter_iso3c\")\n\n\n\nA stacked line graph of all export partners in the ARTIS dataset\n# loading library\nlibrary(exploreARTIS)\n\nplot_ts(mini_artis, artis_var = \"exporter_iso3c\", plot.type = \"stacked\")\n\n\n\n\n\n\nplot_sankey() creates a sankey plot to display flows among nodes across columns. This function is flexible in allowing the user to specify which data columns should be used to produce the colunns of the sankey plot. This function includes the argument prop_flow_cutoff to control how many groups are included in “other” (which can help keep the larger flows readable). It also includes an argumen to drop the group “other” entirely if preferred.\n# loading library\nlibrary(exploreARTIS)\n\n# Sankey plot of all seafood trade\nplot_sankey(mini_artis, cols = c(\"sciname\", \"exporter_iso3c\", \"importer_iso3c\"))\n\n\n\n\n\n\nr plot_chord() creates a chord diagram for visualizing flows among countries/regions.\n# loading library\nlibrary(exploreARTIS)\n\n# Chord diagram of all seafood trade\nplot_chord(mini_artis, region_colors = region7_palette)\n\n\n\nChord Diagram - all seafood trade\n\n\nIndividual countries can be pulled out to highlight their trade by specifying the country/countries’ iso3c code(s) in the focal_country argument.\n# loading library\nlibrary(exploreARTIS)\n\n# Chord diagram of all seafood trade with Vietnam highlighted\nplot_chord(mini_artis, focal_country = \"VNM\", region_colors = region7_palette)\n\n\n\nChord Diagram - all seafood trade\n\n\n\n\n\nplot_map() creates maps that are optionally colored by country_fill and optionally include flow arrows colored by flow volume with flow_arrows. The number of arrows can be specified with n_flows.\n# loading library\nlibrary(exploreARTIS)\n\n# Map of top seafood exports and flows\nplot_map(mini_artis, country_fill = \"importer_iso3c\", flow_arrows = TRUE, arrow_label = \"Trade (live t)\", fill_label = \"Import (live t)\")\n\n\n\nIndividual country’s trade flows can be isolated by filtering the importer or exporter column before passing it to the plot function.\n# loading library\nlibrary(exploreARTIS)\n\n# Map of seafood exports from Chile\nmini_artis %&gt;% filter(exporter_iso3c == \"CHL\") %&gt;% plot_map(country_fill = \"importer_iso3c\", flow_arrows = TRUE, arrow_label = \"Trade (live t)\", fill_label = \"Import (live t)\")\n\n\n\n\n\n\nBoth plot_ts() and r plot_bar() allow facetting by an ARTIS variable with the facet_variable argument. If a facet variable is specified then facet_values must also be defined, either as a number (the number of facets to create) or a vector (the specific facets to create).\n# loading libraries\nlibrary(exploreARTIS)\n\n# Area plot of top importers facetted by method\nplot_ts(mini_artis, artis_var = \"importer_iso3c\", plot.type = \"stacked\", facet_variable = \"method\", facet_values = c(\"capture\", \"aquaculture\"))\n\n\n\n# loading libraries\nlibrary(exploreARTIS)\n\n# Bar plot of top importers facetted by method\nplot_bar(mini_artis, bar_group = \"importer_iso3c\", facet_variable = \"method\", facet_values = c(\"capture\", \"aquaculture\"))"
  }
]