[
  {
    "objectID": "external-content/artis-hpc-readme.html",
    "href": "external-content/artis-hpc-readme.html",
    "title": "ARTIS HPC",
    "section": "",
    "text": "This repository outlines the instructions and scripts needed to create the ARTIS High Performance Computer (HPC) on Amazon Web Services (AWS).\n\n\n\nCopy the most up-to-date set of model inputs to the project root directory artis-hpc\nCopy the most up-to-date ARTIS R package folder and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package NAMESPACE file and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package DESCRIPTION file and place within artis-hpc/docker_image_files_original/\n\nIf running on a new Apple chip arm64: 1. Copy arm64_venv_requirements.txt file from the root directory to the artis-hpc/docker_image_files_original/ 2. Rename the file artis-hpc/docker_image_files_original/arm64_venv_requirements.txt to artis-hpc/docker_image_files_original/requirements.txt 3. Open artis-hpc/docker_image_files_original/run_artis_hs12.R and uncomment line 20.\n\n\n\n\nTerraform\n\nThis is a set of code scripts that create all the AWS infrastructure needed for the ARTIS HPC\nDestroy all AWS infrastructure for the ARTIS HPC after the ARTIS model has finished (save on unnecessary costs)\n\nDocker\n\nThis is used to create a docker image that our HPC jobs will use to run the ARTIS model code\n\nPython\n\nThrough the docker and AWS python (boto3) clients, this will provide code that:\n\nPush all model input data to AWS S3\nBuild docker image needed that the AWS Batch jobs will need to run ARTIS model\nPush docker image to AWS ECR\nSubmit jobs to ARTIS HPC\nPull all model outputs data\n\n\n\n\n\n\n\nHomebrew\nAWS CLI\nTerraform CLI\nPython\n\nPython packages\n\ndocker\nboto3\n\n\n\n\n\n\nInstall homebrew by running the terminal command /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nClose existing terminal window where installation command was run and open a new terminal window\nConfirm homebrew has been installed, run terminal command brew --version, no error messsage should appear.\n\nIf after homebrew installation you get a message stating brew command not found: 1. Edit zsh config file, run terminal command: vim ~/.zshrc 2. Type “i” to enter edit mode 3. Copy paste this line into the file you opened: export PATH=/opt/homebrew/bin:$PATH 4. Press Shift and : 5. Type “wq” 6. Press enter 7. Source new config file, run terminal command source ~/.zshrc\n\n\n\nFollowing instructions from AWS\nNote: If you already have AWS CLI installed please still confirm by following step 3 below. Both instructions should run without an error message.\nThe following instructions are for MacOS users: 1. Run terminal command curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" 2. Run terminal command sudo installer -pkg AWSCLIV2.pkg -target / 3. Confirm AWS CLI has been installed: 1. Run terminal command which aws 2. Run terminal command aws --version\n\n\n\nNote: If you already have homebrew installed please confirm by running brew --version, no error message should occur.\nTo install terraform on MacOS we will be using homebrew. If you do not have homebrew installed on your computer please follow the installation instructions here, before continuing.\nBased on Terraform CLI installation instructions provided here. 1. Run terminal command brew tap hashicorp/tap 2. Run terminal command brew install hashicorp/tap/terraform 3. Run terminal command brew update 4. Run terminal command brew upgrade hashicorp/tap/terraform\nIf this has been unsuccessful you might need to install xcode command line tools, try: - Run terminal command: sudo xcode-select --install\n\n\n\n\n\nAn AWS root user was created\nAWS root user has created an admin user group with “AdministratorAccess” permissions.\nAWS root user has created IAM users\nAWS root user has add IAM users to admin group\nAWS IAM users have their AWS AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY\n\nTo create an AWS root user visit aws.amazon.com.\nTo create an AWS IAM user: - FIXIT: include screenshots for creating an IAM user with the correct admin permissions.\nNote: If you created ANY AWS RESOURCES for ARTIS manually please delete these before continuing. These resources should\n\n\n\nFIXIT: Explain what each terminal command is doing 1. Run terminal command: export AWS_ACCESS_KEY=[YOUR_AWS_ACCESS_KEY] 2. Run terminal command: export AWS_SECRET_ACCESS_KEY=[YOUR_AWS_SECRET_ACCESS_KEY] 3. Run terminal command: export AWS_REGION=us-east-1 4. Run terminal command aws configure set aws_access_key_id \"[YOUR_AWS_ACCESS_KEY]\" 5. Run terminal command aws configure set aws_secret_access_key \"[YOUR_AWS_SECRET_KEY]\"\n\n\n\nNote: Please make sure that your terminal is currently in your working directory that should end in artis-hpc, by running the terminal command pwd.\n\nCreate a virtual environment, run terminal command:python3 -m venv venv\nOpen virtual environment, run terminal command: source venv/bin/activate\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nCheck that all python modules have been downloaded, run terminal command pip freeze and check that all modules in the requirements.txt file are included.\n\nIf an error occurs please follow these instructions: 1. Upgrade your version of pip by running terminal command: pip install --upgrade pip 2. Install all required python modules, run terminal command: pip3 install -r requirements.txt 3. If errors still occur install each python package in the requirements.txt file individually, run terminal command pip3 install [PACKAGE NAME] ie pip3 install urllib3.\n\n\n\nNote: the initial_setup.py script will create all necessary AWS infrastructure, upload all model inputs to an AWS S3 bucket, and create and upload a docker image based on the ARTIS codebase. It will also submit jobs to the ARTIS HPC.\n\nOpen Docker Desktop\nTake note of any existing docker images and container relating to other projects, and delete all docker container relating to ARTIS, delete all docker images relating to ARTIS.\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nIf you are using an Apple Silicone chip (M1, M2, M3, etc) your chip will be “arm64”, otherwise for intel chips it will be “x86”\n\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: This will create the docker image from scratch. If you have an existing docker image you would like to use include the -di [existing docker image name] with the command.\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name] -di [existing docker image name]:latest\n\n\n\nExample: - If you are creating the docker image from scratch (If you change any R code for ARTIS you will have to recreate a docker image): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image - If you have an existing docker image (for example only need to re-upload a new set of model inputs): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image -di myname-artis-image:latest\nNote: If terraform states that it created all resources however when you log into the AWS console to confirm cannot see them, they have most likely been created as part of another account. Run terraform destroy -auto-approveon the command line. Confirmed you have followed the AWS CLI set up instructions with the correct set of keys (AWS access key and AWS secret access key)."
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#update-artis-model-scripts-and-model-inputs",
    "href": "external-content/artis-hpc-readme.html#update-artis-model-scripts-and-model-inputs",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Copy the most up-to-date set of model inputs to the project root directory artis-hpc\nCopy the most up-to-date ARTIS R package folder and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package NAMESPACE file and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package DESCRIPTION file and place within artis-hpc/docker_image_files_original/\n\nIf running on a new Apple chip arm64: 1. Copy arm64_venv_requirements.txt file from the root directory to the artis-hpc/docker_image_files_original/ 2. Rename the file artis-hpc/docker_image_files_original/arm64_venv_requirements.txt to artis-hpc/docker_image_files_original/requirements.txt 3. Open artis-hpc/docker_image_files_original/run_artis_hs12.R and uncomment line 20."
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#technologies-used",
    "href": "external-content/artis-hpc-readme.html#technologies-used",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Terraform\n\nThis is a set of code scripts that create all the AWS infrastructure needed for the ARTIS HPC\nDestroy all AWS infrastructure for the ARTIS HPC after the ARTIS model has finished (save on unnecessary costs)\n\nDocker\n\nThis is used to create a docker image that our HPC jobs will use to run the ARTIS model code\n\nPython\n\nThrough the docker and AWS python (boto3) clients, this will provide code that:\n\nPush all model input data to AWS S3\nBuild docker image needed that the AWS Batch jobs will need to run ARTIS model\nPush docker image to AWS ECR\nSubmit jobs to ARTIS HPC\nPull all model outputs data"
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#installation",
    "href": "external-content/artis-hpc-readme.html#installation",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Homebrew\nAWS CLI\nTerraform CLI\nPython\n\nPython packages\n\ndocker\nboto3\n\n\n\n\n\n\nInstall homebrew by running the terminal command /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nClose existing terminal window where installation command was run and open a new terminal window\nConfirm homebrew has been installed, run terminal command brew --version, no error messsage should appear.\n\nIf after homebrew installation you get a message stating brew command not found: 1. Edit zsh config file, run terminal command: vim ~/.zshrc 2. Type “i” to enter edit mode 3. Copy paste this line into the file you opened: export PATH=/opt/homebrew/bin:$PATH 4. Press Shift and : 5. Type “wq” 6. Press enter 7. Source new config file, run terminal command source ~/.zshrc\n\n\n\nFollowing instructions from AWS\nNote: If you already have AWS CLI installed please still confirm by following step 3 below. Both instructions should run without an error message.\nThe following instructions are for MacOS users: 1. Run terminal command curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" 2. Run terminal command sudo installer -pkg AWSCLIV2.pkg -target / 3. Confirm AWS CLI has been installed: 1. Run terminal command which aws 2. Run terminal command aws --version\n\n\n\nNote: If you already have homebrew installed please confirm by running brew --version, no error message should occur.\nTo install terraform on MacOS we will be using homebrew. If you do not have homebrew installed on your computer please follow the installation instructions here, before continuing.\nBased on Terraform CLI installation instructions provided here. 1. Run terminal command brew tap hashicorp/tap 2. Run terminal command brew install hashicorp/tap/terraform 3. Run terminal command brew update 4. Run terminal command brew upgrade hashicorp/tap/terraform\nIf this has been unsuccessful you might need to install xcode command line tools, try: - Run terminal command: sudo xcode-select --install"
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#assumptions",
    "href": "external-content/artis-hpc-readme.html#assumptions",
    "title": "ARTIS HPC",
    "section": "",
    "text": "An AWS root user was created\nAWS root user has created an admin user group with “AdministratorAccess” permissions.\nAWS root user has created IAM users\nAWS root user has add IAM users to admin group\nAWS IAM users have their AWS AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY\n\nTo create an AWS root user visit aws.amazon.com.\nTo create an AWS IAM user: - FIXIT: include screenshots for creating an IAM user with the correct admin permissions.\nNote: If you created ANY AWS RESOURCES for ARTIS manually please delete these before continuing. These resources should"
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#aws-cli-setup",
    "href": "external-content/artis-hpc-readme.html#aws-cli-setup",
    "title": "ARTIS HPC",
    "section": "",
    "text": "FIXIT: Explain what each terminal command is doing 1. Run terminal command: export AWS_ACCESS_KEY=[YOUR_AWS_ACCESS_KEY] 2. Run terminal command: export AWS_SECRET_ACCESS_KEY=[YOUR_AWS_SECRET_ACCESS_KEY] 3. Run terminal command: export AWS_REGION=us-east-1 4. Run terminal command aws configure set aws_access_key_id \"[YOUR_AWS_ACCESS_KEY]\" 5. Run terminal command aws configure set aws_secret_access_key \"[YOUR_AWS_SECRET_KEY]\""
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#python-installation",
    "href": "external-content/artis-hpc-readme.html#python-installation",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Note: Please make sure that your terminal is currently in your working directory that should end in artis-hpc, by running the terminal command pwd.\n\nCreate a virtual environment, run terminal command:python3 -m venv venv\nOpen virtual environment, run terminal command: source venv/bin/activate\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nCheck that all python modules have been downloaded, run terminal command pip freeze and check that all modules in the requirements.txt file are included.\n\nIf an error occurs please follow these instructions: 1. Upgrade your version of pip by running terminal command: pip install --upgrade pip 2. Install all required python modules, run terminal command: pip3 install -r requirements.txt 3. If errors still occur install each python package in the requirements.txt file individually, run terminal command pip3 install [PACKAGE NAME] ie pip3 install urllib3."
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#creating-aws-infrastructure-with-a-setup-file",
    "href": "external-content/artis-hpc-readme.html#creating-aws-infrastructure-with-a-setup-file",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Note: the initial_setup.py script will create all necessary AWS infrastructure, upload all model inputs to an AWS S3 bucket, and create and upload a docker image based on the ARTIS codebase. It will also submit jobs to the ARTIS HPC.\n\nOpen Docker Desktop\nTake note of any existing docker images and container relating to other projects, and delete all docker container relating to ARTIS, delete all docker images relating to ARTIS.\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nIf you are using an Apple Silicone chip (M1, M2, M3, etc) your chip will be “arm64”, otherwise for intel chips it will be “x86”\n\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: This will create the docker image from scratch. If you have an existing docker image you would like to use include the -di [existing docker image name] with the command.\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name] -di [existing docker image name]:latest\n\n\n\nExample: - If you are creating the docker image from scratch (If you change any R code for ARTIS you will have to recreate a docker image): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image - If you have an existing docker image (for example only need to re-upload a new set of model inputs): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image -di myname-artis-image:latest\nNote: If terraform states that it created all resources however when you log into the AWS console to confirm cannot see them, they have most likely been created as part of another account. Run terraform destroy -auto-approveon the command line. Confirmed you have followed the AWS CLI set up instructions with the correct set of keys (AWS access key and AWS secret access key)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aquatic Resource Trade in Species (ARTIS) Manual",
    "section": "",
    "text": "It’s possible to create beautiful documentation to share online with Quarto that auto-updates with GitHub. This is very new and incredibly cool. This tutorial is an example of a quarto website — it is a really powerful way to create and share your work. You can communicate about science using the same reproducible workflow you and/or your colleagues use for analyses, whether or not you write code.\nCreating websites with Quarto can be done without knowing R, Python or HTML, CSS, etc, and that’s where we’ll start. However, Quarto integrates with these tools so you can make your websites as complex and beautiful as you like as you see examples and reuse and remix from others in the open community. This tutorial borrows heavily from a lot of great tutorials and resources you should check out too – there are links throughout.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Aquatic Resource Trade in Species (ARTIS) Manual",
    "section": "",
    "text": "It’s possible to create beautiful documentation to share online with Quarto that auto-updates with GitHub. This is very new and incredibly cool. This tutorial is an example of a quarto website — it is a really powerful way to create and share your work. You can communicate about science using the same reproducible workflow you and/or your colleagues use for analyses, whether or not you write code.\nCreating websites with Quarto can be done without knowing R, Python or HTML, CSS, etc, and that’s where we’ll start. However, Quarto integrates with these tools so you can make your websites as complex and beautiful as you like as you see examples and reuse and remix from others in the open community. This tutorial borrows heavily from a lot of great tutorials and resources you should check out too – there are links throughout.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-is-quarto",
    "href": "index.html#what-is-quarto",
    "title": "Aquatic Resource Trade in Species (ARTIS) Manual",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto helps you have your ideas and your code in one place, and present it in a beautiful way.\nQuarto unifies and extends the RMarkdown ecosystem - it unifies by combining the functionality of R Markdown, bookdown, distill, xaringian, etc into a single consistent system. And it extends in several ways: all features are possible beyond R too, including Python and Javascript. It also has more “guardrails”: accessibility and inclusion are centered in the design. Quarto is for people who love RMarkdown, and it’s for people who have never used RMarkdown.\nThe ability for Quarto to streamline collaboration has been so cool and important for our NASA Openscapes project. Quarto has been a common place for us to collaborate - across R and Python languages and coding expertise.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-is-this-tutorial",
    "href": "index.html#what-is-this-tutorial",
    "title": "Aquatic Resource Trade in Species (ARTIS) Manual",
    "section": "What is this tutorial?",
    "text": "What is this tutorial?\nThis is a 1-hour tutorial that can be used to teach or as self-paced learning.\nWe introduce Quarto by exploring this tutorial website, and practicing the basic Quarto workflow using different tools (GitHub browser, RStudio, and Jupyter) for editing your website.\nWe’ll start off from the browser so you don’t need to install any additional software, however this approach is very limited and you will soon outgrow its capabilities. If you don’t already have a workflow to edit files and sync to GitHub from your computer, I recommend RStudio. You don’t need to know R to use RStudio, and it has powerful editor features that make for happy workflows.\nQuarto.org is the go-to place for full documentation and more tutorials!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#example-quarto-sites",
    "href": "index.html#example-quarto-sites",
    "title": "Aquatic Resource Trade in Species (ARTIS) Manual",
    "section": "Example Quarto sites",
    "text": "Example Quarto sites\nA few Quarto websites from Openscapes - so far we have been using Quarto for documentation using Quarto and Markdown files and Jupyter Notebooks.\n\nChampions Lessons Series\nOpenscapes Approach Guide\nNASA Earthdata Cloud Cookbook\n2021 NASA Cloud Hackathon\nFaylab Lab Manual\nA Quarto tip a day, by Mine Çetinkaya-Rundel",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Aquatic Resource Trade in Species (ARTIS) Manual",
    "section": "About",
    "text": "About\nOpenscapes is about better science for future us. We help researchers reimagine data analysis, develop modern skills that are of immediate value to them, and cultivate collaborative and inclusive research teams as part of the broader global open movement.\nWe’re developing this tutorial to help folks with different levels of technical skills use Quarto for documentation and tutorial building. This tutorial was originally created for several different audiences: NASA-Openscapes researcher support engineers using Python, communications directors at organizations promoting open science who do not identify as coders, and fisheries scientists curious about transitioning from RMarkdown. We’re hoping it’s useful to folks with backgrounds as wide as these; if you find it useful or have suggestions for improvement, please let us know by clicking “Edit this page” or “Report an issue” at the upper right side of any page.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "artis-our-model/est-trade-net.html",
    "href": "artis-our-model/est-trade-net.html",
    "title": "National mass-balance",
    "section": "",
    "text": "Estimating species bilateral trade flows occurs in two steps: first, solving the national production-trade mass balance, and second, converting reported commodity trade flow estimates to species trade flow estimates based on the estimated species mix going into each domestic and foreign exported commodity.\n\nNational mass-balance\nWe start with the fact that exports must equal production and imports, minus consumption. Since exports are reported as commodities, we solve this mass balance problem in terms of commodities. Production data are reported for each species, so we estimate the elements of a matrix that represents the proportion of production going into each commodity. Since an imported commodity can be processed and exported as a different commodity, we also estimate the proportion of each import being converted into a different commodity. Then for a given country,\ne = V_1∘X⋅p+V_2∘W⋅g-c+ϵ\nIf n is the number of species and m is the number of commodities, then: V_1 is a sparse (m×n) matrix with product conversion factors corresponding to the unknowns in X; X is a sparse (m×n) matrix of the proportion of each species in each commodity; p is a vector of domestic species production (n×1); V_2 is a sparse (m×m) matrix with product conversion factors corresponding to the entries of W; W is a (m×m) matrix of the processed imported commodities; g be a vector of imports (m×1), c is a vector of domestic consumption (m×1), and; ϵ is a vector of error terms (m×1).\nWe compiled reported values for V_1, V_2, e, p and g, and estimate the entries of X, W, c, and ϵ. We first converted this problem to a system of linear equations. Using the property that vec(ABC)=(C^T⊗A)vec(B), we can create A_b=(y^T⊗D_m)D_V, where D_m is a diagonal matrix of ones, with dimension m and D_V is a diagonal matrix with the elements of vec(V). The vector of unknowns is then x_b=vec(Z). We then solve this system of equations with a quadratic optimization solver such that the mass balance equalities are satisfied, trade codes with higher species resolution in X are prioritized, the elements of X, W, and c are otherwise relatively even (i.e., we assume an even distribution of production among commodities unless the data suggests otherwise), that ϵ is as small as possible (i.e., minimize the error), and all unknowns are greater than or equal to zero.\nPositive error terms represent situations where reported production and imports cannot explain exports. This can occur due to under- or un-reported production or imports, over-reporting of exports, errors in the live weight conversion factors, or inconsistencies in the year production and trade are attributed to.\nWe solve the mass-balance problem for each country-year-HS version combination using the Python package “solve_qp.” The estimated species mixes in national production (X), processing of imports (W) and the error term (ϵ) are passed to the next stage of the analysis.\n\n\nConverting the product trade network to a species trade network\nFirst, we compute the mix of species going into each trade code for each country’s domestic exports. To do this, we reweight X so it represents the proportion of each species in each code rather than the proportion of production of a species going into each product. Each country’s estimated X matrix is multiplied by p to get the mass of each species in each commodity. The total mass of each commodity is found by summing all the species volume grouped by commodity and the proportion of each species within a commodity is then calculated by dividing all volumes by their respective commodity mass totals.\nEach country’s exports can be sourced from domestic production, imported products that are subsequently exported, with or without processing (i.e., foreign exports), or from an unknown source (i.e., error exports). Since the mix of these sources cannot be derived from the mass balance equation alone, we calculate a range for sourcing following33. We calculate the maximum possible domestic exports by taking the minimum between the domestic production and total exports. Similarly, we calculated the maximum volume of exports sourced from imports, by taking the minimum between each product’s imports (accounting for processing estimated by W) and exports. The minimum domestic exports are calculated as the minimum between production and the difference in exports and the maximum calculated foreign exports, with the remainder as error exports (minimum foreign exports are calculated in an analogous way). The above results represent midpoint estimates.\n\nmax domestic exportse_(domestic,max) = min(p_domestic domestic production,etotal exports)\nmax foreign exportse_(foreign,max) = min(importsg,total exportse)\nmin domestic exports e_(domestic,min)= min(pdomestic production,etotal exports - e_(foreign,max) max foreign exports)\ne_(foreign,min) min foreign exports = min(importsg,total exportse - me_(domestic,max) ax domestic exports)\ne_(domestic,mid) midpoint domestic exports = (e_(domestic,max) max domestic exports + e_(domestic,min) min domestic exports)/2\ne_(foreign,mid) midpoint foreign exports = (e_(foreign,max) max foreign exports + e_(foreign,min) min foreign exports)/2\n\nFor these three estimates (maximum, minimum and midpoint) we calculate the domestic and foreign weights by dividing domestic export values and foreign export values by total export. We then distribute each country’s exports into domestic, foreign and error exports by multiplying exports by domestic, foreign and error proportions (Fig S8). For each export source, we apply a different species mix to each HS code based on the estimated source country. For domestic exports, we use the exporting country’s estimated X matrix (Fig S9). For error exports, the geographical origin is unknown and may arise from unreported production, so we cannot meaningfully assign a species mix to the code. Consequently, we identify the lowest taxonomic resolution common to all species within the code and assign that name to the trade flow.\nFor foreign exports, we trace the origins back in the supply chain a maximum of three steps (i.e., producer to intermediate exporter to final exporter to final importer), with any remaining foreign export or flows less than 1 tonne left as “unknown” source (Supplementary Fig. Fig S88). The small flows left unresolved comprise around 1% of total trade (Supplementary Fig. 8Fig S10). To link an export of foreign origin to its source country, we use a reweighted version of W to estimate the original imported product codes and connect those to their source country, using a proportional breakdown of each country’s imports of that code. Foreign exports of one country that originated from foreign exports of another country are isolated and undergo the process above to identify the source country. The species mix for foreign trade flows are based on either the source country’s estimated X matrix or the method described above for error exports (Supplementary Fig.9Fig S9).\n\n\nNetwork post-estimation processing\nOnce the species trade flow network is built, we remove all volumes traded below 0.1 tonnes, as the multiplication by small proportions generates overly specific, and likely unrealistic, small flows. Next, to generate a complete time series, we need to compile estimates from across the HS versions. All HS versions are reported since they have been created, for example HS96 reports trade from 1996 until the present. However, the more recent HS versions generally include more specific trade codes and therefore are preferred over older versions. It takes a few years before an HS version is fully adopted, resulting in lower total trade volumes for the first few years an HS version is available compared to the previous HS versions (Supplementary Fig. Fig S7). To provide the most accurate representation of trade, we create a continuous time series by adopting the most recent HS version available after its total trade has met up with the total trade reported under previous HS versions. This results in HS96 being used for 1996 - 2004, HS02 for 2004 - 2009, HS07 for 2010 - 2012 and HS12 for 2013 - 2020. To check the reasonability of estimated trade flows, we first confirmed that all trade flows sum to the original BACI trade flows when grouped by exporter, importer, year, HS code and expressed as product weight. Note that some flows are slightly lower due to the 0.1 tonne threshold (maximum difference of 72 tonnes representing a percent difference of 0.19%). Second, we confirmed that the estimates from the mass balance problem satisfy the problem constraints. Third, we checked that domestic exports of species in live weight equivalent do not exceed production of that species. Fourth, we confirmed that exports of foreign source do not exceed imports of that species. Only 1.4% of cases across all years showed a country’s foreign export of a species exceeded the total import of that species.\nNote: The original text this section is based on is from Gephart et al. (2024) Nature Communications [add link]. Please reference that paper when referencing this information: [Insert reference]",
    "crumbs": [
      "Our Model",
      "National mass-balance"
    ]
  },
  {
    "objectID": "artis-our-model/index.html",
    "href": "artis-our-model/index.html",
    "title": "Our Model",
    "section": "",
    "text": "Our Model",
    "crumbs": [
      "Our Model"
    ]
  },
  {
    "objectID": "artis-our-model/est-consumption.html",
    "href": "artis-our-model/est-consumption.html",
    "title": "Estimating Consuption",
    "section": "",
    "text": "Estimating Consuption\nThe species trade network and FAO production data were used to calculate national apparent consumption by scientific name, habitat, and method for each year.\nFirst, domestic production of products is estimated by multiplying production data by the corresponding estimated X matrix. We then calculate domestic consumption by subtracting domestic exports by HS code from domestic production of products. Domestic consumption by species is then derived based on the volume of domestic consumption and the estimated species composition for the associated code.\nForeign consumption represents the quantity of product imported that was consumed in country (i.e., not subsequently exported). To calculate foreign consumption, we subtract foreign exports from the quantity of processed imports. Processed imports represent the quantity of each product, by HS code, available after accounting for processing, by multiplying the appropriate estimated W by the import vector i. We convert processed imports to live weight by multiplying by the live weight conversion factor. To distinguish human consumable products, we filter out all processed product consumption of fishmeal, fish oil and ornamental products.\nFinally, we disaggregated foreign consumption of processed HS products to species. We assume the species and trade sourcing distribution of foreign consumption of a given code is proportional to the species distribution of the original imported HS products from which a final code was sourced based on the estimated species trade network. We therefore disaggregate foreign consumption by multiplying foreign consumption by the trade flow proportions of imports across all trade partners and species information.\nSince apparent consumption is based on a disappearance model, estimated values are subject to multiple sources of error. Due to discrepancies in production and trade reporting for select countries (e.g., as arises with joint ventures), a few countries had unrealistically large estimated per capita consumption. For country specific consumption estimates, we capped total per capita consumption to 100 kg, as this is slightly above the upper estimate FAOSTAT25. We then adjusted the supply by export partners, scientific name, production method, habitat, and source country for those countries proportionally. A second factor that influences estimated apparent consumption relates to the approach for removing non-human consumable products, particularly the domestic production and use of fishmeal. While we base our estimates on the species mix entering HS code 230120, this is estimated based on exports and domestic use patterns could diverge, leading to errors in the volume removed. Additionally, since many species can enter code 230120 and there is limited empirical data to inform volumes of species entering fishmeal by country, there is greater uncertainty in the exact species mix, and therefore greater uncertainty in the species volumes to exclude from direct human consumption calculations.",
    "crumbs": [
      "Our Model",
      "Estimating Consuption"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#creating-python-virtual-environment",
    "href": "artis-run-model/run-locally.html#creating-python-virtual-environment",
    "title": "Running Locally",
    "section": "Creating python virtual environment",
    "text": "Creating python virtual environment\n\nOpen the artis-model repository in RStudio.\nClick on the terminal tab.\nType “pwd” in the terminal.\nCopy the result of the “pwd” terminal command.\nType “python3 -m venv [RESULT FROM pwd]/venv” (ie. “python3 -m venv /home/artis-model/venv”)\nType “source venv/bin/activate” in terminal.\nType “pip3 install qpsolvers” in terminal.\nType “pip3 install quadprog” in terminal.\nType “pip3 install cvxopt” in terminal.\nConfirm you have successfully installed the packages qpsolvers, quadprog, cvxopt by running “pip list”.\nType “deactivate” in terminal.\nClick on the Console tab.\n\nNote that you only need to install the solvers the first time you run this code. Warnings about the latest version of pip may also appear during the installation - these are okay, but errors are not.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#high-level-overview",
    "href": "artis-run-model/run-locally.html#high-level-overview",
    "title": "Running Locally",
    "section": "High level overview",
    "text": "High level overview\nThe following diagrams describes how ARTIS trade records are obtained.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#code-workflows",
    "href": "artis-run-model/run-locally.html#code-workflows",
    "title": "Running Locally",
    "section": "Code workflows",
    "text": "Code workflows\nThe following diagrams describe the how the codebase follows the workflow illustrated above.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-faq.html#why-dont-the-trade-totals-match-what-a-country-reports",
    "href": "artis-faq.html#why-dont-the-trade-totals-match-what-a-country-reports",
    "title": "ARTIS Manual",
    "section": "Why don’t the trade totals match what a country reports?",
    "text": "Why don’t the trade totals match what a country reports?",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#how-is-distant-water-fishing-captured",
    "href": "artis-faq.html#how-is-distant-water-fishing-captured",
    "title": "ARTIS Manual",
    "section": "How is distant water fishing captured?",
    "text": "How is distant water fishing captured?",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#what-is-the-effect-of-underreported-catch",
    "href": "artis-faq.html#what-is-the-effect-of-underreported-catch",
    "title": "ARTIS Manual",
    "section": "What is the effect of underreported catch?",
    "text": "What is the effect of underreported catch?",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#what-is-the-effect-of-products-in-transit-through-an-intermediate-country",
    "href": "artis-faq.html#what-is-the-effect-of-products-in-transit-through-an-intermediate-country",
    "title": "ARTIS Manual",
    "section": "What is the effect of products in transit through an intermediate country?",
    "text": "What is the effect of products in transit through an intermediate country?",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#why-are-there-sudden-spikes",
    "href": "artis-faq.html#why-are-there-sudden-spikes",
    "title": "ARTIS Manual",
    "section": "Why are there sudden spikes?",
    "text": "Why are there sudden spikes?\nThere are two common causes of artificial spikes. The first is related to improved resolution in a country’s production data. If a country previously reported production at the genus level, for example, but then starts reporting production at the species level, it could create a spike in the trade associated with the species name. In this case, one could look for a corresponding drop in the genus level data. The second reason is related to change in the HS code system. HS codes are updated every 5 years. To create a continuous time series, we default to providing",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#why-are-there-multiple-rows-for-a-single-hs-product-code-in-the-code-descriptions-metadata-file",
    "href": "artis-faq.html#why-are-there-multiple-rows-for-a-single-hs-product-code-in-the-code-descriptions-metadata-file",
    "title": "ARTIS Manual",
    "section": "Why are there multiple rows for a single HS product code in the code descriptions metadata file?",
    "text": "Why are there multiple rows for a single HS product code in the code descriptions metadata file?\nProducts table check why there are duplicates (usually caused by multiple presentations or states associated with one code, 030719, H5 probably HS12, 20 codes with 2 states or duplicates). Need to remove duplicates when joining.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-infrastructure/index.html",
    "href": "artis-infrastructure/index.html",
    "title": "Distribution Infrastructure",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla dolor lacus, suscipit sed fringilla vitae, finibus nec dui. Praesent porttitor bibendum nibh, sit amet imperdiet metus blandit quis. Vivamus leo velit, pretium ut porta non, eleifend at dui. Aenean quis purus eu nibh ornare ornare. Integer vulputate ipsum eu dolor auctor, et pretium nulla vehicula. Praesent vehicula nulla id mauris pharetra porta. Sed vitae libero fermentum, eleifend justo a, ornare est. Sed tincidunt dapibus tortor, vel scelerisque diam vestibulum eu. Nulla consectetur ut lectus vitae efficitur. Phasellus sit amet sodales purus. Proin et arcu vel erat finibus porta a sed arcu. Nullam pretium, nisi finibus viverra feugiat, lacus libero blandit erat, id dictum eros massa eget metus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla dolor lacus, suscipit sed fringilla vitae, finibus nec dui. Praesent porttitor bibendum nibh, sit amet imperdiet metus blandit quis. Vivamus leo velit, pretium ut porta non, eleifend at dui. Aenean quis purus eu nibh ornare ornare. Integer vulputate ipsum eu dolor auctor, et pretium nulla vehicula. Praesent vehicula nulla id mauris pharetra porta. Sed vitae libero fermentum, eleifend justo a, ornare est. Sed tincidunt dapibus tortor, vel scelerisque diam vestibulum eu. Nulla consectetur ut lectus vitae efficitur. Phasellus sit amet sodales purus. Proin et arcu vel erat finibus porta a sed arcu. Nullam pretium, nisi finibus viverra feugiat, lacus libero blandit erat, id dictum eros massa eget metus.",
    "crumbs": [
      "Distribution Infrastructure"
    ]
  },
  {
    "objectID": "artis-infrastructure/index.html#section",
    "href": "artis-infrastructure/index.html#section",
    "title": "Distribution Infrastructure",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla dolor lacus, suscipit sed fringilla vitae, finibus nec dui. Praesent porttitor bibendum nibh, sit amet imperdiet metus blandit quis. Vivamus leo velit, pretium ut porta non, eleifend at dui. Aenean quis purus eu nibh ornare ornare. Integer vulputate ipsum eu dolor auctor, et pretium nulla vehicula. Praesent vehicula nulla id mauris pharetra porta. Sed vitae libero fermentum, eleifend justo a, ornare est. Sed tincidunt dapibus tortor, vel scelerisque diam vestibulum eu. Nulla consectetur ut lectus vitae efficitur. Phasellus sit amet sodales purus. Proin et arcu vel erat finibus porta a sed arcu. Nullam pretium, nisi finibus viverra feugiat, lacus libero blandit erat, id dictum eros massa eget metus.",
    "crumbs": [
      "Distribution Infrastructure"
    ]
  },
  {
    "objectID": "artis-infrastructure/api.html",
    "href": "artis-infrastructure/api.html",
    "title": "API",
    "section": "",
    "text": "API",
    "crumbs": [
      "Distribution Infrastructure",
      "API"
    ]
  },
  {
    "objectID": "artis-infrastructure/sql.html",
    "href": "artis-infrastructure/sql.html",
    "title": "SQL Database",
    "section": "",
    "text": "SQL Database",
    "crumbs": [
      "Distribution Infrastructure",
      "SQL Database"
    ]
  },
  {
    "objectID": "artis-infrastructure/archive.html",
    "href": "artis-infrastructure/archive.html",
    "title": "Archiveed Versions (KNB)",
    "section": "",
    "text": "Archiveed Versions (KNB)\nPull from https://github.com/Seafood-Globalization-Lab/knb-submit/blob/main/README.md (Note: this README is not currently populated)",
    "crumbs": [
      "Distribution Infrastructure",
      "Archiveed Versions (KNB)"
    ]
  },
  {
    "objectID": "visualize-data.html",
    "href": "visualize-data.html",
    "title": "Visualize ARTIS data",
    "section": "",
    "text": "Visualize ARTIS data\npull from https://github.com/Seafood-Globalization-Lab/exploreARTIS/blob/main/README.md"
  },
  {
    "objectID": "artis-run-model/run-aws.html#update-artis-model-scripts-and-model-inputs",
    "href": "artis-run-model/run-aws.html#update-artis-model-scripts-and-model-inputs",
    "title": "Running on AWS",
    "section": "Update ARTIS model scripts and model inputs",
    "text": "Update ARTIS model scripts and model inputs\n\nCopy the most up-to-date set of model inputs to the project root directory artis-hpc\nCopy the most up-to-date ARTIS R package folder and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package NAMESPACE file and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package DESCRIPTION file and place within artis-hpc/docker_image_files_original/\n\nIf running on a new Apple chip arm64: 1. Copy arm64_venv_requirements.txt file from the root directory to the artis-hpc/docker_image_files_original/ 2. Rename the file artis-hpc/docker_image_files_original/arm64_venv_requirements.txt to artis-hpc/docker_image_files_original/requirements.txt 3. Open artis-hpc/docker_image_files_original/run_artis_hs12.R and uncomment line 20.",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#technologies-used",
    "href": "artis-run-model/run-aws.html#technologies-used",
    "title": "Running on AWS",
    "section": "Technologies used",
    "text": "Technologies used\n\nTerraform\n\nThis is a set of code scripts that create all the AWS infrastructure needed for the ARTIS HPC\nDestroy all AWS infrastructure for the ARTIS HPC after the ARTIS model has finished (save on unnecessary costs)\n\nDocker\n\nThis is used to create a docker image that our HPC jobs will use to run the ARTIS model code\n\nPython\n\nThrough the docker and AWS python (boto3) clients, this will provide code that:\n\nPush all model input data to AWS S3\nBuild docker image needed that the AWS Batch jobs will need to run ARTIS model\nPush docker image to AWS ECR\nSubmit jobs to ARTIS HPC\nPull all model outputs data",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#installation",
    "href": "artis-run-model/run-aws.html#installation",
    "title": "Running on AWS",
    "section": "Installation",
    "text": "Installation\n\nHomebrew\nAWS CLI\nTerraform CLI\nPython\n\nPython packages\n\ndocker\nboto3\n\n\n\n\nHomebrew installation\n\nInstall homebrew by running the terminal command /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nClose existing terminal window where installation command was run and open a new terminal window\nConfirm homebrew has been installed, run terminal command brew --version, no error messsage should appear.\n\nIf after homebrew installation you get a message stating brew command not found: 1. Edit zsh config file, run terminal command: vim ~/.zshrc 2. Type “i” to enter edit mode 3. Copy paste this line into the file you opened: export PATH=/opt/homebrew/bin:$PATH 4. Press Shift and : 5. Type “wq” 6. Press enter 7. Source new config file, run terminal command source ~/.zshrc\n\n\nAWS CLI installation\nFollowing instructions from AWS\nNote: If you already have AWS CLI installed please still confirm by following step 3 below. Both instructions should run without an error message.\nThe following instructions are for MacOS users: 1. Run terminal command curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" 2. Run terminal command sudo installer -pkg AWSCLIV2.pkg -target / 3. Confirm AWS CLI has been installed: 1. Run terminal command which aws 2. Run terminal command aws --version\n\n\nTerraform CLI installation\nNote: If you already have homebrew installed please confirm by running brew --version, no error message should occur.\nTo install terraform on MacOS we will be using homebrew. If you do not have homebrew installed on your computer please follow the installation instructions here, before continuing.\nBased on Terraform CLI installation instructions provided here. 1. Run terminal command brew tap hashicorp/tap 2. Run terminal command brew install hashicorp/tap/terraform 3. Run terminal command brew update 4. Run terminal command brew upgrade hashicorp/tap/terraform\nIf this has been unsuccessful you might need to install xcode command line tools, try: - Run terminal command: sudo xcode-select --install",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#assumptions",
    "href": "artis-run-model/run-aws.html#assumptions",
    "title": "Running on AWS",
    "section": "Assumptions:",
    "text": "Assumptions:\n\nAn AWS root user was created\nAWS root user has created an admin user group with “AdministratorAccess” permissions.\nAWS root user has created IAM users\nAWS root user has add IAM users to admin group\nAWS IAM users have their AWS AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY\n\nTo create an AWS root user visit aws.amazon.com.\nTo create an AWS IAM user: - FIXIT: include screenshots for creating an IAM user with the correct admin permissions.\nNote: If you created ANY AWS RESOURCES for ARTIS manually please delete these before continuing. These resources should",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#aws-cli-setup",
    "href": "artis-run-model/run-aws.html#aws-cli-setup",
    "title": "Running on AWS",
    "section": "AWS CLI Setup",
    "text": "AWS CLI Setup\nFIXIT: Explain what each terminal command is doing 1. Run terminal command: export AWS_ACCESS_KEY=[YOUR_AWS_ACCESS_KEY] 2. Run terminal command: export AWS_SECRET_ACCESS_KEY=[YOUR_AWS_SECRET_ACCESS_KEY] 3. Run terminal command: export AWS_REGION=us-east-1 4. Run terminal command aws configure set aws_access_key_id \"[YOUR_AWS_ACCESS_KEY]\" 5. Run terminal command aws configure set aws_secret_access_key \"[YOUR_AWS_SECRET_KEY]\"",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#python-installation",
    "href": "artis-run-model/run-aws.html#python-installation",
    "title": "Running on AWS",
    "section": "Python Installation",
    "text": "Python Installation\nNote: Please make sure that your terminal is currently in your working directory that should end in artis-hpc, by running the terminal command pwd.\n\nCreate a virtual environment, run terminal command:python3 -m venv venv\nOpen virtual environment, run terminal command: source venv/bin/activate\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nCheck that all python modules have been downloaded, run terminal command pip freeze and check that all modules in the requirements.txt file are included.\n\nIf an error occurs please follow these instructions: 1. Upgrade your version of pip by running terminal command: pip install --upgrade pip 2. Install all required python modules, run terminal command: pip3 install -r requirements.txt 3. If errors still occur install each python package in the requirements.txt file individually, run terminal command pip3 install [PACKAGE NAME] ie pip3 install urllib3.",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#creating-aws-infrastructure-with-a-setup-file",
    "href": "artis-run-model/run-aws.html#creating-aws-infrastructure-with-a-setup-file",
    "title": "Running on AWS",
    "section": "Creating AWS Infrastructure with a setup file",
    "text": "Creating AWS Infrastructure with a setup file\nNote: the initial_setup.py script will create all necessary AWS infrastructure, upload all model inputs to an AWS S3 bucket, and create and upload a docker image based on the ARTIS codebase. It will also submit jobs to the ARTIS HPC.\n\nOpen Docker Desktop\nTake note of any existing docker images and container relating to other projects, and delete all docker container relating to ARTIS, delete all docker images relating to ARTIS.\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nIf you are using an Apple Silicone chip (M1, M2, M3, etc) your chip will be “arm64”, otherwise for intel chips it will be “x86”\n\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: This will create the docker image from scratch. If you have an existing docker image you would like to use include the -di [existing docker image name] with the command.\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name] -di [existing docker image name]:latest\n\n\n\nExample: - If you are creating the docker image from scratch (If you change any R code for ARTIS you will have to recreate a docker image): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image - If you have an existing docker image (for example only need to re-upload a new set of model inputs): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image -di myname-artis-image:latest\nNote: If terraform states that it created all resources however when you log into the AWS console to confirm cannot see them, they have most likely been created as part of another account. Run terraform destroy -auto-approveon the command line. Confirmed you have followed the AWS CLI set up instructions with the correct set of keys (AWS access key and AWS secret access key).",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-our-model/data-inputs.html",
    "href": "artis-our-model/data-inputs.html",
    "title": "Production",
    "section": "",
    "text": "Production\nAquatic resource production comes from the Food and Agriculture Organization (FAO), which provides national capture and aquaculture production5. The Food and Agriculture Organization provides annual capture and aquaculture production data for around 240 countries, territories, or land areas from 1950 to 2020. The FAO data reports production in tonnes (live weight equivalent) of around 550 farmed and 1600 wild capture species and species groups. FAO production data consists primarily of official national statistics, with some verifiable supplemental information from academic reviews, consultant reports, and other specialist literature. Data reported by nations are checked by the FAO for consistency and questionable values are verified with the reporting offices. When countries fail to report production, FAO uses past values to estimate production. For the purposes of this analysis, we do not distinguish between nationally reported, and FAO estimated values.\nAccording to the Coordinating Working Party on Fishery Statistics, catch and landings should be assigned to the country of the flag flown by the fishing vessel irrespective of the location of the fishing. This means that production resulting from a country operating a fishing vessel in a foreign country’s territory should be recorded in the national statistics of the foreign fishing vessel. However, if the vessel is chartered by a company based in the home country or the vessel is fishing for the country under a joint venture contract or similar agreement and the operation is integral to the economy of the host country, this does not apply. Consequently, our estimates of source country generally represent who harvested or caught the aquatic resource regardless of where it was produced (i.e., distant water fishing would generally be attributed to the flag state). In cases of exceptions related to select chartered foreign vessels, joint ventures, or other similar agreements, catch by a foreign vessel but reported by the host country may not match trade reporting if catch does not move through the customs boundary. These instances generate excess apparent consumption.\n\n\nBilateral trade data\nWe use the CEPII BACI world trade database, which is a reconciled version of the UN Comtrade database49. Trade data are reported to the UN by both importers and exporters following the Harmonized System (HS) codes. The HS trade code system organizes traded goods into a hierarchy, with the highest level represented by two-digit codes (e.g., Chapter 03 covers “Fish and Crustaceans, Molluscs and Other Aquatic Invertebrates”), which are broken down into 4-digit headings (e.g., heading 0301 covers “Live fish”), which are then subdivided into 6-digit subheadings (e.g., subheading 030111 covers “Live ornamental freshwater fish”). National statistics offices may further subdivide HS codes into 7- to 12-digit codes but since these are not standard across countries, the HS 6-digit codes are the most highly resolved trade codes available globally. HS codes are administered by the World Customs Organization, which updates the codes every five years. HS versions can be used from their introduction through the present, meaning that the HS 2002 version provides a time series of trade from 2002 to the present whereas the HS 2017 version only provides a time series back to 2017. Notably, HS version 2012 included major revisions to the HS codes relevant to fisheries and aquaculture products. CEPII reconciles discrepancies in mirror trade records, which occur in around 35% of observations (for all traded commodities), by first removing transportation costs and using a weighting scheme based on each country’s reporting reliability to average discrepancies in reported mirror flows. BACI data focuses on trade flows between individual countries since 1994 and therefore drops flows within some groups of countries (e.g., Belgium-Luxembourg) to ensure consistent geographies. The resulting data set covers trade for over 200 countries and 5,000 products. Further details on the BACI data set are available in49. While BACI resolves many data issues contained in the raw UN Comtrade database, it does not correct for all implausible trade flows, which can especially arise if one country misreports a value and the partner country does not report a value50. Further, there are instances where one country reports on trade that is optional to report, and the partner country does not. Here, we do not identify and re-estimate any values reported in BACI. Excessively large exports will generally result in high error terms, while high imports will result in high apparent consumption.\nTrade statistics are managed by each territory and generally guided by the Kyoto Convention. For the purposes of trade data reporting, imports and exports represent all goods which add or subtract, respectively, from the stock of material resources within an economic territory, but not goods which merely pass through a country’s economic territory. The economic territory generally coincides with the customs territory, which refers to the territory in which the country’s custom laws apply. Goods which enter a country for processing are included within trade statistics. Goods which pass through a country “in transit,” including those which are transshipped, are not recommended to be reported in trade statistics, though there are exceptions51 and known instances where one country reports trade which is “in transit” but the partner does not, which creates discrepancies that are not corrected for within BACI. Fishery products from within the country, the country’s waters, or obtained by a vessel of that country are considered goods wholly produced in that country. Catch by foreign vessels and catch by national vessels on the high seas landed in a country’s ports are recorded as imports by the country the products are landed in and as exports by the foreign nation, where economically or environmentally significant. For further trade statistic guideline details, see51.\n\n\nLive weight conversions\nGlobal trade data is reported in terms of the product weight. To convert from product weight (i.e., net weight) to the live weight equivalent, a live weight conversion factor must be applied for each HS code. Live weight conversion factors are sourced from the European Market Observatory for Fisheries and Aquaculture Products (EUMOFA)52, along with various national and international governmental report values. EUMOFA data reports live weight conversion factors by CN-8 codes, so the mean of the live weight conversion factors falling within each HS 6-digit code are used. The EUMOFA data assigns products primarily destined for industrial purposes (e.g., fish meal and fish oil), co-products (e.g., caviar) and live trade a value of zero. In this analysis, co-products retained a live weight conversion factor value of zero to avoid double counting, but live animal trade was assigned a live weight conversion factor of 1 and fish meal and fish oil was assigned an average value of 2.9853. Data compiled from national and international reports were categorized into taxa types (mollusks, crustaceans, fishes, and other aquatic invertebrates), FAO ISSCAAP groups, species or taxon name, type of processing, and country of processing.\nLive weight conversion factors applied to trade data introduce a source of uncertainty and error due to uncertainty in conversion factors is not reported and a single live weight conversion factor is often presented per code, regardless of the species or region of origin. This is a limitation given that there are geographical and temporal variation in live weight conversion factors due to differences in processing technology. Despite this limitation, EUMOFA data offers better documentation and alignment with HS commodity codes than other live weight conversion factor data sources18 and is updated annually, providing documentation for changes in live weight conversion factors. Additionally, by supplementing the EUMOFA data with the other reported values we can better capture specific species processing into various product forms and some regional variability.\nAll conversion factors were reported as live weight to product weight ratios. These conversion factors were mapped onto possible species to commodity or commodity to commodity conversions, described below. For commodity-to-commodity conversions, we estimate the conversion factors (i.e., processing loss rate) as the additional mass lost when converting from the live weight to the original product form relative to converting from live weight to the processed product form. This can be calculated as the live weight conversion factor for the original product form divided by the live weight factor for the processed product form. We assume that mass cannot be gained through processing and therefore impose a maximum value of one to this ratio. Seafood production and commodity conversion For each country-year-HS version combination, we estimate the proportion of each species going into each commodity and the proportion of each imported commodity processed into each other commodity. Each species can only be converted into a subset of the commodities. For example, Atlantic salmon, Salmo salar, can be converted into whole frozen salmon or frozen salmon filets, but cannot be converted to a frozen tilapia filet. Similarly, each commodity can only be converted to a subset of other commodities through processing. For example, whole frozen salmon can be processed into frozen salmon filets, but not vice versa and neither salmon commodity can be converted to a tilapia commodity through processing. Defining possible conversions restricts the solution space to realistic results and improves estimation by reducing the number of unknowns. We describe this assignment process in detail below.\n\n\nTaxonomic group to commodity assignment\nSpecies production to commodity assignment is a many-to-many matching problem, wherein one commodity can consist of multiple species and one species can be converted to multiple commodities. All taxonomic names reported in the FAO production data were matched to HS 6-digit codes based on the code descriptions and HS system hierarchy. The first matching step required dividing all taxonomic groups into the broad commodity groups at the 4-digit level (fish, crustaceans, molluscs and aquatic invertebrates). Within each of these groups, taxonomic groups were matched based on 6 types of matching categories:\n1. Explicit taxa match - Scientific names are matched based on taxonomic information provided in the code description 2. NEC match - All remaining unmatched species within the 4-digit level are assigned to the “not elsewhere considered” (NEC) code 3. NEC by taxa match - When a code description signifies an NEC group, but limits this based on a taxonomic category (e.g., Salmonidae, N.E.C.), the NEC grouping occurs at this level, rather than the broad NEC match 4. Broad commodity match - Only the broad taxonomic groups inform this assignment since no further taxonomic information is provided 5. Aquarium trade match - Assigned to ornamental species trade based on species found in the aquarium/ornamental trade54 6. Fishmeal - Assigned to fishmeal codes if at least 1% of production goes to fishmeal production globally during the study period based on the end use designation from Sea Around Us production data55. Although an estimated 27% of fishmeal is derived from processing by-products3, the species, geographical, and temporal variation in that estimate is currently unknown. Consequently, fishmeal is currently treated as sourced from whole fish reduction. This does not affect the total trade or trade patterns of fishmeal but does result in an overestimate of the proportion of production going to fishmeal in cases where by-products are used.\nAfter all species are matched to the appropriate HS codes, we use the list of species to define codes as inland, marine, diadromous, or mixed. Higher order taxonomic groups are then only matched with HS codes that include their habitat. For example, production of inland actinopterygii is matched with codes that include inland species that fall within actinopterygii, but not with exclusively marine codes, even if they contain species that fall within actinopterygii.\n\n\nCommodity to commodity processing assignment\nAs with the species to commodity assignment, the commodity-to-commodity assignment is a many-to-many data problem. Here, one commodity can be processed into multiple other commodities (i.e., frozen salmon can be processed into salmon filets or canned salmon), which also means one commodity could have come from multiple other commodities. To create these assignments, we established rules for which product transformations are technically possible. First, a product cannot transfer outside of its broad commodity group (e.g., fish, crustaceans, mollusc, aquatic invertebrate). Second, where a more refined species or species group was given (e.g., tunas, salmons, etc.) a product cannot be transformed outside that group. Third, products are classified in terms of their state (e.g., alive, fresh, frozen, etc.) and presentation (e.g., e.g., whole, fileted, salted/dried/preserved meats, reductions such as fish meal and fish oil, etc.) and cannot be converted into less processed forms (e.g., frozen salmon filets cannot turn into a frozen whole salmon). Fourth, specific commodities (i.e. mentioning specific species) and NEC commodities can become broad commodities (where appropriate), however broad commodities cannot become more specific or NEC commodities.\n\n\nCountry standardization and regions\nThe FAO production and BACI trade datasets do not share the same set of countries and territories. For the production and trade data to balance, it is important for the set of territories falling under a given name to align across the datasets. To avoid instances where, for example, production is reported under a territory, but trade is reported under the sovereign nation, we generally group all territories with the sovereign nation. As countries gain independence, they are added as a trade partner in the database. Due to this country standardization circular flows may occur when a sovereign nation trades with their territory. These circular flows are filtered out of the standardized BACI trade flows (i.e., internal trade is not included).\nNote: The original text this section is based on is from Gephart et al. (2024) Nature Communications [add link]. Please reference that paper when referencing this information: [Insert reference]",
    "crumbs": [
      "Our Model",
      "Production"
    ]
  },
  {
    "objectID": "artis-our-model/disagg-trade.html",
    "href": "artis-our-model/disagg-trade.html",
    "title": "Disaggregating Reported Trade",
    "section": "",
    "text": "Disaggregating Reported Trade\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla dolor lacus, suscipit sed fringilla vitae, finibus nec dui. Praesent porttitor bibendum nibh, sit amet imperdiet metus blandit quis. Vivamus leo velit, pretium ut porta non, eleifend at dui. Aenean quis purus eu nibh ornare ornare. Integer vulputate ipsum eu dolor auctor, et pretium nulla vehicula. Praesent vehicula nulla id mauris pharetra porta. Sed vitae libero fermentum, eleifend justo a, ornare est. Sed tincidunt dapibus tortor, vel scelerisque diam vestibulum eu. Nulla consectetur ut lectus vitae efficitur. Phasellus sit amet sodales purus. Proin et arcu vel erat finibus porta a sed arcu. Nullam pretium, nisi finibus viverra feugiat, lacus libero blandit erat, id dictum eros massa eget metus.",
    "crumbs": [
      "Our Model",
      "Disaggregating Reported Trade"
    ]
  },
  {
    "objectID": "artis-about.html",
    "href": "artis-about.html",
    "title": "About Us",
    "section": "",
    "text": "Praesent ornare dolor turpis, sed tincidunt nisl pretium eget. Curabitur sed iaculis ex, vitae tristique sapien. Quisque nec ex dolor. Quisque ut nisl a libero egestas molestie. Nulla vel porta nulla. Phasellus id pretium arcu. Etiam sed mi pellentesque nibh scelerisque elementum sed at urna. Ut congue molestie nibh, sit amet pretium ligula consectetur eu. Integer consectetur augue justo, at placerat erat posuere at. Ut elementum urna lectus, vitae bibendum neque pulvinar quis. Suspendisse vulputate cursus eros id maximus. Duis pulvinar facilisis massa, et condimentum est viverra congue. Curabitur ornare convallis nisl. Morbi dictum scelerisque turpis quis pellentesque. Etiam lectus risus, luctus lobortis risus ut, rutrum vulputate justo. Nulla facilisi.",
    "crumbs": [
      "About us"
    ]
  },
  {
    "objectID": "artis-about.html#section",
    "href": "artis-about.html#section",
    "title": "About Us",
    "section": "Section",
    "text": "Section\nProin sodales neque erat, varius cursus diam tincidunt sit amet. Etiam scelerisque fringilla nisl eu venenatis. Donec sem ipsum, scelerisque ac venenatis quis, hendrerit vel mauris. Praesent semper erat sit amet purus condimentum, sit amet auctor mi feugiat. In hac habitasse platea dictumst. Nunc ac mauris in massa feugiat bibendum id in dui. Praesent accumsan urna at lacinia aliquet. Proin ultricies eu est quis pellentesque. In vel lorem at nisl rhoncus cursus eu quis mi. In eu rutrum ante, quis placerat justo. Etiam euismod nibh nibh, sed elementum nunc imperdiet in. Praesent gravida nunc vel odio lacinia, at tempus nisl placerat. Aenean id ipsum sed est sagittis hendrerit non in tortor.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.",
    "crumbs": [
      "About us"
    ]
  },
  {
    "objectID": "external-content/artis-model-readme.html#creating-python-virtual-environment",
    "href": "external-content/artis-model-readme.html#creating-python-virtual-environment",
    "title": "artis-model",
    "section": "Creating python virtual environment",
    "text": "Creating python virtual environment\n\nOpen the artis-model repository in RStudio.\nClick on the terminal tab.\nType “pwd” in the terminal.\nCopy the result of the “pwd” terminal command.\nType “python3 -m venv [RESULT FROM pwd]/venv” (ie. “python3 -m venv /home/artis-model/venv”)\nType “source venv/bin/activate” in terminal.\nType “pip3 install qpsolvers” in terminal.\nType “pip3 install quadprog” in terminal.\nType “pip3 install cvxopt” in terminal.\nConfirm you have successfully installed the packages qpsolvers, quadprog, cvxopt by running “pip list”.\nType “deactivate” in terminal.\nClick on the Console tab.\n\nNote that you only need to install the solvers the first time you run this code. Warnings about the latest version of pip may also appear during the installation - these are okay, but errors are not."
  },
  {
    "objectID": "external-content/artis-model-readme.html#high-level-overview",
    "href": "external-content/artis-model-readme.html#high-level-overview",
    "title": "artis-model",
    "section": "High level overview",
    "text": "High level overview\nThe following diagrams describes how ARTIS trade records are obtained."
  },
  {
    "objectID": "external-content/artis-model-readme.html#code-workflows",
    "href": "external-content/artis-model-readme.html#code-workflows",
    "title": "artis-model",
    "section": "Code workflows",
    "text": "Code workflows\nThe following diagrams describe the how the codebase follows the workflow illustrated above."
  }
]