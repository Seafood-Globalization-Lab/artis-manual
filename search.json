[
  {
    "objectID": "external-content/artis-hpc-readme.html",
    "href": "external-content/artis-hpc-readme.html",
    "title": "ARTIS HPC",
    "section": "",
    "text": "This repository outlines the instructions and scripts needed to create the ARTIS High Performance Computer (HPC) on Amazon Web Services (AWS).\n\n\n\nCopy the most up-to-date set of model inputs to the project root directory artis-hpc\nCopy the most up-to-date ARTIS R package folder and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package NAMESPACE file and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package DESCRIPTION file and place within artis-hpc/docker_image_files_original/\n\nIf running on a new Apple chip arm64: 1. Copy arm64_venv_requirements.txt file from the root directory to the artis-hpc/docker_image_files_original/ 2. Rename the file artis-hpc/docker_image_files_original/arm64_venv_requirements.txt to artis-hpc/docker_image_files_original/requirements.txt 3. Open artis-hpc/docker_image_files_original/run_artis_hs12.R and uncomment line 20.\n\n\n\n\nTerraform\n\nThis is a set of code scripts that create all the AWS infrastructure needed for the ARTIS HPC\nDestroy all AWS infrastructure for the ARTIS HPC after the ARTIS model has finished (save on unnecessary costs)\n\nDocker\n\nThis is used to create a docker image that our HPC jobs will use to run the ARTIS model code\n\nPython\n\nThrough the docker and AWS python (boto3) clients, this will provide code that:\n\nPush all model input data to AWS S3\nBuild docker image needed that the AWS Batch jobs will need to run ARTIS model\nPush docker image to AWS ECR\nSubmit jobs to ARTIS HPC\nPull all model outputs data\n\n\n\n\n\n\n\nHomebrew\nAWS CLI\nTerraform CLI\nPython\n\nPython packages\n\ndocker\nboto3\n\n\n\n\n\n\nInstall homebrew by running the terminal command /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nClose existing terminal window where installation command was run and open a new terminal window\nConfirm homebrew has been installed, run terminal command brew --version, no error messsage should appear.\n\nIf after homebrew installation you get a message stating brew command not found: 1. Edit zsh config file, run terminal command: vim ~/.zshrc 2. Type “i” to enter edit mode 3. Copy paste this line into the file you opened: export PATH=/opt/homebrew/bin:$PATH 4. Press Shift and : 5. Type “wq” 6. Press enter 7. Source new config file, run terminal command source ~/.zshrc\n\n\n\nFollowing instructions from AWS\nNote: If you already have AWS CLI installed please still confirm by following step 3 below. Both instructions should run without an error message.\nThe following instructions are for MacOS users: 1. Run terminal command curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" 2. Run terminal command sudo installer -pkg AWSCLIV2.pkg -target / 3. Confirm AWS CLI has been installed: 1. Run terminal command which aws 2. Run terminal command aws --version\n\n\n\nNote: If you already have homebrew installed please confirm by running brew --version, no error message should occur.\nTo install terraform on MacOS we will be using homebrew. If you do not have homebrew installed on your computer please follow the installation instructions here, before continuing.\nBased on Terraform CLI installation instructions provided here. 1. Run terminal command brew tap hashicorp/tap 2. Run terminal command brew install hashicorp/tap/terraform 3. Run terminal command brew update 4. Run terminal command brew upgrade hashicorp/tap/terraform\nIf this has been unsuccessful you might need to install xcode command line tools, try: - Run terminal command: sudo xcode-select --install\n\n\n\n\n\nAn AWS root user was created\nAWS root user has created an admin user group with “AdministratorAccess” permissions.\nAWS root user has created IAM users\nAWS root user has add IAM users to admin group\nAWS IAM users have their AWS AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY\n\nTo create an AWS root user visit aws.amazon.com.\nTo create an AWS IAM user: - FIXIT: include screenshots for creating an IAM user with the correct admin permissions.\nNote: If you created ANY AWS RESOURCES for ARTIS manually please delete these before continuing. These resources should\n\n\n\nFIXIT: Explain what each terminal command is doing 1. Run terminal command: export AWS_ACCESS_KEY=[YOUR_AWS_ACCESS_KEY] 2. Run terminal command: export AWS_SECRET_ACCESS_KEY=[YOUR_AWS_SECRET_ACCESS_KEY] 3. Run terminal command: export AWS_REGION=us-east-1 4. Run terminal command aws configure set aws_access_key_id \"[YOUR_AWS_ACCESS_KEY]\" 5. Run terminal command aws configure set aws_secret_access_key \"[YOUR_AWS_SECRET_KEY]\"\n\n\n\nNote: Please make sure that your terminal is currently in your working directory that should end in artis-hpc, by running the terminal command pwd.\n\nCreate a virtual environment, run terminal command:python3 -m venv venv\nOpen virtual environment, run terminal command: source venv/bin/activate\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nCheck that all python modules have been downloaded, run terminal command pip freeze and check that all modules in the requirements.txt file are included.\n\nIf an error occurs please follow these instructions: 1. Upgrade your version of pip by running terminal command: pip install --upgrade pip 2. Install all required python modules, run terminal command: pip3 install -r requirements.txt 3. If errors still occur install each python package in the requirements.txt file individually, run terminal command pip3 install [PACKAGE NAME] ie pip3 install urllib3.\n\n\n\nNote: the initial_setup.py script will create all necessary AWS infrastructure, upload all model inputs to an AWS S3 bucket, and create and upload a docker image based on the ARTIS codebase. It will also submit jobs to the ARTIS HPC.\n\nOpen Docker Desktop\nTake note of any existing docker images and container relating to other projects, and delete all docker container relating to ARTIS, delete all docker images relating to ARTIS.\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nIf you are using an Apple Silicone chip (M1, M2, M3, etc) your chip will be “arm64”, otherwise for intel chips it will be “x86”\n\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: This will create the docker image from scratch. If you have an existing docker image you would like to use include the -di [existing docker image name] with the command.\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name] -di [existing docker image name]:latest\n\n\n\nExample: - If you are creating the docker image from scratch (If you change any R code for ARTIS you will have to recreate a docker image): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image - If you have an existing docker image (for example only need to re-upload a new set of model inputs): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image -di myname-artis-image:latest\nNote: If terraform states that it created all resources however when you log into the AWS console to confirm cannot see them, they have most likely been created as part of another account. Run terraform destroy -auto-approveon the command line. Confirmed you have followed the AWS CLI set up instructions with the correct set of keys (AWS access key and AWS secret access key)."
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#update-artis-model-scripts-and-model-inputs",
    "href": "external-content/artis-hpc-readme.html#update-artis-model-scripts-and-model-inputs",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Copy the most up-to-date set of model inputs to the project root directory artis-hpc\nCopy the most up-to-date ARTIS R package folder and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package NAMESPACE file and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package DESCRIPTION file and place within artis-hpc/docker_image_files_original/\n\nIf running on a new Apple chip arm64: 1. Copy arm64_venv_requirements.txt file from the root directory to the artis-hpc/docker_image_files_original/ 2. Rename the file artis-hpc/docker_image_files_original/arm64_venv_requirements.txt to artis-hpc/docker_image_files_original/requirements.txt 3. Open artis-hpc/docker_image_files_original/run_artis_hs12.R and uncomment line 20."
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#technologies-used",
    "href": "external-content/artis-hpc-readme.html#technologies-used",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Terraform\n\nThis is a set of code scripts that create all the AWS infrastructure needed for the ARTIS HPC\nDestroy all AWS infrastructure for the ARTIS HPC after the ARTIS model has finished (save on unnecessary costs)\n\nDocker\n\nThis is used to create a docker image that our HPC jobs will use to run the ARTIS model code\n\nPython\n\nThrough the docker and AWS python (boto3) clients, this will provide code that:\n\nPush all model input data to AWS S3\nBuild docker image needed that the AWS Batch jobs will need to run ARTIS model\nPush docker image to AWS ECR\nSubmit jobs to ARTIS HPC\nPull all model outputs data"
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#installation",
    "href": "external-content/artis-hpc-readme.html#installation",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Homebrew\nAWS CLI\nTerraform CLI\nPython\n\nPython packages\n\ndocker\nboto3\n\n\n\n\n\n\nInstall homebrew by running the terminal command /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nClose existing terminal window where installation command was run and open a new terminal window\nConfirm homebrew has been installed, run terminal command brew --version, no error messsage should appear.\n\nIf after homebrew installation you get a message stating brew command not found: 1. Edit zsh config file, run terminal command: vim ~/.zshrc 2. Type “i” to enter edit mode 3. Copy paste this line into the file you opened: export PATH=/opt/homebrew/bin:$PATH 4. Press Shift and : 5. Type “wq” 6. Press enter 7. Source new config file, run terminal command source ~/.zshrc\n\n\n\nFollowing instructions from AWS\nNote: If you already have AWS CLI installed please still confirm by following step 3 below. Both instructions should run without an error message.\nThe following instructions are for MacOS users: 1. Run terminal command curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" 2. Run terminal command sudo installer -pkg AWSCLIV2.pkg -target / 3. Confirm AWS CLI has been installed: 1. Run terminal command which aws 2. Run terminal command aws --version\n\n\n\nNote: If you already have homebrew installed please confirm by running brew --version, no error message should occur.\nTo install terraform on MacOS we will be using homebrew. If you do not have homebrew installed on your computer please follow the installation instructions here, before continuing.\nBased on Terraform CLI installation instructions provided here. 1. Run terminal command brew tap hashicorp/tap 2. Run terminal command brew install hashicorp/tap/terraform 3. Run terminal command brew update 4. Run terminal command brew upgrade hashicorp/tap/terraform\nIf this has been unsuccessful you might need to install xcode command line tools, try: - Run terminal command: sudo xcode-select --install"
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#assumptions",
    "href": "external-content/artis-hpc-readme.html#assumptions",
    "title": "ARTIS HPC",
    "section": "",
    "text": "An AWS root user was created\nAWS root user has created an admin user group with “AdministratorAccess” permissions.\nAWS root user has created IAM users\nAWS root user has add IAM users to admin group\nAWS IAM users have their AWS AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY\n\nTo create an AWS root user visit aws.amazon.com.\nTo create an AWS IAM user: - FIXIT: include screenshots for creating an IAM user with the correct admin permissions.\nNote: If you created ANY AWS RESOURCES for ARTIS manually please delete these before continuing. These resources should"
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#aws-cli-setup",
    "href": "external-content/artis-hpc-readme.html#aws-cli-setup",
    "title": "ARTIS HPC",
    "section": "",
    "text": "FIXIT: Explain what each terminal command is doing 1. Run terminal command: export AWS_ACCESS_KEY=[YOUR_AWS_ACCESS_KEY] 2. Run terminal command: export AWS_SECRET_ACCESS_KEY=[YOUR_AWS_SECRET_ACCESS_KEY] 3. Run terminal command: export AWS_REGION=us-east-1 4. Run terminal command aws configure set aws_access_key_id \"[YOUR_AWS_ACCESS_KEY]\" 5. Run terminal command aws configure set aws_secret_access_key \"[YOUR_AWS_SECRET_KEY]\""
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#python-installation",
    "href": "external-content/artis-hpc-readme.html#python-installation",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Note: Please make sure that your terminal is currently in your working directory that should end in artis-hpc, by running the terminal command pwd.\n\nCreate a virtual environment, run terminal command:python3 -m venv venv\nOpen virtual environment, run terminal command: source venv/bin/activate\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nCheck that all python modules have been downloaded, run terminal command pip freeze and check that all modules in the requirements.txt file are included.\n\nIf an error occurs please follow these instructions: 1. Upgrade your version of pip by running terminal command: pip install --upgrade pip 2. Install all required python modules, run terminal command: pip3 install -r requirements.txt 3. If errors still occur install each python package in the requirements.txt file individually, run terminal command pip3 install [PACKAGE NAME] ie pip3 install urllib3."
  },
  {
    "objectID": "external-content/artis-hpc-readme.html#creating-aws-infrastructure-with-a-setup-file",
    "href": "external-content/artis-hpc-readme.html#creating-aws-infrastructure-with-a-setup-file",
    "title": "ARTIS HPC",
    "section": "",
    "text": "Note: the initial_setup.py script will create all necessary AWS infrastructure, upload all model inputs to an AWS S3 bucket, and create and upload a docker image based on the ARTIS codebase. It will also submit jobs to the ARTIS HPC.\n\nOpen Docker Desktop\nTake note of any existing docker images and container relating to other projects, and delete all docker container relating to ARTIS, delete all docker images relating to ARTIS.\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nIf you are using an Apple Silicone chip (M1, M2, M3, etc) your chip will be “arm64”, otherwise for intel chips it will be “x86”\n\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: This will create the docker image from scratch. If you have an existing docker image you would like to use include the -di [existing docker image name] with the command.\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name] -di [existing docker image name]:latest\n\n\n\nExample: - If you are creating the docker image from scratch (If you change any R code for ARTIS you will have to recreate a docker image): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image - If you have an existing docker image (for example only need to re-upload a new set of model inputs): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image -di myname-artis-image:latest\nNote: If terraform states that it created all resources however when you log into the AWS console to confirm cannot see them, they have most likely been created as part of another account. Run terraform destroy -auto-approveon the command line. Confirmed you have followed the AWS CLI set up instructions with the correct set of keys (AWS access key and AWS secret access key)."
  },
  {
    "objectID": "external-content/artis-model-readme.html#creating-python-virtual-environment",
    "href": "external-content/artis-model-readme.html#creating-python-virtual-environment",
    "title": "artis-model",
    "section": "Creating python virtual environment",
    "text": "Creating python virtual environment\n\nOpen the artis-model repository in RStudio.\nClick on the terminal tab.\nType “pwd” in the terminal.\nCopy the result of the “pwd” terminal command.\nType “python3 -m venv [RESULT FROM pwd]/venv” (ie. “python3 -m venv /home/artis-model/venv”)\nType “source venv/bin/activate” in terminal.\nType “pip3 install qpsolvers” in terminal.\nType “pip3 install quadprog” in terminal.\nType “pip3 install cvxopt” in terminal.\nConfirm you have successfully installed the packages qpsolvers, quadprog, cvxopt by running “pip list”.\nType “deactivate” in terminal.\nClick on the Console tab.\n\nNote that you only need to install the solvers the first time you run this code. Warnings about the latest version of pip may also appear during the installation - these are okay, but errors are not."
  },
  {
    "objectID": "external-content/artis-model-readme.html#high-level-overview",
    "href": "external-content/artis-model-readme.html#high-level-overview",
    "title": "artis-model",
    "section": "High level overview",
    "text": "High level overview\nThe following diagrams describes how ARTIS trade records are obtained."
  },
  {
    "objectID": "external-content/artis-model-readme.html#code-workflows",
    "href": "external-content/artis-model-readme.html#code-workflows",
    "title": "artis-model",
    "section": "Code workflows",
    "text": "Code workflows\nThe following diagrams describe the how the codebase follows the workflow illustrated above."
  },
  {
    "objectID": "artis-about.html",
    "href": "artis-about.html",
    "title": "About Us",
    "section": "",
    "text": "Ut ut condimentum augue, nec eleifend nisl. Sed facilisis egestas odio ac pretium. Pellentesque consequat magna sed venenatis sagittis. Vivamus feugiat lobortis magna vitae accumsan. Pellentesque euismod malesuada hendrerit. Ut non mauris non arcu condimentum sodales vitae vitae dolor. Nullam dapibus, velit eget lacinia rutrum, ipsum justo malesuada odio, et lobortis sapien magna vel lacus. Nulla purus neque, hendrerit non malesuada eget, mattis vel erat. Suspendisse potenti.",
    "crumbs": [
      "About us"
    ]
  },
  {
    "objectID": "artis-about.html#section",
    "href": "artis-about.html#section",
    "title": "About Us",
    "section": "Section",
    "text": "Section\nProin sodales neque erat, varius cursus diam tincidunt sit amet. Etiam scelerisque fringilla nisl eu venenatis. Donec sem ipsum, scelerisque ac venenatis quis, hendrerit vel mauris. Praesent semper erat sit amet purus condimentum, sit amet auctor mi feugiat. In hac habitasse platea dictumst. Nunc ac mauris in massa feugiat bibendum id in dui. Praesent accumsan urna at lacinia aliquet. Proin ultricies eu est quis pellentesque. In vel lorem at nisl rhoncus cursus eu quis mi. In eu rutrum ante, quis placerat justo. Etiam euismod nibh nibh, sed elementum nunc imperdiet in. Praesent gravida nunc vel odio lacinia, at tempus nisl placerat. Aenean id ipsum sed est sagittis hendrerit non in tortor.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.",
    "crumbs": [
      "About us"
    ]
  },
  {
    "objectID": "artis-our-model/disagg-trade.html",
    "href": "artis-our-model/disagg-trade.html",
    "title": "Disaggregating Reported Trade",
    "section": "",
    "text": "Disaggregating Reported Trade\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla dolor lacus, suscipit sed fringilla vitae, finibus nec dui. Praesent porttitor bibendum nibh, sit amet imperdiet metus blandit quis. Vivamus leo velit, pretium ut porta non, eleifend at dui. Aenean quis purus eu nibh ornare ornare. Integer vulputate ipsum eu dolor auctor, et pretium nulla vehicula. Praesent vehicula nulla id mauris pharetra porta. Sed vitae libero fermentum, eleifend justo a, ornare est. Sed tincidunt dapibus tortor, vel scelerisque diam vestibulum eu. Nulla consectetur ut lectus vitae efficitur. Phasellus sit amet sodales purus. Proin et arcu vel erat finibus porta a sed arcu. Nullam pretium, nisi finibus viverra feugiat, lacus libero blandit erat, id dictum eros massa eget metus.",
    "crumbs": [
      "Our Model",
      "Disaggregating Reported Trade"
    ]
  },
  {
    "objectID": "artis-our-model/data-inputs.html",
    "href": "artis-our-model/data-inputs.html",
    "title": "Production",
    "section": "",
    "text": "Production\nAquatic resource production comes from the Food and Agriculture Organization (FAO), which provides national capture and aquaculture production5. The Food and Agriculture Organization provides annual capture and aquaculture production data for around 240 countries, territories, or land areas from 1950 to 2020. The FAO data reports production in tonnes (live weight equivalent) of around 550 farmed and 1600 wild capture species and species groups. FAO production data consists primarily of official national statistics, with some verifiable supplemental information from academic reviews, consultant reports, and other specialist literature. Data reported by nations are checked by the FAO for consistency and questionable values are verified with the reporting offices. When countries fail to report production, FAO uses past values to estimate production. For the purposes of this analysis, we do not distinguish between nationally reported, and FAO estimated values.\nAccording to the Coordinating Working Party on Fishery Statistics, catch and landings should be assigned to the country of the flag flown by the fishing vessel irrespective of the location of the fishing. This means that production resulting from a country operating a fishing vessel in a foreign country’s territory should be recorded in the national statistics of the foreign fishing vessel. However, if the vessel is chartered by a company based in the home country or the vessel is fishing for the country under a joint venture contract or similar agreement and the operation is integral to the economy of the host country, this does not apply. Consequently, our estimates of source country generally represent who harvested or caught the aquatic resource regardless of where it was produced (i.e., distant water fishing would generally be attributed to the flag state). In cases of exceptions related to select chartered foreign vessels, joint ventures, or other similar agreements, catch by a foreign vessel but reported by the host country may not match trade reporting if catch does not move through the customs boundary. These instances generate excess apparent consumption.\n\n\nBilateral trade data\nWe use the CEPII BACI world trade database, which is a reconciled version of the UN Comtrade database49. Trade data are reported to the UN by both importers and exporters following the Harmonized System (HS) codes. The HS trade code system organizes traded goods into a hierarchy, with the highest level represented by two-digit codes (e.g., Chapter 03 covers “Fish and Crustaceans, Molluscs and Other Aquatic Invertebrates”), which are broken down into 4-digit headings (e.g., heading 0301 covers “Live fish”), which are then subdivided into 6-digit subheadings (e.g., subheading 030111 covers “Live ornamental freshwater fish”). National statistics offices may further subdivide HS codes into 7- to 12-digit codes but since these are not standard across countries, the HS 6-digit codes are the most highly resolved trade codes available globally. HS codes are administered by the World Customs Organization, which updates the codes every five years. HS versions can be used from their introduction through the present, meaning that the HS 2002 version provides a time series of trade from 2002 to the present whereas the HS 2017 version only provides a time series back to 2017. Notably, HS version 2012 included major revisions to the HS codes relevant to fisheries and aquaculture products.\nCEPII reconciles discrepancies in mirror trade records, which occur in around 35% of observations (for all traded commodities), by first removing transportation costs and using a weighting scheme based on each country’s reporting reliability to average discrepancies in reported mirror flows. BACI data focuses on trade flows between individual countries since 1994 and therefore drops flows within some groups of countries (e.g., Belgium-Luxembourg) to ensure consistent geographies. The resulting data set covers trade for over 200 countries and 5,000 products. Further details on the BACI data set are available in49. While BACI resolves many data issues contained in the raw UN Comtrade database, it does not correct for all implausible trade flows, which can especially arise if one country misreports a value and the partner country does not report a value50. Further, there are instances where one country reports on trade that is optional to report, and the partner country does not. Here, we do not identify and re-estimate any values reported in BACI. Excessively large exports will generally result in high error terms, while high imports will result in high apparent consumption.\nTrade statistics are managed by each territory and generally guided by the Kyoto Convention. For the purposes of trade data reporting, imports and exports represent all goods which add or subtract, respectively, from the stock of material resources within an economic territory, but not goods which merely pass through a country’s economic territory. The economic territory generally coincides with the customs territory, which refers to the territory in which the country’s custom laws apply. Goods which enter a country for processing are included within trade statistics. Goods which pass through a country “in transit,” including those which are transshipped, are not recommended to be reported in trade statistics, though there are exceptions and known instances where one country reports trade which is “in transit” but the partner does not, which creates discrepancies that are not corrected for within BACI. Fishery products from within the country, the country’s waters, or obtained by a vessel of that country are considered goods wholly produced in that country. Catch by foreign vessels and catch by national vessels on the high seas landed in a country’s ports are recorded as imports by the country the products are landed in and as exports by the foreign nation, where economically or environmentally significant. For further trade statistic guideline details, see International Merchandise Trade Statistics: Concepts and Definitions 2010.\n\n\nLive weight conversions\nGlobal trade data is reported in terms of the product weight. To convert from product weight (i.e., net weight) to the live weight equivalent, a live weight conversion factor must be applied for each HS code. Live weight conversion factors are sourced from the European Market Observatory for Fisheries and Aquaculture Products (EUMOFA)52, along with various national and international governmental report values. EUMOFA data reports live weight conversion factors by CN-8 codes, so the mean of the live weight conversion factors falling within each HS 6-digit code are used. The EUMOFA data assigns products primarily destined for industrial purposes (e.g., fish meal and fish oil), co-products (e.g., caviar) and live trade a value of zero. In this analysis, co-products retained a live weight conversion factor value of zero to avoid double counting, but live animal trade was assigned a live weight conversion factor of 1 and fish meal and fish oil was assigned an average value of 2.9853. Data compiled from national and international reports were categorized into taxa types (mollusks, crustaceans, fishes, and other aquatic invertebrates), FAO ISSCAAP groups, species or taxon name, type of processing, and country of processing.\nLive weight conversion factors applied to trade data introduce a source of uncertainty and error due to uncertainty in conversion factors is not reported and a single live weight conversion factor is often presented per code, regardless of the species or region of origin. This is a limitation given that there are geographical and temporal variation in live weight conversion factors due to differences in processing technology. Despite this limitation, EUMOFA data offers better documentation and alignment with HS commodity codes than other live weight conversion factor data sources18 and is updated annually, providing documentation for changes in live weight conversion factors. Additionally, by supplementing the EUMOFA data with the other reported values we can better capture specific species processing into various product forms and some regional variability.\nAll conversion factors were reported as live weight to product weight ratios. These conversion factors were mapped onto possible species to commodity or commodity to commodity conversions, described below. For commodity-to-commodity conversions, we estimate the conversion factors (i.e., processing loss rate) as the additional mass lost when converting from the live weight to the original product form relative to converting from live weight to the processed product form. This can be calculated as the live weight conversion factor for the original product form divided by the live weight factor for the processed product form. We assume that mass cannot be gained through processing and therefore impose a maximum value of one to this ratio. Seafood production and commodity conversion For each country-year-HS version combination, we estimate the proportion of each species going into each commodity and the proportion of each imported commodity processed into each other commodity. Each species can only be converted into a subset of the commodities. For example, Atlantic salmon, Salmo salar, can be converted into whole frozen salmon or frozen salmon filets, but cannot be converted to a frozen tilapia filet. Similarly, each commodity can only be converted to a subset of other commodities through processing. For example, whole frozen salmon can be processed into frozen salmon filets, but not vice versa and neither salmon commodity can be converted to a tilapia commodity through processing. Defining possible conversions restricts the solution space to realistic results and improves estimation by reducing the number of unknowns. We describe this assignment process in detail below.\n\n\nTaxonomic group to commodity assignment\nSpecies production to commodity assignment is a many-to-many matching problem, wherein one commodity can consist of multiple species and one species can be converted to multiple commodities. All taxonomic names reported in the FAO production data were matched to HS 6-digit codes based on the code descriptions and HS system hierarchy. The first matching step required dividing all taxonomic groups into the broad commodity groups at the 4-digit level (fish, crustaceans, molluscs and aquatic invertebrates). Within each of these groups, taxonomic groups were matched based on 6 types of matching categories:\n1. Explicit taxa match - Scientific names are matched based on taxonomic information provided in the code description 2. NEC match - All remaining unmatched species within the 4-digit level are assigned to the “not elsewhere considered” (NEC) code 3. NEC by taxa match - When a code description signifies an NEC group, but limits this based on a taxonomic category (e.g., Salmonidae, N.E.C.), the NEC grouping occurs at this level, rather than the broad NEC match 4. Broad commodity match - Only the broad taxonomic groups inform this assignment since no further taxonomic information is provided 5. Aquarium trade match - Assigned to ornamental species trade based on species found in the aquarium/ornamental trade54 6. Fishmeal - Assigned to fishmeal codes if at least 1% of production goes to fishmeal production globally during the study period based on the end use designation from Sea Around Us production data55. Although an estimated 27% of fishmeal is derived from processing by-products3, the species, geographical, and temporal variation in that estimate is currently unknown. Consequently, fishmeal is currently treated as sourced from whole fish reduction. This does not affect the total trade or trade patterns of fishmeal but does result in an overestimate of the proportion of production going to fishmeal in cases where by-products are used.\nAfter all species are matched to the appropriate HS codes, we use the list of species to define codes as inland, marine, diadromous, or mixed. Higher order taxonomic groups are then only matched with HS codes that include their habitat. For example, production of inland actinopterygii is matched with codes that include inland species that fall within actinopterygii, but not with exclusively marine codes, even if they contain species that fall within actinopterygii.\n\n\nCommodity to commodity processing assignment\nAs with the species to commodity assignment, the commodity-to-commodity assignment is a many-to-many data problem. Here, one commodity can be processed into multiple other commodities (i.e., frozen salmon can be processed into salmon filets or canned salmon), which also means one commodity could have come from multiple other commodities. To create these assignments, we established rules for which product transformations are technically possible. First, a product cannot transfer outside of its broad commodity group (e.g., fish, crustaceans, mollusc, aquatic invertebrate). Second, where a more refined species or species group was given (e.g., tunas, salmons, etc.) a product cannot be transformed outside that group. Third, products are classified in terms of their state (e.g., alive, fresh, frozen, etc.) and presentation (e.g., e.g., whole, fileted, salted/dried/preserved meats, reductions such as fish meal and fish oil, etc.) and cannot be converted into less processed forms (e.g., frozen salmon filets cannot turn into a frozen whole salmon). Fourth, specific commodities (i.e. mentioning specific species) and NEC commodities can become broad commodities (where appropriate), however broad commodities cannot become more specific or NEC commodities.\n\n\nCountry standardization and regions\nThe FAO production and BACI trade datasets do not share the same set of countries and territories. For the production and trade data to balance, it is important for the set of territories falling under a given name to align across the datasets. To avoid instances where, for example, production is reported under a territory, but trade is reported under the sovereign nation, we generally group all territories with the sovereign nation. As countries gain independence, they are added as a trade partner in the database. Due to this country standardization circular flows may occur when a sovereign nation trades with their territory. These circular flows are filtered out of the standardized BACI trade flows (i.e., internal trade is not included).\nNote: The original text this section is based on is from Gephart et al. (2024) Nature Communications [add link]. Please reference that paper when referencing this information: [Insert reference]",
    "crumbs": [
      "Our Model",
      "Production"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#update-artis-model-scripts-and-model-inputs",
    "href": "artis-run-model/run-aws.html#update-artis-model-scripts-and-model-inputs",
    "title": "Running on AWS",
    "section": "Update ARTIS model scripts and model inputs",
    "text": "Update ARTIS model scripts and model inputs\n\nCopy the most up-to-date set of model inputs to the project root directory artis-hpc\nCopy the most up-to-date ARTIS R package folder and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package NAMESPACE file and place within artis-hpc/docker_image_files_original/\nCopy the most up-to-date ARTIS R package DESCRIPTION file and place within artis-hpc/docker_image_files_original/\n\nIf running on a new Apple chip arm64: 1. Copy arm64_venv_requirements.txt file from the root directory to the artis-hpc/docker_image_files_original/ 2. Rename the file artis-hpc/docker_image_files_original/arm64_venv_requirements.txt to artis-hpc/docker_image_files_original/requirements.txt 3. Open artis-hpc/docker_image_files_original/run_artis_hs12.R and uncomment line 20.",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#technologies-used",
    "href": "artis-run-model/run-aws.html#technologies-used",
    "title": "Running on AWS",
    "section": "Technologies used",
    "text": "Technologies used\n\nTerraform\n\nThis is a set of code scripts that create all the AWS infrastructure needed for the ARTIS HPC\nDestroy all AWS infrastructure for the ARTIS HPC after the ARTIS model has finished (save on unnecessary costs)\n\nDocker\n\nThis is used to create a docker image that our HPC jobs will use to run the ARTIS model code\n\nPython\n\nThrough the docker and AWS python (boto3) clients, this will provide code that:\n\nPush all model input data to AWS S3\nBuild docker image needed that the AWS Batch jobs will need to run ARTIS model\nPush docker image to AWS ECR\nSubmit jobs to ARTIS HPC\nPull all model outputs data",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#installation",
    "href": "artis-run-model/run-aws.html#installation",
    "title": "Running on AWS",
    "section": "Installation",
    "text": "Installation\n\nHomebrew\nAWS CLI\nTerraform CLI\nPython\n\nPython packages\n\ndocker\nboto3\n\n\n\n\nHomebrew installation\n\nInstall homebrew by running the terminal command /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nClose existing terminal window where installation command was run and open a new terminal window\nConfirm homebrew has been installed, run terminal command brew --version, no error messsage should appear.\n\nIf after homebrew installation you get a message stating brew command not found: 1. Edit zsh config file, run terminal command: vim ~/.zshrc 2. Type “i” to enter edit mode 3. Copy paste this line into the file you opened: export PATH=/opt/homebrew/bin:$PATH 4. Press Shift and : 5. Type “wq” 6. Press enter 7. Source new config file, run terminal command source ~/.zshrc\n\n\nAWS CLI installation\nFollowing instructions from AWS\nNote: If you already have AWS CLI installed please still confirm by following step 3 below. Both instructions should run without an error message.\nThe following instructions are for MacOS users: 1. Run terminal command curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" 2. Run terminal command sudo installer -pkg AWSCLIV2.pkg -target / 3. Confirm AWS CLI has been installed: 1. Run terminal command which aws 2. Run terminal command aws --version\n\n\nTerraform CLI installation\nNote: If you already have homebrew installed please confirm by running brew --version, no error message should occur.\nTo install terraform on MacOS we will be using homebrew. If you do not have homebrew installed on your computer please follow the installation instructions here, before continuing.\nBased on Terraform CLI installation instructions provided here. 1. Run terminal command brew tap hashicorp/tap 2. Run terminal command brew install hashicorp/tap/terraform 3. Run terminal command brew update 4. Run terminal command brew upgrade hashicorp/tap/terraform\nIf this has been unsuccessful you might need to install xcode command line tools, try: - Run terminal command: sudo xcode-select --install",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#assumptions",
    "href": "artis-run-model/run-aws.html#assumptions",
    "title": "Running on AWS",
    "section": "Assumptions:",
    "text": "Assumptions:\n\nAn AWS root user was created\nAWS root user has created an admin user group with “AdministratorAccess” permissions.\nAWS root user has created IAM users\nAWS root user has add IAM users to admin group\nAWS IAM users have their AWS AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY\n\nTo create an AWS root user visit aws.amazon.com.\nTo create an AWS IAM user: - FIXIT: include screenshots for creating an IAM user with the correct admin permissions.\nNote: If you created ANY AWS RESOURCES for ARTIS manually please delete these before continuing. These resources should",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#aws-cli-setup",
    "href": "artis-run-model/run-aws.html#aws-cli-setup",
    "title": "Running on AWS",
    "section": "AWS CLI Setup",
    "text": "AWS CLI Setup\nFIXIT: Explain what each terminal command is doing 1. Run terminal command: export AWS_ACCESS_KEY=[YOUR_AWS_ACCESS_KEY] 2. Run terminal command: export AWS_SECRET_ACCESS_KEY=[YOUR_AWS_SECRET_ACCESS_KEY] 3. Run terminal command: export AWS_REGION=us-east-1 4. Run terminal command aws configure set aws_access_key_id \"[YOUR_AWS_ACCESS_KEY]\" 5. Run terminal command aws configure set aws_secret_access_key \"[YOUR_AWS_SECRET_KEY]\"",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#python-installation",
    "href": "artis-run-model/run-aws.html#python-installation",
    "title": "Running on AWS",
    "section": "Python Installation",
    "text": "Python Installation\nNote: Please make sure that your terminal is currently in your working directory that should end in artis-hpc, by running the terminal command pwd.\n\nCreate a virtual environment, run terminal command:python3 -m venv venv\nOpen virtual environment, run terminal command: source venv/bin/activate\nInstall all required python modules, run terminal command: pip3 install -r requirements.txt\nCheck that all python modules have been downloaded, run terminal command pip freeze and check that all modules in the requirements.txt file are included.\n\nIf an error occurs please follow these instructions: 1. Upgrade your version of pip by running terminal command: pip install --upgrade pip 2. Install all required python modules, run terminal command: pip3 install -r requirements.txt 3. If errors still occur install each python package in the requirements.txt file individually, run terminal command pip3 install [PACKAGE NAME] ie pip3 install urllib3.",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "artis-run-model/run-aws.html#creating-aws-infrastructure-with-a-setup-file",
    "href": "artis-run-model/run-aws.html#creating-aws-infrastructure-with-a-setup-file",
    "title": "Running on AWS",
    "section": "Creating AWS Infrastructure with a setup file",
    "text": "Creating AWS Infrastructure with a setup file\nNote: the initial_setup.py script will create all necessary AWS infrastructure, upload all model inputs to an AWS S3 bucket, and create and upload a docker image based on the ARTIS codebase. It will also submit jobs to the ARTIS HPC.\n\nOpen Docker Desktop\nTake note of any existing docker images and container relating to other projects, and delete all docker container relating to ARTIS, delete all docker images relating to ARTIS.\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nIf you are using an Apple Silicone chip (M1, M2, M3, etc) your chip will be “arm64”, otherwise for intel chips it will be “x86”\n\nCreate AWS infrastructure, upload model inputs and ARTIS docker image, run terminal command: python3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name]\n\nNote: This will create the docker image from scratch. If you have an existing docker image you would like to use include the -di [existing docker image name] with the command.\n\npython3 initial_setup.py -chip [YOUR CHIP INFRASTRUCTURE] -aws_access_key [YOUR AWS KEY] -aws_secret_key [YOUR AWS SECRET KEY] -s3 [S3 bucket name of your choice]  -ecr [Docker image repository name] -di [existing docker image name]:latest\n\n\n\nExample: - If you are creating the docker image from scratch (If you change any R code for ARTIS you will have to recreate a docker image): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image - If you have an existing docker image (for example only need to re-upload a new set of model inputs): - python3 initial_setup.py -chip arm64 -aws_access_key abc1234 -aws_secret_key secretabc1234 -s3 myname-artis-s3 -ecr myname-artis-image -di myname-artis-image:latest\nNote: If terraform states that it created all resources however when you log into the AWS console to confirm cannot see them, they have most likely been created as part of another account. Run terraform destroy -auto-approveon the command line. Confirmed you have followed the AWS CLI set up instructions with the correct set of keys (AWS access key and AWS secret access key).",
    "crumbs": [
      "Running the Model",
      "Running on AWS"
    ]
  },
  {
    "objectID": "visualize-data.html",
    "href": "visualize-data.html",
    "title": "Visualize ARTIS data",
    "section": "",
    "text": "Visualize ARTIS data\npull from https://github.com/Seafood-Globalization-Lab/exploreARTIS/blob/main/README.md"
  },
  {
    "objectID": "artis-infrastructure/archive.html",
    "href": "artis-infrastructure/archive.html",
    "title": "Archiveed Versions (KNB)",
    "section": "",
    "text": "Archiveed Versions (KNB)\nPull from https://github.com/Seafood-Globalization-Lab/knb-submit/blob/main/README.md (Note: this README is not currently populated)",
    "crumbs": [
      "Distribution Infrastructure",
      "Archiveed Versions (KNB)"
    ]
  },
  {
    "objectID": "artis-infrastructure/sql.html#installations",
    "href": "artis-infrastructure/sql.html#installations",
    "title": "SQL Database",
    "section": "Installations",
    "text": "Installations\n\nDownload PostgreSQL: https://www.postgresql.org/download/\nDownload pgAdmin: https://www.pgadmin.org/download/",
    "crumbs": [
      "Distribution Infrastructure",
      "SQL Database"
    ]
  },
  {
    "objectID": "artis-infrastructure/sql.html#updating-cloud-heroku-database",
    "href": "artis-infrastructure/sql.html#updating-cloud-heroku-database",
    "title": "SQL Database",
    "section": "Updating Cloud (Heroku) Database",
    "text": "Updating Cloud (Heroku) Database\nReference: https://stackoverflow.com/questions/11769860/connect-to-a-heroku-database-with-pgadmin\n\nGathering Heroku database credentials:\n\nSign into the Heroku platform\nClick on the “artis” app\n\n\n\nClick on the “Resources” tab \nClick on “Heroku Postgres” in the list of resources available (this should open a new browser tab) \nClick on the “Credentials” tab \nClick on the arrow by “default 1 app” (this should provide a drop down set of options and details)\nClick on the “show” button to reveal the password for the database\n\n\n\nCreating a connection between the Heroku Database and pgadmin:\n\nOpen pgAdmin\nRight click on the Server list on the left hand side\nSelect Servers &gt; Register &gt; Server (this will open a new smaller window with additional settings) \nEnter a server name (this will only be a local name reference) like “HEROKU_ARTIS” \nClick on the “Connection” tab  The details needed to fill in the following information can be found on the Heroku credentials page we found earlier:\nEnter the host name under the “Host name/address”\nEnter the port number under the “Port” field\nEnter the database name under the “Maintenance database” field\nEnter the user name under the “Username” field\nEnter the password (make sure to click reveal in Heroku) under the “Password”\nSelect Save password for future use\nClick on “Advanced” tab \nEnter the database name under the “DB restriction” field (There should now be a new database connection in your pgAdmin dropdown)\n\n\n\nTest connection to Heroku database:\n\nClick on server connection you created earlier (this will appear under the server name you wrote in earlier, ie “HEROKU_ARTIS”)\nClick the arrow by the server connection name (this should create provide a drop down with options and the 1 database)\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin) \nRun the SQL command “SELECT * FROM users;” (this should return immediately, with a table of the users that have access to the ARTIS API)\n\n\n\nAdd new data to cloud database\n\nFind the tables drop down under the database connection options on the left hand side of pgAdmin \n\nRepeat the following instructions for each table you want to update:\n\nIf the table already exists:\n\n\nRight click on the existing table and select the “Delete/Drop” option \n\n\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin)\nPaste and run the SQL script for creating the table you are interested in updating\nRight-click the “Tables” dropdown and select “Refresh” \nRight-click on the table you just re-created, and select “Import/Export Data” (this will open a new dialog box) \nConfirm the “Import” tab is selected and use the “Filename” field to find the table data you would like to include. \nSelect the “Options” tab\nConfirm the “Header” toggle is activated and the “NULL Strings” field has the value “NA” \nSelect the “Columns” tab\nMake sure the “record_id” column IS NOT part of the “Columns to import” field. If it is please delete this column from the list.",
    "crumbs": [
      "Distribution Infrastructure",
      "SQL Database"
    ]
  },
  {
    "objectID": "artis-infrastructure/sql.html#directory-and-file-structure",
    "href": "artis-infrastructure/sql.html#directory-and-file-structure",
    "title": "SQL Database",
    "section": "Directory and File Structure",
    "text": "Directory and File Structure\n\nprep_db_files.R\n\nTakes raw snet files to a database table\n\ncreate_sql_tables\n\nSQL files to create the different tables",
    "crumbs": [
      "Distribution Infrastructure",
      "SQL Database"
    ]
  },
  {
    "objectID": "artis-infrastructure/sql.html#database-structure",
    "href": "artis-infrastructure/sql.html#database-structure",
    "title": "SQL Database",
    "section": "Database Structure",
    "text": "Database Structure\n\nARTIS snet table\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3C code for direct exporter country\n\n\nimporter_iso3c\nISO3C code for direct importer country\n\n\nsource_country_iso3c\nISO3C code for the country that produced the specific product\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs6\nHS 6 digit code used to identify what product is being traded.\n\n\nsciname\nspecies name traded under the specific HS product and 6-digit code.\n\n\nhabitat\nclassifies whether the specific species’ habitat (marine/inland/unknown).\n\n\nmethod\ndefines method of production (aquaculture/capture/unknown).\n\n\nproduct_weight_t\nproduct weight in tonnes.\n\n\nlive_weight_t\nlive weight in tonnes.\n\n\nyear\nyear in which trade occured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexporter_iso3c\nimporter_iso3c\nsource_country_iso3c\ndom_source\nhs6\nsciname\nhabitat\nmethod\nproduct_weight_t\nlive_weight_t\nyear\n\n\n\n\nCAN\nUSA\nCAN\ndomestic export\n030212\noncorhynchus keta\nmarine\ncapture\n870.34\n1131.45\n2017\n\n\nCHL\nITA\nPER\nforeign export\n230120\nengraulis ringens\nmarine\ncapture\n344.889\n1026.11\n2017\n\n\n\nNote:\n\nDomestic Export: An export where the specific product was produced in the same country as it was exported from.\nForeign Export: An export where a specific product is imported from a source country and then re-exported by another country.\nError Export: An export that cannot be explained by domestic or foreign export records nor production records.\n\n\n\nProduction table\nThis table has all FAO production records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 code for the producing country\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nlive_weight_t\nLive weight in tonnes.\n\n\nyear\nYear species was produced.\n\n\n\n\nSample Production table entry\n\n\n\n\n\n\n\n\n\n\n\niso3c\nsciname\nmethod\nhabitat\nlive_weight_t\nyear\n\n\n\n\nSWE\nabramis brama\ncapture\ninland\n7\n2006\n\n\n\n\n\n\nSAU Production table\nThis table has all SAU production records for all countries in ARTIS for 1996 - 2019. Note all production is marine capture.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\ncountry_name_en\nProducing country name in english\n\n\ncountry_iso3_alpha\nProducing country ISO3 3 letter code\n\n\ncountry_iso3_numeric\nProducing country ISO3 numeric code\n\n\neez\nExclusive Economic Zone\n\n\nsector\neconomic sector\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nyear\nYear species was produced\n\n\nlive_weight_t\nLive weight in tonnes\n\n\n\n\n\nBaci table\nThis table has all BACI bilateral trade records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3 3 letter code for direct exporter country\n\n\nimporter_iso3c\nISO3 3 letter code for direct importer country\n\n\nhs6\nHS 6 digit code used to identify what product is being traded\n\n\nproduct_weight_t\nproduct weight in tonnes\n\n\nhs_version\nHS code version for the year used\n\n\nyear\nyear trade occured\n\n\n\n\n\nCountries table\nThis table contains metadata about countries in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code for country\n\n\ncountry_name\nCountry name in english\n\n\nowid_region\nCountry’s region as defined by Our World in Data\n\n\ncontinent\nCountry’s continent as defined by R countrycode package\n\n\n\n\n\nNutrient metadata table\nThis table contains the nutrient content per 100g of the species in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\nsciname\nspecies scientific name\n\n\ncalcium_mg\ncalcium content (mg) per 100 g of species\n\n\niron_mg\niron content (mg) per 100 g of species\n\n\nprotein_g\nprotein content (g) per 100 g of species\n\n\nfattyacids_g\nfatty acid content (g) per 100 g of species\n\n\nvitamina_mcg\nvitamin a content (mcg) per 100 g of species\n\n\nvitaminb12_mcg\nvitamin b12 content (mcg) per 100 g of species\n\n\nzinc_mg\nzinc content (mg) per 100 g of species\n\n\n\n\n\nComplete consumption table\nThis table contains consumption estimates from trade.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code of consuming country\n\n\nhs6\nHS 6 digit code used to identify what product is being consumed\n\n\nsciname\nspecies scientific name\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nsource_country_iso3c\nISO3 3 letter code of country of origin\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs_version\nHS code version for the year\n\n\npopulation\npopulation of consuming country (sourced from FAO)\n\n\nyear\nyear of consumption\n\n\n\n\n\nCode max resolved taxa table\nThis is a conversion table to resolve a scientific name from higher order taxa (family, order, class etc) to a more specific species based on the HS product and HS version used.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs_version\nHS code version for the year\n\n\nhs6\nspecies scientific name\n\n\nsciname\noriginal scientific name determined by production records\n\n\nsciname_hs_modified\nmore specific/resolved scientific name given hs version and hs product\n\n\n\n\n\n\nProducts table\nThis table contains information about what each HS 6-digit code represents.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs6\nHS 6-digit product code\n\n\ndescription\nProduct description\n\n\npresentation\nProduct form (fillet, whole, fats and oils, non-fish, non-fmp form, other body parts, other meat, livers and roes)\n\n\nstate\nProduct state (live, frozen, preserved, fresh, not for humans, reduced)\n\n\n\n\nSample products table entry\n\n\n\n\n\n\n\n\n\nhs6\ndescription\npresentation\nstate\n\n\n\n\n030212\nFish; Pacific salmon (oncorhynchus spp.), Atlantic salmon (salmo salar), Danube salmon (hucho hucho), fresh or chilled (excluding fillets, livers, roes and other fish meat of heading no. 0304)\nwhole\nfresh",
    "crumbs": [
      "Distribution Infrastructure",
      "SQL Database"
    ]
  },
  {
    "objectID": "artis-infrastructure/api.html",
    "href": "artis-infrastructure/api.html",
    "title": "API",
    "section": "",
    "text": "API\nPull from https://github.com/Seafood-Globalization-Lab/ARTIS-API/blob/main/readme.md (note: this one likely needs updated)",
    "crumbs": [
      "Distribution Infrastructure",
      "API"
    ]
  },
  {
    "objectID": "artis-infrastructure/index.html",
    "href": "artis-infrastructure/index.html",
    "title": "Distribution Infrastructure",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla dolor lacus, suscipit sed fringilla vitae, finibus nec dui. Praesent porttitor bibendum nibh, sit amet imperdiet metus blandit quis. Vivamus leo velit, pretium ut porta non, eleifend at dui. Aenean quis purus eu nibh ornare ornare. Integer vulputate ipsum eu dolor auctor, et pretium nulla vehicula. Praesent vehicula nulla id mauris pharetra porta. Sed vitae libero fermentum, eleifend justo a, ornare est. Sed tincidunt dapibus tortor, vel scelerisque diam vestibulum eu. Nulla consectetur ut lectus vitae efficitur. Phasellus sit amet sodales purus. Proin et arcu vel erat finibus porta a sed arcu. Nullam pretium, nisi finibus viverra feugiat, lacus libero blandit erat, id dictum eros massa eget metus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla dolor lacus, suscipit sed fringilla vitae, finibus nec dui. Praesent porttitor bibendum nibh, sit amet imperdiet metus blandit quis. Vivamus leo velit, pretium ut porta non, eleifend at dui. Aenean quis purus eu nibh ornare ornare. Integer vulputate ipsum eu dolor auctor, et pretium nulla vehicula. Praesent vehicula nulla id mauris pharetra porta. Sed vitae libero fermentum, eleifend justo a, ornare est. Sed tincidunt dapibus tortor, vel scelerisque diam vestibulum eu. Nulla consectetur ut lectus vitae efficitur. Phasellus sit amet sodales purus. Proin et arcu vel erat finibus porta a sed arcu. Nullam pretium, nisi finibus viverra feugiat, lacus libero blandit erat, id dictum eros massa eget metus.",
    "crumbs": [
      "Distribution Infrastructure"
    ]
  },
  {
    "objectID": "artis-infrastructure/index.html#section",
    "href": "artis-infrastructure/index.html#section",
    "title": "Distribution Infrastructure",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla dolor lacus, suscipit sed fringilla vitae, finibus nec dui. Praesent porttitor bibendum nibh, sit amet imperdiet metus blandit quis. Vivamus leo velit, pretium ut porta non, eleifend at dui. Aenean quis purus eu nibh ornare ornare. Integer vulputate ipsum eu dolor auctor, et pretium nulla vehicula. Praesent vehicula nulla id mauris pharetra porta. Sed vitae libero fermentum, eleifend justo a, ornare est. Sed tincidunt dapibus tortor, vel scelerisque diam vestibulum eu. Nulla consectetur ut lectus vitae efficitur. Phasellus sit amet sodales purus. Proin et arcu vel erat finibus porta a sed arcu. Nullam pretium, nisi finibus viverra feugiat, lacus libero blandit erat, id dictum eros massa eget metus.",
    "crumbs": [
      "Distribution Infrastructure"
    ]
  },
  {
    "objectID": "artis-faq.html",
    "href": "artis-faq.html",
    "title": "ARTIS Manual",
    "section": "",
    "text": "The ARTIS model disaggregates reported aquatic resource products (e.g., frozen salmon filets) into species/species groups based on two pieces of information: 1) reported production data and 2) the lowest taxonomic resolution of the code.\nFor the first piece, the mass balance portion of the model estimates the species/species group mix within each domestically-produced product code. We also estimate the processing and re-export of imported products. This allows us to estimate the share of exports sourced from domestic versus foreign production and by using the bilateral trade data, we can trace products back to the country of harvest. The species/species group mix within the code can then be assigned. Consequently, if a country reports production of “bivalvia,” then “bivalvia” will appear in the trade network.\nFor the second source of taxonomic information, we look at all species that fall within a code and identify the lowest common taxonomic name. All exports identified as “error” receive this name. We also have an additional column that can be joined to ARTIS so that these names can be used in place of the model specified name when the lowest taxonomic name for the code is more detailed than the name based on the production data. Note though that if these improved names are used, domestic exports can exceed domestic production. These names (sciname_hs_modified) can be obtained for the trade data by joining the appropriate additional table by the original scientific name, the code, and the HS version. The column “sciname_hs_modified” is provided in the consumption table.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#what-does-it-mean-when-a-scientific-name-is-not-a-true-species-name",
    "href": "artis-faq.html#what-does-it-mean-when-a-scientific-name-is-not-a-true-species-name",
    "title": "ARTIS Manual",
    "section": "",
    "text": "The ARTIS model disaggregates reported aquatic resource products (e.g., frozen salmon filets) into species/species groups based on two pieces of information: 1) reported production data and 2) the lowest taxonomic resolution of the code.\nFor the first piece, the mass balance portion of the model estimates the species/species group mix within each domestically-produced product code. We also estimate the processing and re-export of imported products. This allows us to estimate the share of exports sourced from domestic versus foreign production and by using the bilateral trade data, we can trace products back to the country of harvest. The species/species group mix within the code can then be assigned. Consequently, if a country reports production of “bivalvia,” then “bivalvia” will appear in the trade network.\nFor the second source of taxonomic information, we look at all species that fall within a code and identify the lowest common taxonomic name. All exports identified as “error” receive this name. We also have an additional column that can be joined to ARTIS so that these names can be used in place of the model specified name when the lowest taxonomic name for the code is more detailed than the name based on the production data. Note though that if these improved names are used, domestic exports can exceed domestic production. These names (sciname_hs_modified) can be obtained for the trade data by joining the appropriate additional table by the original scientific name, the code, and the HS version. The column “sciname_hs_modified” is provided in the consumption table.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#why-dont-the-trade-totals-match-what-a-country-reports",
    "href": "artis-faq.html#why-dont-the-trade-totals-match-what-a-country-reports",
    "title": "ARTIS Manual",
    "section": "Why don’t the trade totals match what a country reports?",
    "text": "Why don’t the trade totals match what a country reports?\nReported trade data can (and often does) disagree across sources and it is generally not easy to determine why. Recall that ARTIS represents a disaggregation of BACI data. BACI is a reconciled version of UN Comtrade data, which is the official international database for bilateral trade. A given country’s trade reported to UN Comtrade can differ from what appears in BACI due to the method for reconciling the mirror trade records (see Data or CEPII BACI for more).\nIn addition to differences between BACI and UN Comtrade, sometimes national statistics available through a government’s website differ from what appears in UN Comtrade. There are likely multiple reasons why this could occur and we cannot explain these in all cases. A few factors that can be relevant include differences in the national trade codes (which often expand upon the 6-digit HS system), differences in when the data was last updated (which is particularly relevant for trade data for recent years) and how national statistics report products in-transit through the country.\nAnother source of trade data is bill of ladings data, which is generally not publicly available (though it can be purchased). Bill of ladings data includes finer spatial and temporal resolution and also includes information on the companies involved in trade. We have observed some large differences in the bill of ladings data that we have worked with compared to national and international statistics. Understanding for any individual country and bill of ladings data source why requires researching the specifics of the data source itself (i.e., we cannot provide a general explanation for this).\nOne other point to confirm when comparing data sources is that both sources are reporting the values in the same units (e.g., gross product weight in kg) as there are often multiple weight fields, in addition to value.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#how-is-distant-water-fishing-captured",
    "href": "artis-faq.html#how-is-distant-water-fishing-captured",
    "title": "ARTIS Manual",
    "section": "How is distant water fishing captured?",
    "text": "How is distant water fishing captured?\nWhen developing the ARTIS model, we assume that countries report data as it should be reported under existing international standards. According to the Coordinating Working Party on Fishery Statistics, catch and landings should be assigned to the country of the flag flown by the fishing vessel irrespective of the location of the fishing. This means that production resulting from a country operating a fishing vessel in a foreign country’s territory should be recorded in the national statistics of the foreign fishing vessel. However, if the vessel is chartered by a company based in the home country or the vessel is fishing for the country under a joint venture contract or similar agreement and the operation is integral to the economy of the host country, this does not apply. Consequently, our estimates of source country generally represent who harvested or caught the aquatic resource regardless of where it was produced (i.e., distant water fishing would generally be attributed to the flag state). In cases of exceptions related to select chartered foreign vessels, joint ventures, or other similar agreements, catch by a foreign vessel but reported by the host country may not match trade reporting if catch does not move through the customs boundary. These instances generate excess apparent consumption.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#what-is-the-effect-of-underreported-catch",
    "href": "artis-faq.html#what-is-the-effect-of-underreported-catch",
    "title": "ARTIS Manual",
    "section": "What is the effect of underreported catch?",
    "text": "What is the effect of underreported catch?\nUnreported or underreported catch results in lower production data than what actually occurred. Since the ARTIS model is searching for a solution that explains a country’s exports while minimizing the error term (among other objectives), the first impact of un/under-reported catch is that the country’s apparent consumption will be lower. However, since no terms can go negative, if the exports still cannot be explained by the production and imports, then it will result in a positive error term for the product. It can be helpful to think about the mass-balance problem to understand the effects of changes in production data:\n\n\n\nDiagram representation of mass-balance problem solved in ARTIS model",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#what-is-the-effect-of-products-in-transit-through-an-intermediate-country",
    "href": "artis-faq.html#what-is-the-effect-of-products-in-transit-through-an-intermediate-country",
    "title": "ARTIS Manual",
    "section": "What is the effect of products in transit through an intermediate country?",
    "text": "What is the effect of products in transit through an intermediate country?\nTrade statistics are managed by each territory and generally guided by the Kyoto Convention. For the purposes of trade data reporting, imports and exports represent all goods which add or subtract, respectively, from the stock of material resources within an economic territory, but not goods which merely pass through a country’s economic territory. The economic territory generally coincides with the customs territory, which refers to the territory in which the country’s custom laws apply. Goods which enter a country for processing are included within trade statistics. Goods which pass through a country “in transit,” including those which are transshipped, are not recommended to be reported in trade statistics, though there are exceptions and known instances where one country reports trade which is “in transit” but the partner does not, which creates discrepancies that are not corrected for within BACI. Fishery products from within the country, the country’s waters, or obtained by a vessel of that country are considered goods wholly produced in that country. Catch by foreign vessels and catch by national vessels on the high seas landed in a country’s ports are recorded as imports by the country the products are landed in and as exports by the foreign nation, where economically or environmentally significant. For further trade statistic guideline details, see International Merchandise Trade Statistics: Concepts and Definitions 2010.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#why-are-there-sudden-spikes",
    "href": "artis-faq.html#why-are-there-sudden-spikes",
    "title": "ARTIS Manual",
    "section": "Why are there sudden spikes?",
    "text": "Why are there sudden spikes?\nThere are two common causes of artificial spikes. The first is related to improved resolution in a country’s production data. If a country previously reported production at the genus level, for example, but then starts reporting production at the species level, it could create a spike in the trade associated with the species name. In this case, one could look for a corresponding drop in the genus level data. The second reason is related to change in the HS code system. HS codes are updated every 5 years. To create a continuous time series, we default to providing",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#why-are-there-multiple-rows-for-a-single-hs-product-code-in-the-code-descriptions-metadata-file",
    "href": "artis-faq.html#why-are-there-multiple-rows-for-a-single-hs-product-code-in-the-code-descriptions-metadata-file",
    "title": "ARTIS Manual",
    "section": "Why are there multiple rows for a single HS product code in the code descriptions metadata file?",
    "text": "Why are there multiple rows for a single HS product code in the code descriptions metadata file?\nProducts table check why there are duplicates (usually caused by multiple presentations or states associated with one code, 030719, H5 probably HS12, 20 codes with 2 states or duplicates). Duplicates should be removed before joining.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-faq.html#is-there-price-data-available",
    "href": "artis-faq.html#is-there-price-data-available",
    "title": "ARTIS Manual",
    "section": "Is there price data available?",
    "text": "Is there price data available?\nPrice data generally refers to a single transaction, as the price of a product varies throughout a year (or month or even a day). True price data does exist for some products (e.g., through Urner Barry), but people sometimes mean they are looking for unit value data (value/volume) as a proxy for the average price over the period. This can be generated from UN Comtrade or BACI data and we do have an ancillary file containing these values. However, we have not yet included this information in the core database because the data has not been fully explored yet.\nAnother point to note when it comes to price (or unit value data) is that prices change along the supply chain, particularly with value added processing. So the unit value derived from trade data is not necessarily indicative of the price a landing or the final price consumers face.",
    "crumbs": [
      "FAQs"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#creating-python-virtual-environment",
    "href": "artis-run-model/run-locally.html#creating-python-virtual-environment",
    "title": "Running Locally",
    "section": "Creating python virtual environment",
    "text": "Creating python virtual environment\n\nOpen the artis-model repository in RStudio.\nClick on the terminal tab.\nType “pwd” in the terminal.\nCopy the result of the “pwd” terminal command.\nType “python3 -m venv [RESULT FROM pwd]/venv” (ie. “python3 -m venv /home/artis-model/venv”)\nType “source venv/bin/activate” in terminal.\nType “pip3 install qpsolvers” in terminal.\nType “pip3 install quadprog” in terminal.\nType “pip3 install cvxopt” in terminal.\nConfirm you have successfully installed the packages qpsolvers, quadprog, cvxopt by running “pip list”.\nType “deactivate” in terminal.\nClick on the Console tab.\n\nNote that you only need to install the solvers the first time you run this code. Warnings about the latest version of pip may also appear during the installation - these are okay, but errors are not.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#high-level-overview",
    "href": "artis-run-model/run-locally.html#high-level-overview",
    "title": "Running Locally",
    "section": "High level overview",
    "text": "High level overview\nThe following diagrams describes how ARTIS trade records are obtained.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-run-model/run-locally.html#code-workflows",
    "href": "artis-run-model/run-locally.html#code-workflows",
    "title": "Running Locally",
    "section": "Code workflows",
    "text": "Code workflows\nThe following diagrams describe the how the codebase follows the workflow illustrated above.",
    "crumbs": [
      "Running the Model",
      "Running Locally"
    ]
  },
  {
    "objectID": "artis-our-model/est-consumption.html",
    "href": "artis-our-model/est-consumption.html",
    "title": "Estimating Consuption",
    "section": "",
    "text": "Estimating Consuption\nThe species trade network and FAO production data were used to calculate national apparent consumption by scientific name, habitat, and method for each year.\nFirst, domestic production of products is estimated by multiplying production data by the corresponding estimated X matrix. We then calculate domestic consumption by subtracting domestic exports by HS code from domestic production of products. Domestic consumption by species is then derived based on the volume of domestic consumption and the estimated species composition for the associated code.\nForeign consumption represents the quantity of product imported that was consumed in country (i.e., not subsequently exported). To calculate foreign consumption, we subtract foreign exports from the quantity of processed imports. Processed imports represent the quantity of each product, by HS code, available after accounting for processing, by multiplying the appropriate estimated W by the import vector i. We convert processed imports to live weight by multiplying by the live weight conversion factor. To distinguish human consumable products, we filter out all processed product consumption of fishmeal, fish oil and ornamental products.\nFinally, we disaggregated foreign consumption of processed HS products to species. We assume the species and trade sourcing distribution of foreign consumption of a given code is proportional to the species distribution of the original imported HS products from which a final code was sourced based on the estimated species trade network. We therefore disaggregate foreign consumption by multiplying foreign consumption by the trade flow proportions of imports across all trade partners and species information.\nSince apparent consumption is based on a disappearance model, estimated values are subject to multiple sources of error. Due to discrepancies in production and trade reporting for select countries (e.g., as arises with joint ventures), a few countries had unrealistically large estimated per capita consumption. For country specific consumption estimates, we capped total per capita consumption to 100 kg, as this is slightly above the upper estimate FAOSTAT25. We then adjusted the supply by export partners, scientific name, production method, habitat, and source country for those countries proportionally. A second factor that influences estimated apparent consumption relates to the approach for removing non-human consumable products, particularly the domestic production and use of fishmeal. While we base our estimates on the species mix entering HS code 230120, this is estimated based on exports and domestic use patterns could diverge, leading to errors in the volume removed. Additionally, since many species can enter code 230120 and there is limited empirical data to inform volumes of species entering fishmeal by country, there is greater uncertainty in the exact species mix, and therefore greater uncertainty in the species volumes to exclude from direct human consumption calculations.",
    "crumbs": [
      "Our Model",
      "Estimating Consuption"
    ]
  },
  {
    "objectID": "artis-our-model/index.html",
    "href": "artis-our-model/index.html",
    "title": "Our Model",
    "section": "",
    "text": "Our Model\nTo estimate the aquatic food species trade network, we compiled and aligned data on fishery and aquaculture production, live weight conversion factors, and bilateral global trade. The data span the globe and encompass decades of changes in country and species names and product forms. Over 4000 live weight conversion factors were compiled and matched to 2000+ farmed and wild capture aquatic species which in turn were matched to 900+ traded seafood product descriptions. Though we include nonfood (e.g., fish meal, bait, and ornamental trade) production and trade in the database, we exclude this from the analysis of aquatic food production and consumption. We also exclude mammals, reptiles, fowl, or seaweeds, along with co-products (e.g., caviar, shark fins, and fish meat) to avoid double counting, from the model and resulting database.\nSpecies trade flow estimates occur in two steps. First, we take a mass balance approach, where each country’s seafood exports must equal the domestic production, plus imports, minus domestic consumption, after accounting for processing losses. For each country, we estimate the proportion of seafood production going into each possible commodity, the proportion of each imported commodity processed and exported, and the domestic consumption of each commodity. We then use these estimates with bilateral trade data to solve for the global species flows. This approach substantially improves upon previous efforts by estimating species-level trade, covering all production environments (marine and freshwater) and production methods (farmed and wild caught), and including the processing and export of imported products.\nARTIS also differs from other available trade databases and models in important ways. Databases presenting reported bilateral trade, such as UN Comtrade, do not include source country and are reported in terms of products which generally obscures the species and production method. Closely related or derived databases, such as BACI, thus have the same issues when trying to understand aquatic food origin and species (see below for more details on reported trade data). FAO also provides bilateral trade data on fishery products, with products at a greater disaggregation than what the 6-digit Harmonized System offers, though this still does not provide sourcing or full species/production method information and the data is currently only available for 2019-2021. Multiregional Input-Output models do resolve flows to the source country and these methods could be adapted to derive a similar database once the species mix of domestically exported products is defined. However, existing input-output databases do not disaggregate aquatic foods at this resolution, generally representing “fisheries” and “food and beverages” as a single or in only a few sectors, as these models are generally constrained by the resolution of national input-output and supply and use tables. A notable exception is FABIO, which disaggregates food and agricultural flows into 130 agriculture, food and forestry products, though seafood is represented as a single group.\n\n\n\nOverview of the ARTIS data processing and model",
    "crumbs": [
      "Our Model"
    ]
  },
  {
    "objectID": "artis-our-model/est-trade-net.html",
    "href": "artis-our-model/est-trade-net.html",
    "title": "National mass-balance",
    "section": "",
    "text": "Estimating species bilateral trade flows occurs in two steps: first, solving the national production-trade mass balance, and second, converting reported commodity trade flow estimates to species trade flow estimates based on the estimated species mix going into each domestic and foreign exported commodity.\n\nNational mass-balance\nWe start with the fact that exports must equal production and imports, minus consumption. Since exports are reported as commodities, we solve this mass balance problem in terms of commodities. Production data are reported for each species, so we estimate the elements of a matrix that represents the proportion of production going into each commodity. Since an imported commodity can be processed and exported as a different commodity, we also estimate the proportion of each import being converted into a different commodity. Then for a given country,\ne = V_1∘X⋅p+V_2∘W⋅g-c+ϵ\nIf n is the number of species and m is the number of commodities, then: V_1 is a sparse (m×n) matrix with product conversion factors corresponding to the unknowns in X; X is a sparse (m×n) matrix of the proportion of each species in each commodity; p is a vector of domestic species production (n×1); V_2 is a sparse (m×m) matrix with product conversion factors corresponding to the entries of W; W is a (m×m) matrix of the processed imported commodities; g be a vector of imports (m×1), c is a vector of domestic consumption (m×1), and; ϵ is a vector of error terms (m×1).\nWe compiled reported values for V_1, V_2, e, p and g, and estimate the entries of X, W, c, and ϵ. We first converted this problem to a system of linear equations. Using the property that vec(ABC)=(C^T⊗A)vec(B), we can create A_b=(y^T⊗D_m)D_V, where D_m is a diagonal matrix of ones, with dimension m and D_V is a diagonal matrix with the elements of vec(V). The vector of unknowns is then x_b=vec(Z). We then solve this system of equations with a quadratic optimization solver such that the mass balance equalities are satisfied, trade codes with higher species resolution in X are prioritized, the elements of X, W, and c are otherwise relatively even (i.e., we assume an even distribution of production among commodities unless the data suggests otherwise), that ϵ is as small as possible (i.e., minimize the error), and all unknowns are greater than or equal to zero.\nPositive error terms represent situations where reported production and imports cannot explain exports. This can occur due to under- or un-reported production or imports, over-reporting of exports, errors in the live weight conversion factors, or inconsistencies in the year production and trade are attributed to.\nWe solve the mass-balance problem for each country-year-HS version combination using the Python package “solve_qp.” The estimated species mixes in national production (X), processing of imports (W) and the error term (ϵ) are passed to the next stage of the analysis.\n\n\nConverting the product trade network to a species trade network\nFirst, we compute the mix of species going into each trade code for each country’s domestic exports. To do this, we reweight X so it represents the proportion of each species in each code rather than the proportion of production of a species going into each product. Each country’s estimated X matrix is multiplied by p to get the mass of each species in each commodity. The total mass of each commodity is found by summing all the species volume grouped by commodity and the proportion of each species within a commodity is then calculated by dividing all volumes by their respective commodity mass totals.\nEach country’s exports can be sourced from domestic production, imported products that are subsequently exported, with or without processing (i.e., foreign exports), or from an unknown source (i.e., error exports). Since the mix of these sources cannot be derived from the mass balance equation alone, we calculate a range for sourcing following33. We calculate the maximum possible domestic exports by taking the minimum between the domestic production and total exports. Similarly, we calculated the maximum volume of exports sourced from imports, by taking the minimum between each product’s imports (accounting for processing estimated by W) and exports. The minimum domestic exports are calculated as the minimum between production and the difference in exports and the maximum calculated foreign exports, with the remainder as error exports (minimum foreign exports are calculated in an analogous way). The above results represent midpoint estimates.\n\nmax domestic exportse_(domestic,max) = min(p_domestic domestic production,etotal exports)\nmax foreign exportse_(foreign,max) = min(importsg,total exportse)\nmin domestic exports e_(domestic,min)= min(pdomestic production,etotal exports - e_(foreign,max) max foreign exports)\ne_(foreign,min) min foreign exports = min(importsg,total exportse - me_(domestic,max) ax domestic exports)\ne_(domestic,mid) midpoint domestic exports = (e_(domestic,max) max domestic exports + e_(domestic,min) min domestic exports)/2\ne_(foreign,mid) midpoint foreign exports = (e_(foreign,max) max foreign exports + e_(foreign,min) min foreign exports)/2\n\nFor these three estimates (maximum, minimum and midpoint) we calculate the domestic and foreign weights by dividing domestic export values and foreign export values by total export. We then distribute each country’s exports into domestic, foreign and error exports by multiplying exports by domestic, foreign and error proportions (Fig S8). For each export source, we apply a different species mix to each HS code based on the estimated source country. For domestic exports, we use the exporting country’s estimated X matrix (Fig S9). For error exports, the geographical origin is unknown and may arise from unreported production, so we cannot meaningfully assign a species mix to the code. Consequently, we identify the lowest taxonomic resolution common to all species within the code and assign that name to the trade flow.\nFor foreign exports, we trace the origins back in the supply chain a maximum of three steps (i.e., producer to intermediate exporter to final exporter to final importer), with any remaining foreign export or flows less than 1 tonne left as “unknown” source (Supplementary Fig. Fig S88). The small flows left unresolved comprise around 1% of total trade (Supplementary Fig. 8Fig S10). To link an export of foreign origin to its source country, we use a reweighted version of W to estimate the original imported product codes and connect those to their source country, using a proportional breakdown of each country’s imports of that code. Foreign exports of one country that originated from foreign exports of another country are isolated and undergo the process above to identify the source country. The species mix for foreign trade flows are based on either the source country’s estimated X matrix or the method described above for error exports (Supplementary Fig.9Fig S9).\n\n\nNetwork post-estimation processing\nOnce the species trade flow network is built, we remove all volumes traded below 0.1 tonnes, as the multiplication by small proportions generates overly specific, and likely unrealistic, small flows. Next, to generate a complete time series, we need to compile estimates from across the HS versions. All HS versions are reported since they have been created, for example HS96 reports trade from 1996 until the present. However, the more recent HS versions generally include more specific trade codes and therefore are preferred over older versions. It takes a few years before an HS version is fully adopted, resulting in lower total trade volumes for the first few years an HS version is available compared to the previous HS versions (Supplementary Fig. Fig S7). To provide the most accurate representation of trade, we create a continuous time series by adopting the most recent HS version available after its total trade has met up with the total trade reported under previous HS versions. This results in HS96 being used for 1996 - 2004, HS02 for 2004 - 2009, HS07 for 2010 - 2012 and HS12 for 2013 - 2020. To check the reasonability of estimated trade flows, we first confirmed that all trade flows sum to the original BACI trade flows when grouped by exporter, importer, year, HS code and expressed as product weight. Note that some flows are slightly lower due to the 0.1 tonne threshold (maximum difference of 72 tonnes representing a percent difference of 0.19%). Second, we confirmed that the estimates from the mass balance problem satisfy the problem constraints. Third, we checked that domestic exports of species in live weight equivalent do not exceed production of that species. Fourth, we confirmed that exports of foreign source do not exceed imports of that species. Only 1.4% of cases across all years showed a country’s foreign export of a species exceeded the total import of that species.\nNote: The original text this section is based on is from Gephart et al. (2024) Nature Communications [add link]. Please reference that paper when referencing this information: [Insert reference]",
    "crumbs": [
      "Our Model",
      "National mass-balance"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aquatic Resource Trade in Species (ARTIS) Manual",
    "section": "",
    "text": "Food systems have become increasingly globalized, with over a quarter of all food now traded internationally. Seafood is among the most highly traded foods and it is becoming increasingly globalized, with trade doubling in recent decades. At the same time, seafood is now widely recognized as a critical source of nutrition. Thus, social and environmental threats to local seafood production, including environmental extremes, price impacts of market integration, networked risks, and increased availability of processed foods, must be evaluated in the context of global trade. These issues are paralleled by similar questions for other natural resources and are central to global food systems research. However, our collective understanding of the environmental and human outcomes of food system globalization is limited by a fundamental gap between production and trade data. We bridge this gap in the Aquatic Resource Trade in Species (ARTIS) database by providing the first global estimates of seafood species and nutrient trade flows from 1996–2020.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "external-content/artis-database-readme.html",
    "href": "external-content/artis-database-readme.html",
    "title": "ARTIS Database",
    "section": "",
    "text": "Download PostgreSQL: https://www.postgresql.org/download/\nDownload pgAdmin: https://www.pgadmin.org/download/\n\n\n\n\nReference: https://stackoverflow.com/questions/11769860/connect-to-a-heroku-database-with-pgadmin\n\n\n\nSign into the Heroku platform\nClick on the “artis” app\n\n\n\nClick on the “Resources” tab \nClick on “Heroku Postgres” in the list of resources available (this should open a new browser tab) \nClick on the “Credentials” tab \nClick on the arrow by “default 1 app” (this should provide a drop down set of options and details)\nClick on the “show” button to reveal the password for the database\n\n\n\n\n\nOpen pgAdmin\nRight click on the Server list on the left hand side\nSelect Servers &gt; Register &gt; Server (this will open a new smaller window with additional settings) \nEnter a server name (this will only be a local name reference) like “HEROKU_ARTIS” \nClick on the “Connection” tab  The details needed to fill in the following information can be found on the Heroku credentials page we found earlier:\nEnter the host name under the “Host name/address”\nEnter the port number under the “Port” field\nEnter the database name under the “Maintenance database” field\nEnter the user name under the “Username” field\nEnter the password (make sure to click reveal in Heroku) under the “Password”\nSelect Save password for future use\nClick on “Advanced” tab \nEnter the database name under the “DB restriction” field (There should now be a new database connection in your pgAdmin dropdown)\n\n\n\n\n\nClick on server connection you created earlier (this will appear under the server name you wrote in earlier, ie “HEROKU_ARTIS”)\nClick the arrow by the server connection name (this should create provide a drop down with options and the 1 database)\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin) \nRun the SQL command “SELECT * FROM users;” (this should return immediately, with a table of the users that have access to the ARTIS API)\n\n\n\n\n\nFind the tables drop down under the database connection options on the left hand side of pgAdmin \n\nRepeat the following instructions for each table you want to update:\n\nIf the table already exists:\n\n\nRight click on the existing table and select the “Delete/Drop” option \n\n\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin)\nPaste and run the SQL script for creating the table you are interested in updating\nRight-click the “Tables” dropdown and select “Refresh” \nRight-click on the table you just re-created, and select “Import/Export Data” (this will open a new dialog box) \nConfirm the “Import” tab is selected and use the “Filename” field to find the table data you would like to include. \nSelect the “Options” tab\nConfirm the “Header” toggle is activated and the “NULL Strings” field has the value “NA” \nSelect the “Columns” tab\nMake sure the “record_id” column IS NOT part of the “Columns to import” field. If it is please delete this column from the list. \n\n\n\n\n\n\nprep_db_files.R\n\nTakes raw snet files to a database table\n\ncreate_sql_tables\n\nSQL files to create the different tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3C code for direct exporter country\n\n\nimporter_iso3c\nISO3C code for direct importer country\n\n\nsource_country_iso3c\nISO3C code for the country that produced the specific product\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs6\nHS 6 digit code used to identify what product is being traded.\n\n\nsciname\nspecies name traded under the specific HS product and 6-digit code.\n\n\nhabitat\nclassifies whether the specific species’ habitat (marine/inland/unknown).\n\n\nmethod\ndefines method of production (aquaculture/capture/unknown).\n\n\nproduct_weight_t\nproduct weight in tonnes.\n\n\nlive_weight_t\nlive weight in tonnes.\n\n\nyear\nyear in which trade occured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexporter_iso3c\nimporter_iso3c\nsource_country_iso3c\ndom_source\nhs6\nsciname\nhabitat\nmethod\nproduct_weight_t\nlive_weight_t\nyear\n\n\n\n\nCAN\nUSA\nCAN\ndomestic export\n030212\noncorhynchus keta\nmarine\ncapture\n870.34\n1131.45\n2017\n\n\nCHL\nITA\nPER\nforeign export\n230120\nengraulis ringens\nmarine\ncapture\n344.889\n1026.11\n2017\n\n\n\nNote:\n\nDomestic Export: An export where the specific product was produced in the same country as it was exported from.\nForeign Export: An export where a specific product is imported from a source country and then re-exported by another country.\nError Export: An export that cannot be explained by domestic or foreign export records nor production records.\n\n\n\n\nThis table has all FAO production records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 code for the producing country\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nlive_weight_t\nLive weight in tonnes.\n\n\nyear\nYear species was produced.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niso3c\nsciname\nmethod\nhabitat\nlive_weight_t\nyear\n\n\n\n\nSWE\nabramis brama\ncapture\ninland\n7\n2006\n\n\n\n\n\n\n\nThis table has all SAU production records for all countries in ARTIS for 1996 - 2019. Note all production is marine capture.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\ncountry_name_en\nProducing country name in english\n\n\ncountry_iso3_alpha\nProducing country ISO3 3 letter code\n\n\ncountry_iso3_numeric\nProducing country ISO3 numeric code\n\n\neez\nExclusive Economic Zone\n\n\nsector\neconomic sector\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nyear\nYear species was produced\n\n\nlive_weight_t\nLive weight in tonnes\n\n\n\n\n\n\nThis table has all BACI bilateral trade records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3 3 letter code for direct exporter country\n\n\nimporter_iso3c\nISO3 3 letter code for direct importer country\n\n\nhs6\nHS 6 digit code used to identify what product is being traded\n\n\nproduct_weight_t\nproduct weight in tonnes\n\n\nhs_version\nHS code version for the year used\n\n\nyear\nyear trade occured\n\n\n\n\n\n\nThis table contains metadata about countries in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code for country\n\n\ncountry_name\nCountry name in english\n\n\nowid_region\nCountry’s region as defined by Our World in Data\n\n\ncontinent\nCountry’s continent as defined by R countrycode package\n\n\n\n\n\n\nThis table contains the nutrient content per 100g of the species in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\nsciname\nspecies scientific name\n\n\ncalcium_mg\ncalcium content (mg) per 100 g of species\n\n\niron_mg\niron content (mg) per 100 g of species\n\n\nprotein_g\nprotein content (g) per 100 g of species\n\n\nfattyacids_g\nfatty acid content (g) per 100 g of species\n\n\nvitamina_mcg\nvitamin a content (mcg) per 100 g of species\n\n\nvitaminb12_mcg\nvitamin b12 content (mcg) per 100 g of species\n\n\nzinc_mg\nzinc content (mg) per 100 g of species\n\n\n\n\n\n\nThis table contains consumption estimates from trade.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code of consuming country\n\n\nhs6\nHS 6 digit code used to identify what product is being consumed\n\n\nsciname\nspecies scientific name\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nsource_country_iso3c\nISO3 3 letter code of country of origin\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs_version\nHS code version for the year\n\n\npopulation\npopulation of consuming country (sourced from FAO)\n\n\nyear\nyear of consumption\n\n\n\n\n\n\nThis is a conversion table to resolve a scientific name from higher order taxa (family, order, class etc) to a more specific species based on the HS product and HS version used.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs_version\nHS code version for the year\n\n\nhs6\nspecies scientific name\n\n\nsciname\noriginal scientific name determined by production records\n\n\nsciname_hs_modified\nmore specific/resolved scientific name given hs version and hs product\n\n\n\n\n\n\n\nThis table contains information about what each HS 6-digit code represents.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs6\nHS 6-digit product code\n\n\ndescription\nProduct description\n\n\npresentation\nProduct form (fillet, whole, fats and oils, non-fish, non-fmp form, other body parts, other meat, livers and roes)\n\n\nstate\nProduct state (live, frozen, preserved, fresh, not for humans, reduced)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhs6\ndescription\npresentation\nstate\n\n\n\n\n030212\nFish; Pacific salmon (oncorhynchus spp.), Atlantic salmon (salmo salar), Danube salmon (hucho hucho), fresh or chilled (excluding fillets, livers, roes and other fish meat of heading no. 0304)\nwhole\nfresh"
  },
  {
    "objectID": "external-content/artis-database-readme.html#installations",
    "href": "external-content/artis-database-readme.html#installations",
    "title": "ARTIS Database",
    "section": "",
    "text": "Download PostgreSQL: https://www.postgresql.org/download/\nDownload pgAdmin: https://www.pgadmin.org/download/"
  },
  {
    "objectID": "external-content/artis-database-readme.html#updating-cloud-heroku-database",
    "href": "external-content/artis-database-readme.html#updating-cloud-heroku-database",
    "title": "ARTIS Database",
    "section": "",
    "text": "Reference: https://stackoverflow.com/questions/11769860/connect-to-a-heroku-database-with-pgadmin\n\n\n\nSign into the Heroku platform\nClick on the “artis” app\n\n\n\nClick on the “Resources” tab \nClick on “Heroku Postgres” in the list of resources available (this should open a new browser tab) \nClick on the “Credentials” tab \nClick on the arrow by “default 1 app” (this should provide a drop down set of options and details)\nClick on the “show” button to reveal the password for the database\n\n\n\n\n\nOpen pgAdmin\nRight click on the Server list on the left hand side\nSelect Servers &gt; Register &gt; Server (this will open a new smaller window with additional settings) \nEnter a server name (this will only be a local name reference) like “HEROKU_ARTIS” \nClick on the “Connection” tab  The details needed to fill in the following information can be found on the Heroku credentials page we found earlier:\nEnter the host name under the “Host name/address”\nEnter the port number under the “Port” field\nEnter the database name under the “Maintenance database” field\nEnter the user name under the “Username” field\nEnter the password (make sure to click reveal in Heroku) under the “Password”\nSelect Save password for future use\nClick on “Advanced” tab \nEnter the database name under the “DB restriction” field (There should now be a new database connection in your pgAdmin dropdown)\n\n\n\n\n\nClick on server connection you created earlier (this will appear under the server name you wrote in earlier, ie “HEROKU_ARTIS”)\nClick the arrow by the server connection name (this should create provide a drop down with options and the 1 database)\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin) \nRun the SQL command “SELECT * FROM users;” (this should return immediately, with a table of the users that have access to the ARTIS API)\n\n\n\n\n\nFind the tables drop down under the database connection options on the left hand side of pgAdmin \n\nRepeat the following instructions for each table you want to update:\n\nIf the table already exists:\n\n\nRight click on the existing table and select the “Delete/Drop” option \n\n\nRight-click on the database name in drop down options\nSelect the “Query tool” option (this should open a window in pgAdmin)\nPaste and run the SQL script for creating the table you are interested in updating\nRight-click the “Tables” dropdown and select “Refresh” \nRight-click on the table you just re-created, and select “Import/Export Data” (this will open a new dialog box) \nConfirm the “Import” tab is selected and use the “Filename” field to find the table data you would like to include. \nSelect the “Options” tab\nConfirm the “Header” toggle is activated and the “NULL Strings” field has the value “NA” \nSelect the “Columns” tab\nMake sure the “record_id” column IS NOT part of the “Columns to import” field. If it is please delete this column from the list."
  },
  {
    "objectID": "external-content/artis-database-readme.html#directory-and-file-structure",
    "href": "external-content/artis-database-readme.html#directory-and-file-structure",
    "title": "ARTIS Database",
    "section": "",
    "text": "prep_db_files.R\n\nTakes raw snet files to a database table\n\ncreate_sql_tables\n\nSQL files to create the different tables"
  },
  {
    "objectID": "external-content/artis-database-readme.html#database-structure",
    "href": "external-content/artis-database-readme.html#database-structure",
    "title": "ARTIS Database",
    "section": "",
    "text": "Column Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3C code for direct exporter country\n\n\nimporter_iso3c\nISO3C code for direct importer country\n\n\nsource_country_iso3c\nISO3C code for the country that produced the specific product\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs6\nHS 6 digit code used to identify what product is being traded.\n\n\nsciname\nspecies name traded under the specific HS product and 6-digit code.\n\n\nhabitat\nclassifies whether the specific species’ habitat (marine/inland/unknown).\n\n\nmethod\ndefines method of production (aquaculture/capture/unknown).\n\n\nproduct_weight_t\nproduct weight in tonnes.\n\n\nlive_weight_t\nlive weight in tonnes.\n\n\nyear\nyear in which trade occured.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexporter_iso3c\nimporter_iso3c\nsource_country_iso3c\ndom_source\nhs6\nsciname\nhabitat\nmethod\nproduct_weight_t\nlive_weight_t\nyear\n\n\n\n\nCAN\nUSA\nCAN\ndomestic export\n030212\noncorhynchus keta\nmarine\ncapture\n870.34\n1131.45\n2017\n\n\nCHL\nITA\nPER\nforeign export\n230120\nengraulis ringens\nmarine\ncapture\n344.889\n1026.11\n2017\n\n\n\nNote:\n\nDomestic Export: An export where the specific product was produced in the same country as it was exported from.\nForeign Export: An export where a specific product is imported from a source country and then re-exported by another country.\nError Export: An export that cannot be explained by domestic or foreign export records nor production records.\n\n\n\n\nThis table has all FAO production records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 code for the producing country\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nlive_weight_t\nLive weight in tonnes.\n\n\nyear\nYear species was produced.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niso3c\nsciname\nmethod\nhabitat\nlive_weight_t\nyear\n\n\n\n\nSWE\nabramis brama\ncapture\ninland\n7\n2006\n\n\n\n\n\n\n\nThis table has all SAU production records for all countries in ARTIS for 1996 - 2019. Note all production is marine capture.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\ncountry_name_en\nProducing country name in english\n\n\ncountry_iso3_alpha\nProducing country ISO3 3 letter code\n\n\ncountry_iso3_numeric\nProducing country ISO3 numeric code\n\n\neez\nExclusive Economic Zone\n\n\nsector\neconomic sector\n\n\nsciname\nspecies produced (matches with sciname column in sciname table)\n\n\nyear\nYear species was produced\n\n\nlive_weight_t\nLive weight in tonnes\n\n\n\n\n\n\nThis table has all BACI bilateral trade records for all countries in ARTIS for 1996 - 2020.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nexporter_iso3c\nISO3 3 letter code for direct exporter country\n\n\nimporter_iso3c\nISO3 3 letter code for direct importer country\n\n\nhs6\nHS 6 digit code used to identify what product is being traded\n\n\nproduct_weight_t\nproduct weight in tonnes\n\n\nhs_version\nHS code version for the year used\n\n\nyear\nyear trade occured\n\n\n\n\n\n\nThis table contains metadata about countries in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code for country\n\n\ncountry_name\nCountry name in english\n\n\nowid_region\nCountry’s region as defined by Our World in Data\n\n\ncontinent\nCountry’s continent as defined by R countrycode package\n\n\n\n\n\n\nThis table contains the nutrient content per 100g of the species in the ARTIS database.\n\n\n\nColumn Name\nDescription\n\n\n\n\nsciname\nspecies scientific name\n\n\ncalcium_mg\ncalcium content (mg) per 100 g of species\n\n\niron_mg\niron content (mg) per 100 g of species\n\n\nprotein_g\nprotein content (g) per 100 g of species\n\n\nfattyacids_g\nfatty acid content (g) per 100 g of species\n\n\nvitamina_mcg\nvitamin a content (mcg) per 100 g of species\n\n\nvitaminb12_mcg\nvitamin b12 content (mcg) per 100 g of species\n\n\nzinc_mg\nzinc content (mg) per 100 g of species\n\n\n\n\n\n\nThis table contains consumption estimates from trade.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\niso3c\nISO3 3 letter code of consuming country\n\n\nhs6\nHS 6 digit code used to identify what product is being consumed\n\n\nsciname\nspecies scientific name\n\n\nmethod\nproduction method (aquaculture/capture/unknown)\n\n\nhabitat\nhabitat where species resides (marine/inland/unknown)\n\n\nsource_country_iso3c\nISO3 3 letter code of country of origin\n\n\ndom_source\nDefines whether trade record was a “domestic export”, “foreign export” or “error export”\n\n\nhs_version\nHS code version for the year\n\n\npopulation\npopulation of consuming country (sourced from FAO)\n\n\nyear\nyear of consumption\n\n\n\n\n\n\nThis is a conversion table to resolve a scientific name from higher order taxa (family, order, class etc) to a more specific species based on the HS product and HS version used.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs_version\nHS code version for the year\n\n\nhs6\nspecies scientific name\n\n\nsciname\noriginal scientific name determined by production records\n\n\nsciname_hs_modified\nmore specific/resolved scientific name given hs version and hs product\n\n\n\n\n\n\n\nThis table contains information about what each HS 6-digit code represents.\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nhs6\nHS 6-digit product code\n\n\ndescription\nProduct description\n\n\npresentation\nProduct form (fillet, whole, fats and oils, non-fish, non-fmp form, other body parts, other meat, livers and roes)\n\n\nstate\nProduct state (live, frozen, preserved, fresh, not for humans, reduced)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhs6\ndescription\npresentation\nstate\n\n\n\n\n030212\nFish; Pacific salmon (oncorhynchus spp.), Atlantic salmon (salmo salar), Danube salmon (hucho hucho), fresh or chilled (excluding fillets, livers, roes and other fish meat of heading no. 0304)\nwhole\nfresh"
  }
]